[
    {
        "instance_id": "bq011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many pseudo users were active in the last 7 days but inactive in the last 2 days as of January 7, 2021?",
        "plan": "1. Identify pseudo users (`user_pseudo_id`) active in the last 7 days: query the `events_*` tables to find users who were active in the last 7 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210101` to `20210107`).\n2. Identify pseudo users (`user_pseudo_id`) active in the last 2 days: query the `events_*` tables to find users who were active in the last 2 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210105` to `20210107`).\n3. Combine results and filter:\n- Use a `LEFT JOIN` to combine the two sets of users and filter out users who were active in the last 2 days.\n- Count the distinct user IDs who meet the criteria of being active in the last 7 days but not in the last 2 days.",
        "external_knowledge": "ga4_obfuscated_sample_ecommerce.events.md"
    },
    {
        "instance_id": "bq010",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Find the top-selling product among customers who bought 'Youtube Men\u2019s Vintage Henley' in July 2017, excluding itself.",
        "plan": "1. Extract a distinct list of customers (`fullVisitorId`) who purchased the \"YouTube Men's Vintage Henley\" in July 2017.\n2. Find other products purchased by these customers in July 2017.\n3. Filter out the \"YouTube Men's Vintage Henley\" product itself and aggregate other products purchased by the same customers.\n4. Sort to find the most purchased product.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq009",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Which traffic source receives the top revenue in 2017 and what is the difference (millions, rounded to two decimal places) between its highest and lowest revenue months?",
        "plan": "1. Calculate monthly revenue for each traffic source.\n2. Aggregate the monthly revenues to compute the total yearly revenue for each traffic source.\n3. Determine which traffic source has the highest total revenue for the year 2017.\n4. Retrieve the monthly revenue data for the top traffic source identified in the previous step.\n5. Calculate the difference between the highest and lowest monthly revenues for the top traffic source.\n6. Retrieve the traffic source and the revenue difference.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq001",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "I wonder how many days between the first transaction and the first visit for each transacting visitor in Feburary 2017, along with the device used in the transaction.",
        "plan": "1. Firstly, extract the first visit date for each visitor in the specified range `201702`.\n2. Next, extract the first transaction date for each visitor in Feb 2017.\n3. Then, extract the device categories used for transactions.\n4. Combine the visit, transaction and device data.\n5. Calculate the number of days between the first transaction and the first visit date for each visitor using `DATE_DIFF`.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq002",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the maximum monthly, weekly, and daily product revenues (in millions) generated by the top-performing traffic source in the first half of 2017?",
        "plan": "1. Firstly, we define the date range to be the first half of year 2017: 20170101 to 20170630.\n2. Next, calculate daily revenues for each traffic source.\n3. Similarly, calculate weekly and monthly revenues for each traffic source.\n4. Determine the top-performing traffic source through aggregation and sorting.\n5. Calculate the maximum revenues for this traffic source on daily/weekly/monthly basis respectively.\n6. Return the final results.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq003",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Compare the average pageviews per visitor between purchase and non-purchase sessions for each month from April to July in 2017.",
        "plan": "1. Calculate average pageviews for non-purchase sessions:\n- Extracts the year and month from the `date` field.\n- Filters sessions with no transactions and no product revenue.\n- Aggregates data by month and calculates the average pageviews per visitor.\n2. Similarly, calculate average pageviews for purchase sessions. The difference is that we only include sessions with at least one transaction and product revenue.\n3. Combine and compare the results, that is select and order results by month.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq004",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most popular other purchased product in July 2017 with consumers who bought products relevant to YouTube?",
        "plan": "1. Identify visitors who purchased any YouTube product in July 2017.\n2. Calculate the total quantity of each product (excluding YouTube products) purchased by visitors who bought any YouTube product in July 2017.\n3. Retrieve the product name with the highest total quantity.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq008",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most common next page for visitors who were part of \"Data Share\" campaign and after they accessed the page starting with '/home' in January 2017. And what's the maximum duration time (in seconds) when they visit the corresponding home page?",
        "plan": "1. Identify relevant visits and pages: extract sessions from the `ga_sessions_*` table within January 2017 that contain page hits with paths starting with '/home' and were part of the \"Data Share\" campaign.\n2. Generate visitor page sequence: combine the filtered sessions with their corresponding page hits to get the full visitor ID, visit ID, timestamp, and page path for each page visit. And order the pages by their visit timestamps.\n3. Calculate the next page and duration:\n   - Use the `LEAD` window function to determine the next page and calculate the duration spent on the current page (by subtracting the current timestamp from the next timestamp).\n   - Rank the pages within each session to maintain the sequence of page visits.\n4. Create the page visit sequence CTE: combine the results into a Common Table Expression (CTE) called that includes the visitor ID, visit ID, page path, duration on the page, next page path, and visit step number.\n5. Determine the most common next page after visiting '/home' page. From the table in Step 4, filter for entries where the current page path starts with '/home', group by the next page, and count occurrences to find the most common next page.\n6. Calculate the maximum duration on home pages. Filter for entries where the current page path starts with '/home' and return the maximum duration spent on it.\n7. Combine and return the results in Step 5 and Step 6, which include the most common next page and the maximum duration.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md"
    },
    {
        "instance_id": "bq029",
        "db": "patents-public-data.patents",
        "question": "Get the number of patent publications and the average number of inventors per patent in the US every five years from 1945 to 2020?",
        "plan": "1.Extract information on patent applications in the United States since 1945.\n2.Divide the data into five-year intervals.\n3.Within each interval, count the number of applicants for each patent, ensuring each patent has more than zero applicants.\n4.Calculate both the total number of patents and the average number of applicants per patent for each interval.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq026",
        "db": "patents-public-data.patents",
        "question": "For the assignee who has been the most active in the patent category 'A61K39', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.",
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the top five countries with the most applications for each assignee per year.\n4. Sort to find out which assignee has the highest total number of applications.\n5. Select the year with the most applications for this assignee.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq026_1",
        "db": "patents-public-data.patents",
        "question": "In which year did the assignee with the most applications in the patent category 'A61K39' file the most?",
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the assignee with the most total applications for the \u201cA61K39\u201d patent.\n4. Select the year with the most applications for this assignee",
        "external_knowledge": null
    },
    {
        "instance_id": "bq026_2",
        "db": "patents-public-data.patents",
        "question": "For patent class A01B3, I want to analyze the information of the top 20 assignees based on the total number of applications. Please provide the following five pieces of information: the names of these assignees, their total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq033",
        "db": "patents-public-data.patents",
        "question": "How many US patent applications about IoT applications were filed each month from 2008 to 2022?",
        "plan": "1. Extract patents with abstracts containing \"Internet of Things\" that were applied for in the United States.\n2. Generate a record set starting from January 2008 to December 2022.\n3. Count and record the number of \"Internet of Things\" patents applied for each month.\n4. Sort and return the monthly totals with the highest number of patent applications.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq209",
        "db": "patents-public-data.patents",
        "question": "Could you find out which US utility patent granted in January 2010, with a published application, has the most forward citations over the ten years following its application date?",
        "plan": "1. Retrieve all patents granted in January 2010, considering only those with the kind code B2, which are utility patents issued with a published application.\n2. Query other patents to extract forward citation information from patent publications, ensuring that the citation date of the citing patent falls within ten years of the filing date of the cited patent.\n3. Calculate the number of distinct citation applications for each patent to identify the patent with the most citations.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq027",
        "db": "patents-public-data.patents",
        "question": "For US B2 patents granted in the first seven days of January 2018, tell me the publication number of each patent and the number of backward citations it has received in the SEA category.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq210",
        "db": "patents-public-data.patents",
        "question": "How many US B2 patents granted between 2015 and 2018 contain claims that do not include the word 'claim'?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq211",
        "db": "patents-public-data.patents",
        "question": "Among the US utility B2 patents granted in January 2008, how many of them belong to families that have a total of over 300 distinct applications?",
        "plan": "1.Extract information on patents that were applied for and granted in the United States in January 2008.\n2.Obtain the family IDs associated with these patents.\n3.Retrieve the total number of patents within these families.\n4.Keep the application IDs of patents where the family size exceeds 300 and count their total number.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq213",
        "db": "patents-public-data.patents",
        "question": "What is the most common 4-digit IPC code among US B2 utility patents granted in the first seven days of June 2018?",
        "plan": "1.Data Retrieval: Access patents-public-data.patents.publications, unnest IPC codes, and filter for US patents granted in April 2015 with a 'B2' kind code.\n2.IPC Analysis: Extract the first four characters of each IPC code, count their occurrences within each patent, and group by publication number and IPC subcategory.\n3.Determine Dominance: For each patent, identify the most frequent IPC subcategory using a combination of concatenation and maximum count comparison within the grouped data.\n4.Result Extraction: Order the results by the count of IPC occurrences and limit the output to the top entry to find the single most prevalent IPC subcategory.",
        "external_knowledge": "patents_info.md"
    },
    {
        "instance_id": "bq213_1",
        "db": "patents-public-data.patents",
        "question": "For US B2 utility patents granted in April 2015, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 20 or more times.",
        "plan": "",
        "external_knowledge": "patents_info.md"
    },
    {
        "instance_id": "bq214",
        "db": "patents-public-data.patents",
        "question": "For the 'B2' publication granted in January 2017 in the US that received the most forward citations within a month of its filing date, determine the publication number of the most similar patent from the same filing year.",
        "plan": "1.Extract Relevant Patent Data: Access the patents database to retrieve information for patents granted in the US on June 2nd, 2015, with the kind code B2, indicating they are granted utility patents.\n2.Determine the IPC Codes: For each patent, extract and count occurrences of four-digit IPC codes from the abstracts, identifying how frequently each IPC code appears within a given patent.\n3.Identify Backward Citations: For each selected patent, retrieve backward citations (i.e., earlier patents cited by the patent in question) and join these with the IPC data to analyze the diversity of IPC codes from these citations.\n4.Calculate Originality Score: Compute an originality score for each patent based on the diversity of IPC codes cited. This involves calculating a formula where the diversity is inversely related to the sum of the squares of IPC code occurrences.\n5.Select the Patent with the Highest Originality: From the calculated originality scores, identify the patent with the highest score, which indicates it has the broadest range of influences from prior art, suggesting a high level of innovation.\n6.Output the Result: Return the publication number of the patent with the highest originality score, highlighting it as the most original patent granted on that specific day.",
        "external_knowledge": "patents_info.md"
    },
    {
        "instance_id": "bq214_1",
        "db": "patents-public-data.patents",
        "question": "Identify the top five patents filed in the same year as `US-9023721-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.",
        "plan": "",
        "external_knowledge": "patents_info.md"
    },
    {
        "instance_id": "bq215",
        "db": "patents-public-data.patents",
        "question": "What is the publication number of US patent granted at January 2018, with the highest originality score based on the diversity of 4-digits IPC codes from its backward citations?",
        "plan": "",
        "external_knowledge": "patents_info.md"
    },
    {
        "instance_id": "bq221",
        "db": "patents-public-data.patents",
        "question": "Identify the CPC technology areas with the highest exponential moving average of patent filings each year (smoothing factor 0.2), and provide the full title and the best year for each CPC group at level 5.",
        "plan": "1. First, we define a temporary JavaScript function, which is used to calculate the year with the highest moving average of patent counts.\n2. Use common table expression to extract the CPC codes and filing years for patents, ensuring each patent has an application number and valid filing date. Also filter to include only the primary CPC code (`first = TRUE`).\n3. Calculate the most common patenting technology areas by year and identify the year with the highest moving average of patents for each CPC group. Concretely,\n- Aggregate patent counts by CPC group and filing year.\n- Use the defined function in Step 1. to find the year with the highest moving average of patents for each CPC group.\n- Join the results with the CPC definition table to get the full title for each CPC group. Remember, only include level 5 CPC groups.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq222",
        "db": "patents-public-data.patents",
        "question": "Find the CPC technology areas in Germany with the highest exponential moving average of patent filings each year (smoothing factor 0.1) for patents granted in December 2016. Show me the full title, CPC group and the best year for each CPC group at level 4.",
        "plan": "1. Firstly, we create a temporary function to calculate the highest moving average of patent counts.\n2. Use a common table expression to extract CPC codes and filing years for patents granted in Germany between 2016-12-01 to 2016-12-31, including only the primary CPC code.\n3. Calculate the highest moving average of patent counts for each CPC group, record the filing year information.\n4. Combine the results with CPC definitions to get the full title and also return the results with CPC group and filing year.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq223",
        "db": "patents-public-data.patents",
        "question": "Which assignee and primary CPC subclass full title most frequently cite patents assigned to 'AAAA'?",
        "plan": "1. Firstly, extract citing publication and cited publication details, including only the primary CPC code.\n2. Combine the citing and cited data according the publication number.\n3. Join with CPC definition information to extract CPC subclass title.\n4. Add filter to include only relevant citations.\n5. Group the results by citing assignee and CPC full title, count the citations and order the result by count.\nWe only return the most frequent assignee and CPC subclass title.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq052",
        "db": "patents-public-data.patentsview",
        "question": "I wonder which patents within CPC subsection 'C05' or group 'A01G' in the USA have at least one forward or backward citations within one month of their application dates. Give me the ids, titles, application date, forward/backward citation counts and summary texts.",
        "plan": "1. Identify Relevant Patents:\n   - Select patents in the USA that belong to specific CPC categories (either a particular subsection or group).\n\n2. Citation Filtering:\n   - Determine patents with at least one forward or backward citation within one month of their application dates. Count these citations separately for forward and backward directions.\n\n3. Data Aggregation:\n   - For each relevant patent, gather the patent ID, title, application date, forward and backward citation counts, and summary text.\n\n4. Combine and Order Results:\n   - Join the relevant patent data with citation counts and summary texts, and then order the results by the application date.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq207",
        "db": "patents-public-data.patents\npatents-public-data.uspto_peds",
        "question": "Can you provide the initial publication numbers for our top 100 independent patent claims with the highest word count?",
        "plan": "1.Select Independent Claims: Extract patent number, claim number, and word count for independent claims.\n2.Match Publication Numbers: Join with the match table to get the corresponding publication numbers.\n3.Match Application Numbers and Publication Information: Join with the publications table to get application details and select the earliest publication for each application.\n4.Select and Order Results: Retrieve all fields, order by word count in descending order, and limit to the top 100 records.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq036",
        "db": "bigquery-public-data.github_repos",
        "question": "What was the average number of GitHub commits made per month in 2020 for repositories containing Python code?",
        "plan": "1. Find out the table containing all commit info: `bigquery-public-data.github_repos.commits`\n2. Filter out the commit info in 2020, and for each commit-repo pair save its YearMonth timestamp.\n3. Find `bigquery-public-data.github_repos.languages` table, and range the data in commit - language entries.\n4. Combine them to find the commits which contain Python language and were made in  2020. Group the data according to YearMonth and calculate the number of commits for each month.\n5. Calculate the average monthly number of commits.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq100",
        "db": "bigquery-public-data.github_repos",
        "question": "Find out the most frequently used package in all Go source files.",
        "plan": "1. Extract Imports from File Contents (imports CTE):\n- Selects id and lines containing imports from files where the content matches the regex pattern r'import\\s*\\([^)]*\\)'.\n- SPLIT(REGEXP_EXTRACT(content, r'import\\s*\\(([^)]*)\\)'), '\\n') extracts the content inside the import(...) block and splits it into individual lines.\n2. Filter Go Files (go_files CTE):\n- Selects the id of files that have paths ending with .go (Go source files).\n- Uses GROUP BY id to ensure unique file IDs.\n3. Unnest Lines of Imports (filtered_imports CTE): Unnests the lines from the imports CTE to get each line as a separate row, resulting in rows with id and line.\n4. Join Imports with Go Files (joined_data CTE): Joins the filtered_imports with go_files on the id to filter only those import lines that belong to Go files.\n5. Extract the Package (SELECT statement):\n- Extracts the imported package using REGEXP_EXTRACT(line, r'\"([^\"]+)\"').\n- Filters out NULL string.\n- Groups the results by package, counts the occurrences, orders by the count in descending order, and limits the result to the most frequently imported package.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq101",
        "db": "bigquery-public-data.github_repos",
        "question": "Identify the top 10 most frequently imported packages and their counts in Java source files.",
        "plan": "1. Filter java source files: select Java files containing the \"import\" keyword.\n- condition: sample_path LIKE '%.java' to filter Java files.\n- condition: REGEXP_CONTAINS(content, r'import') to ensure the file contains \"import\" statements.\n2. Split the file content into lines and extract the import lines.\n3. Extract the package names with regex expression `r'([a-z0-9\\._]*)\\.'` to capture the package names.\n4. Count the number of imports for each package using GROUP BY clause.\n5. Order the results by count in descending order and limits to the top 10 packages.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq182",
        "db": "bigquery-public-data.github_repos\ngithubarchive.day",
        "question": "Which primary programming languages, determined by the highest number of bytes in each repository, have its repositories having at least a total of 100 pull requests on January 18, 2023?",
        "plan": "1. Extract event data from `githubarchive.day.20230118` db and get repo name from url.\n2. combine with the db `bigquery-public-data.github_repos.languages` to group the event data by the language of its repo.\n3. Filter to keep the data that has a type of 'PullRequestEvent' and a count of more than 100.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq182_1",
        "db": "bigquery-public-data.github_repos\ngithubarchive.day",
        "question": "How many pull requests in total were created in repositories that include JavaScript as one of their languages, considering data from January 18, 2023?",
        "plan": "1. **Subquery `a` - Extract and Transform Data from `githubarchive` Table:**\r\n   - **Select Fields:** Extract the `type`, the year (`y`), and the quarter (`q`) from the `created_at` column.\r\n   - **Regular Expression Replace:** Clean the `repo.url` to extract the repository name, removing specific URL patterns.\r\n   - **Data Source:** Pull data from the `githubarchive.day.20230118` table.\r\n\r\n2. **Subquery `b` - Filter and Aggregate Data from `github_repos.languages`:**\r\n   - **Nested Subquery:** \r\n     - **Unnest Languages:** Expand the `languages` array to individual rows.\r\n     - **Select Fields:** Extract `repo_name` and `language` details.\r\n   - **Filter Language:** Retain only rows where `language` is 'JavaScript'.\r\n   - **Group By:** Aggregate by `repo_name` and `language` to ensure unique combinations.\r\n\r\n3. **Join Subquery `a` and Subquery `b`:**\r\n   - **Join Condition:** Match `name` from subquery `a` with `name` from subquery `b`.\r\n\r\n4. **Filter by Event Type:**\r\n   - **Condition:** Retain only rows where `type` is 'PullRequestEvent'.\r\n\r\n5. **Count the Results:**\r\n   - **Aggregate Function:** Count the total number of resulting rows and alias the result as `total_pull_requests`.\r\n\r\n6. **Final Output:**\r\n   - **Select Statement:** Return the total count of pull request events for repositories where the language is 'JavaScript'.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq191",
        "db": "bigquery-public-data.github_repos\ngithubarchive.year",
        "question": "Find the top 5 repositories from 2017, which have more than 300 unique users watching them, that also have a `pom.xml` file containing the text `<artifactId>junit</artifactId>`. List these repositories in descending order of the watches they have.",
        "plan": "1. **Define CTE `repos`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct repository names from the `sample_files` table, assigning the result to `repo_in_mirror`.\r\n   - **Subquery `b`:**\r\n     - Select repository names and approximate count of distinct actors who performed 'WatchEvent' in 2017 from `githubarchive.year.2017`, filtering for repositories with more than 300 stars.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using `repo_in_mirror` and `repo_with_stars` to keep all repositories with stars from subquery `b`.\r\n     - Filter out rows where `repo_in_mirror` is NULL.\r\n   - **Result:**\r\n     - CTE `repos` contains repository names and their star counts for repositories with more than 300 stars and which exist in `sample_files`.\r\n\r\n2. **Define CTE `contents`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct entries from `sample_files` where the repository name exists in the `repos` CTE.\r\n   - **Subquery `b`:**\r\n     - Select content ID and content from `sample_contents`.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using the content ID.\r\n   - **Result:**\r\n     - CTE `contents` contains all content entries linked to repositories listed in the `repos` CTE.\r\n\r\n3. **Final Query:**\r\n   - **Join `repos` and `contents`:**\r\n     - Join `repos` with `contents` on `repo_name`.\r\n   - **Filter Results:**\r\n     - Filter for rows where the content contains the string '%junit</artifactId>%'.\r\n     - Filter for rows where the path is 'pom.xml'.\r\n   - **Order and Limit Results:**\r\n     - Order the results by the number of stars in descending order.\r\n     - Limit the output to the top 5 rows.\r\n\r\n4. **Output:**\r\n   - Select repository names and star counts from the filtered and joined results.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq224",
        "db": "bigquery-public-data.github_repos\ngithubarchive.month",
        "question": "Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?",
        "plan": "1. Filter only those repositories whose licenses are listed in the provided array of approved licenses.\n2. Select repository names and count distinct users who watched the repositories, that is to include only `WatchEvent` types.\n3. Similarly, select repository names and count the number of issues (`IssuesEvent`) and forks (`ForkEvent`).\n4. Combine the data and calculate the total counts of forks, issue events, and watches for each repository.\n5. Sort the results in descending order and only return the top repository name.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq192",
        "db": "bigquery-public-data.github_repos",
        "question": "Which repository that has a license of either \"artistic-2.0\", \"isc\", \"mit\", or \"apache-2.0\", contains Python files in the master branch, and has the highest combined count of forks, issues, and watch events in April 2022?",
        "plan": "1. **Define Allowed Repos:**\r\n   - Create a Common Table Expression (CTE) named `allowed_repos` to filter repositories that have specific licenses (`artistic-2.0`, `isc`, `mit`, `apache-2.0`).\r\n   - Select `repo_name` and `license` from the `bigquery-public-data.github_repos.licenses` table where the license matches one of the specified values.\r\n\r\n2. **Calculate Watch Counts:**\r\n   - Create a CTE named `watch_counts` to calculate the number of unique watch events per repository.\r\n   - Select the repository name and count the distinct `actor.login` from the `githubarchive.month.202204` table where the event type is `WatchEvent`.\r\n   - Group by repository name.\r\n\r\n3. **Calculate Issue Counts:**\r\n   - Create a CTE named `issue_counts` to calculate the number of issue events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `IssuesEvent`.\r\n   - Group by repository name.\r\n\r\n4. **Calculate Fork Counts:**\r\n   - Create a CTE named `fork_counts` to calculate the number of fork events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `ForkEvent`.\r\n   - Group by repository name.\r\n\r\n5. **Combine Metadata:**\r\n   - Create a CTE named `metadata` to combine the information from `allowed_repos`, `fork_counts`, `issue_counts`, and `watch_counts`.\r\n   - Perform INNER JOIN operations between `allowed_repos` and the other CTEs (`fork_counts`, `issue_counts`, and `watch_counts`) on `repo_name`.\r\n   - Select `repo_name`, `license`, `forks`, `issue_events`, and `watches` from the combined data.\r\n\r\n6. **Identify Repos with Python Files:**\r\n   - Create a CTE named `github_files_at_head` to identify repositories that have Python files at the head of the master branch.\r\n   - Select `repo_name` from the `bigquery-public-data.github_repos.sample_files` table where `ref` is `\"refs/heads/master\"`, the file path ends with `.py`, and `symlink_target` is `NULL`.\r\n   - Group by `repo_name`.\r\n\r\n7. **Select Top Repository:**\r\n   - Perform an INNER JOIN between `metadata` and `github_files_at_head` on `repo_name`.\r\n   - Select the `repo_name` from `metadata` as `repository`.\r\n   - Order the results by the sum of `forks`, `issue_events`, and `watches` in descending order.\r\n   - Limit the result to the top repository.\r\n\r\n8. **Return Result:**\r\n   - The final query returns the repository name of the top repository with the highest combined count of forks, issue events, and watches among those that have Python files at the head of the master branch and have an allowed license.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq225",
        "db": "bigquery-public-data.github_repos",
        "question": "What's the top 3 widely used languages according to file counts?",
        "plan": "1. Extract languages for each file according to file extensions.\n    - Determine the programming language based on the file extension extracted from the `path` field of the `files` table.\n    - Use the `REGEXP_EXTRACT` function to extract the file extension.\n    - Use a `CASE` statement to map file extensions to their respective languages based on the provided document `lang_and_ext.md`.\n2. Perform an inner join between the `languages` CTE and the `sample_contents` table on the `id` column. This join ensures that only files which have a corresponding content entry are considered.\n3. Filter out rows where `language` or `content` is `NULL`. This ensures that only valid and meaningful entries are included in the final result.\n4. Group and count by languages, and limit the final results to the top 3 entries.",
        "external_knowledge": "lang_and_ext.md"
    },
    {
        "instance_id": "bq180",
        "db": "bigquery-public-data.github_repos",
        "question": "Please help me retrieve the top 3 most frequently used module names from Python and R scripts.",
        "plan": "1. Get all the sample github file data and unnest the lines. Record the file path for each line.\n2. Extract the module names from the \"import\" and \"from\" statements of Python files, and the \"library(...)\" lines of R files.\n3. Count the number of occurences for each module and limit to the top 10 most used ones.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq180_1",
        "db": "bigquery-public-data.github_repos",
        "question": "Can you find the top 5 most frequently imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order.",
        "plan": "1. **Extract File Information:**\r\n   - Join `sample_files` and `sample_contents` tables on `id` to get `file_id`, `repo_name`, `path`, and `content`.\r\n   - Split `content` into individual lines, treating each line as a separate record.\r\n\r\n2. **Identify and Extract Module Imports:**\r\n   - Create a CTE (`extracted_modules`) to filter and process lines containing import statements for Python (`import` or `from`) and R (`library`).\r\n   - For Python files (`.py`):\r\n     - Extract modules imported using `import` and `from` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names and concatenate results.\r\n   - For R files (`.r`):\r\n     - Extract modules imported using `library()` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names.\r\n   - Store extracted modules along with file information and detected language (Python or R).\r\n\r\n3. **Count Module Occurrences:**\r\n   - Create another CTE (`module_counts`) to count occurrences of each module per language.\r\n   - Unnest the modules array to have one module per row.\r\n   - Group by `language` and `module` and count occurrences.\r\n\r\n4. **Select Top 5 Modules for Python:**\r\n   - Create a CTE (`top5_python`) to select the top 5 most frequently used Python modules.\r\n   - Filter `module_counts` for `python` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n5. **Select Top 5 Modules for R:**\r\n   - Create a CTE (`top5_r`) to select the top 5 most frequently used R modules.\r\n   - Filter `module_counts` for `r` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n6. **Combine Results and Order:**\r\n   - Use `UNION ALL` to combine results from `top5_python` and `top5_r`.\r\n   - Order the final results by `language` and `occurrence_count` in descending order to display the top modules for each language.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq193",
        "db": "bigquery-public-data.github_repos",
        "question": "Help me find the third most frequently occurring non-empty line of text in `requirements.txt` files from GitHub repositories that primarily use Python for development.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq295",
        "db": "bigquery-public-data.github_repos",
        "question": "Among the repositories from the GitHub Archive which include a Python file with less than 15,000 bytes in size and a keyword 'def' in the content, find the top 3 that have the highest number of watch events in 2017?",
        "plan": "1. **Common Table Expression (CTE) - `watched_repos`**:\n    - **Objective**: Extract repository names that have been watched.\n    - **Action**: \n        - Select `repo.name` as `repo` from the dataset `githubarchive.month.2017*` where the `type` is `\"WatchEvent\"`.\n\n2. **Common Table Expression (CTE) - `repo_watch_counts`**:\n    - **Objective**: Calculate the watch count for each repository.\n    - **Action**:\n        - Select `repo` and `COUNT(*)` as `watch_count` from the `watched_repos` CTE.\n        - Group the results by `repo` to aggregate watch counts.\n\n3. **Main Query - Data Source**:\n    - **Objective**: Fetch Python files with specific characteristics from GitHub repositories.\n    - **Action**:\n        - Select from `bigquery-public-data.github_repos.sample_files` as `f` and `bigquery-public-data.github_repos.sample_contents` as `c`.\n        - Join `f` and `c` on `f.id = c.id`.\n\n4. **Main Query - Additional Join**:\n    - **Objective**: Incorporate the watch counts into the main query.\n    - **Action**:\n        - Join the results from the previous step with `repo_watch_counts` as `r` on `f.repo_name = r.repo`.\n\n5. **Main Query - Filtering**:\n    - **Objective**: Filter the joined data to match specific criteria for Python files.\n    - **Action**:\n        - Apply a filter to select files where `f.path` ends with `.py`.\n        - Ensure the content size `c.size` is less than 15000 bytes.\n        - Check if the content `c.content` contains the pattern 'def ' using `REGEXP_CONTAINS`.\n\n6. **Main Query - Grouping and Ordering**:\n    - **Objective**: Prepare the final result set with grouping and sorting.\n    - **Action**:\n        - Group the results by `r.repo` and `r.watch_count`.\n        - Order the results by `r.watch_count` in descending order.\n\n7. **Main Query - Limiting Results**:\n    - **Objective**: Restrict the output to the top 3 repositories.\n    - **Action**:\n        - Use `LIMIT 3` to return only the top 3 repositories based on watch count.\n\n8. **Final Output**:\n    - **Objective**: Present the result.\n    - **Action**:\n        - Select and display the repository names and their corresponding watch counts.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq194",
        "db": "bigquery-public-data.github_repos",
        "question": "What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?",
        "plan": "1. **Define CTE `extracted_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `line`\r\n     - Determine `script_type` based on file extension:\r\n       - `.py` -> 'Python'\r\n       - `.r`, `.R`, `.Rmd`, `.rmd` -> 'R'\r\n       - `.ipynb` -> 'IPython'\r\n       - Others -> 'Others'\r\n     - Extract `modules` using regex based on `script_type`:\r\n       - For Python files, extract modules from `import` and `from` statements.\r\n       - For R files, extract modules from `library()` statements.\r\n       - For IPython notebooks, handle `import` and `from` statements within double quotes.\r\n   - **From Subquery:**\r\n     - Join `bigquery-public-data.github_repos.sample_files` and `bigquery-public-data.github_repos.sample_contents` on `id`.\r\n     - Select `file_id`, `repo_name`, `path`, and split `content` into `lines`.\r\n   - **Filter Lines:**\r\n     - For Python files, include lines containing `import` or `from ... import`.\r\n     - For R files, include lines containing `library()`.\r\n     - For IPython notebooks, include lines containing `import` or `from ... import` within double quotes.\r\n\r\n2. **Define CTE `unnested_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `script_type`, `module`\r\n   - **From `extracted_modules`:**\r\n     - Unnest `modules` to get individual `module` names.\r\n\r\n3. **Define CTE `module_frequencies`:**\r\n   - **Select Columns:**\r\n     - `module`\r\n     - `script_type`\r\n     - Count occurrences of each `module` and `script_type` combination as `frequency`\r\n   - **From `unnested_modules`:**\r\n     - Group by `module`, `script_type`\r\n     - Order by `frequency` in descending order.\r\n\r\n4. **Final Select:**\r\n   - **Select Column:**\r\n     - `module`\r\n   - **From `module_frequencies`:**\r\n     - Order by `frequency` in descending order.\r\n     - Limit result to the second most frequent module (using `LIMIT 1 OFFSET 1`).",
        "external_knowledge": null
    },
    {
        "instance_id": "bq019",
        "db": "bigquery-public-data.cms_medicare",
        "question": "For the most common inpatient diagnosis in the US in 2014, what was the citywise average payment respectively in the three cities that had the most cases?",
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n2. Rank all the diagnostic conditions by national number of cases to find the most common diagnostic condition.\n3. Group the data by diagnostic condition name, city and state, and calculate the citywise number of cases, citywise average payment for each entry.\n4. Accordingly, rank the cities by citywise number of cases for the most common diagnostic condition, and calculate the national avg. payments.\n5. Limit to the top 3 cities.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq019_1",
        "db": "bigquery-public-data.cms_medicare",
        "question": "What is the most prescribed medication in each state in 2014?",
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n2. Group the data by drug name and state, and calculate the total claim count for each drug in each state.\n3. For each state, find out the max claim count number among different drugs. (most prescribed)\n4. List out corresponding most prescribed drug name for each state. (don\u2018t need to be in order",
        "external_knowledge": null
    },
    {
        "instance_id": "bq019_2",
        "db": "bigquery-public-data.cms_medicare",
        "question": "Can you tell me which healthcare provider incurs the highest combined average costs for both outpatient and inpatient services in 2014?",
        "plan": "1. find out the corresponding table as described in the instruction `bigquery-public-data.cms_medicare.outpatient_charges_2014` and `bigquery-public-data.cms_medicare.inpatient_charges_2014`\r\n2. Group the outpatient charges data by different medical provider, and calculate the average outpatient cost per service.\r\n3. Do the same to inpatient charges data.\r\n4. Combine them to calculate the sum of two average costs for each medical provider.\r\n5. Rank them to find the peak. (the highest one)",
        "external_knowledge": null
    },
    {
        "instance_id": "bq172",
        "db": "bigquery-public-data.cms_medicare",
        "question": "For the drug with the highest total number of prescriptions in New York State during 2014, could you list the top five states with the highest total claim counts for this drug? Please also include their total claim counts and total drug costs. ",
        "plan": "1. **Identify the Top Drug in NY:**\r\n   - Create a Common Table Expression (CTE) named `ny_top_drug`.\r\n   - Select the `generic_name` of drugs as `drug_name` and calculate the total number of claims (`total_claim_count`) for each drug.\r\n   - Filter the data to include only records where the provider state (`nppes_provider_state`) is 'NY'.\r\n   - Group the results by `drug_name`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top drug (highest `total_claim_count`).\r\n\r\n2. **Identify the Top 5 States for the Top Drug:**\r\n   - Create a second CTE named `top_5_states`.\r\n   - Select the provider state (`nppes_provider_state`) as `state`, and calculate the total number of claims (`total_claim_count`) and total drug cost (`total_drug_cost`) for each state.\r\n   - Filter the data to include only records where the `generic_name` matches the top drug identified in `ny_top_drug`.\r\n   - Group the results by `state`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top 5 states with the highest `total_claim_count`.\r\n\r\n3. **Return the Final Results:**\r\n   - Select the `state`, `total_claim_count`, and `total_drug_cost` from the `top_5_states` CTE to produce the final output.\r\n\r\nThis sequence ensures that we first determine the most prescribed drug in New York, then find the top 5 states where this drug is most frequently prescribed, and finally, present the relevant statistics for these states.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq172_1",
        "db": "bigquery-public-data.cms_medicare",
        "question": "For the provider with the highest total inpatient service cost from 2011-2015, tell me its annual inpatient and outpatient costs for each year during that period.",
        "plan": "1. **Identify Provider with Highest Inpatient Service Cost (2011-2015)**\r\n   - **Combine Inpatient Data:** Use `UNION ALL` to merge inpatient charge data from 2011 through 2015.\r\n   - **Calculate Total Inpatient Cost:** For each provider, compute the total inpatient cost as the sum of `average_medicare_payments` multiplied by `total_discharges`.\r\n   - **Find Top Provider:** Group the results by `provider_id`, order them by `total_ip_cost` in descending order, and limit the output to the top provider.\r\n\r\n2. **Extract Provider ID with Highest Inpatient Cost**\r\n   - **Select Provider ID:** From the previous step, extract the `provider_id` of the provider with the highest total inpatient service cost.\r\n\r\n3. **Retrieve Annual Inpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Inpatient Data:** For the identified provider, fetch the inpatient data from `cms_medicare.inpatient_charges_*`.\r\n   - **Calculate Annual Average Inpatient Cost:** Compute the average inpatient cost per year by averaging `average_medicare_payments` multiplied by `total_discharges`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n4. **Retrieve Annual Outpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Outpatient Data:** For the identified provider, fetch the outpatient data from `cms_medicare.outpatient_charges_*`.\r\n   - **Calculate Annual Average Outpatient Cost:** Compute the average outpatient cost per year by averaging `average_total_payments` multiplied by `outpatient_services`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n5. **Join Inpatient and Outpatient Data**\r\n   - **Merge Inpatient and Outpatient Data:** Perform a LEFT JOIN on the inpatient and outpatient datasets based on `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n6. **Format and Order Final Results**\r\n   - **Select and Format Fields:** Select relevant fields including `State`, `City`, `Provider_ID`, `Provider_Name`, year, and round the average inpatient and outpatient costs.\r\n   - **Order by Year:** Sort the final results by year for chronological presentation.\r\n\r\nThis plan ensures the correct identification of the top provider based on inpatient costs and retrieves comprehensive annual cost data for both inpatient and outpatient services for detailed analysis.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq129",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.census_bureau_usa",
        "question": "What are the top 10 ZIP Code regions with populations over 1000 that have the highest number of Medicare-certified MD providers per capita?",
        "plan": "What are the ten zip codes with highest number of physicians per population (based on Medicare reimbursement data)?\nThe Centers for Medicare and Medicaid Services public dataset provides information on services and procedures provided to Medicare beneficiaries by physicians and other healthcare professionals in 2012. This query analyzes the number of physicians in each zip code (as listed in the Medicare data) compared to underlying population in each zip code according to the 2010 census data. ZCTAs are used in the census data to approximate zip codes.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq032",
        "db": "bigquery-public-data.noaa_hurricanes",
        "question": "Can you provide the latitude of the final coordinates for the hurricane that traveled the second longest distance in the North Atlantic during 2020?",
        "plan": "1. Select hurricane data for the 2020 season located in the North Atlantic (NA).\n2. Obtain position change information for each hurricane based on the time of movement.\n3. Calculate the total distance traveled by each hurricane.\n4. Rank hurricanes based on their total travel distances and select the hurricane that ranks second.\n5. Output the coordinate changes for the movement of this hurricane\n6. Only retain the last coordinate of this hurricane",
        "external_knowledge": null
    },
    {
        "instance_id": "bq032_1",
        "db": "bigquery-public-data.noaa_hurricanes",
        "question": "Please show information of the hurricane with the third longest total travel distance, including its travel coordinates, the cumulative travel distance at each point, and the maximum sustained wind speed at those times.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq117",
        "db": "bigquery-public-data.noaa_historic_severe_storms",
        "question": "What is the total number of severe storm events that occurred in the most affected month over the past 15 years according to NOAA records, considering only the top 100 storm events with the highest property damage?",
        "plan": "",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq071",
        "db": "bigquery-public-data.noaa_hurricanes\nbigquery-public-data.geo_us_boundaries",
        "question": "What are the top 10 zip codes of the areas in the United States that have been affected by the most named hurricanes?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq117_1",
        "db": "bigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.geo_us_boundaries.zip_codes",
        "question": "What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years?",
        "plan": "What zip codes have experienced the most hail storms in the last 10 years?\nThis query combines the severe weather events dataset with the zip code boundary data  available as a BigQuery Public Dataset to group hail events by zip code over the last 10 years.",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq181",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "How much percentage of weather stations recorded temperature data for at least 90% of the days in 2022?",
        "plan": "1. Calculate the total number of days in 2022 that it records the temperature data (not missing) for each station.\n2. Find out the total number of valid stations.\n3. Leverage the knowledge that 2022 has 365 days.\n4. Join the table with the stations table to find out the stations which records for at least 90% of days in 2022.\n5. Calculate the percentage of stations with over 90% records .",
        "external_knowledge": null
    },
    {
        "instance_id": "bq045",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Which weather stations in Washington State had more than 150 rainy days in 2023 but fewer rainy days than in 2022?",
        "plan": "1. Extract data for each weather station for the years 2023 and 2022, focusing on entries that record precipitation levels.\n2. Filter these records to retain only the days with precipitation greater than zero as rainy days.\n3. Merge the records from 2023 and 2022, retaining only those stations where 2023 had more than 150 rainy days and less precipitation than in 2022.\n4. Query station information to obtain the names of these weather stations.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq290",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Can you calculate the difference in maximum temperature, minimum temperature, and average temperature between US and UK weather stations for each day in October 2023, excluding records with missing temperature values?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq031",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Show me the daily weather data (temperature, precipitation, and wind speed) in Rochester for the first season of year 2019, converted to Celsius, centimeters, and meters per second, respectively. Also, include the moving averages (window size = 8) and the differences between the moving averages for up to 8 days prior (all values rounded to one decimal place, sorted by date in ascending order, and records starting from 2019-01-09).",
        "plan": "1. Data Transformation and Filtering:\n   - Convert the date components into a proper date format.\n   - Transform temperature from Fahrenheit to Celsius, precipitation from inches to centimeters, and wind speed from knots to meters per second.\n   - Filter the data to include only the records for the specified location and year.\n\n2. Calculation of Moving Averages:\n   - Calculate the moving averages for temperature, precipitation, and wind speed using a window size of 8 days.\n\n3. Lagging the Moving Averages:\n   - Create lagged versions of the moving averages for up to 8 days prior.\n\n4. Calculating Differences:\n   - Compute the differences between the current moving averages and their lagged versions for up to 8 days.\n\n5. Final Selection and Sorting:\n   - Select the relevant columns, round the values to one decimal place, ensure the lagged values are not null, and sort the results by date in ascending order.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq291",
        "db": "spider2-public-data.noaa_global_forecast_system",
        "question": "What is the daily weather forecast summary, including temperature stats (max, min and average), precipitation, cloud cover, snow and rain, for the specified location (latitude 17.5, longitude 23.25) and on November 28, 2021?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq208",
        "db": "bigquery-public-data.new_york\nbigquery-public-data.noaa_gsod",
        "question": "Can you provide weather stations within a 20-mile radius of Chappaqua, New York (Latitude: 41.197, Longitude: -73.764), and tell me the number of valid temperature observations they have recorded from 2011 to 2020?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq017",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "What are the five longest types of highways within the multipolygon boundary of Japan (as defined by Wikidata ID 'Q17') by total length?",
        "plan": "1.Select records identified by Wikidata ID Q17 to capture the geographic area of Japan, retrieving geometries of the 'multipolygons' feature type.\n2.Choose records of the 'lines' feature type, which typically represent highways and other linear geographic features, ensuring that the highway data's geometry intersects with the boundaries of Japan.\n3.Extract the type of each highway and calculate the length of these highways.\n4.Group the results by highway type and then order them by the total length of the highways in descending order.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq131",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of San Francisco (as defined by Wikidata ID 'Q62')?",
        "plan": "1. Retrieve the 'multipolygons' geometry for the area identified by Wikidata ID Q62, setting the boundaries for the query.\n2. Choose point features tagged as 'highway' with 'bus_stop' values, ensuring they are within the defined area.\n3. Extract the bus network information from the 'network' key for each stop, and count the number of stops per network.\n4. Group the results by bus network and order them by the descending count of stops to identify the network with the most stops.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq106",
        "db": "bigquery-public-data.geo_openstreetmap\nspider2-public-data.noaa_global_forecast_system",
        "question": "For the two-week forecast made on November 28th, 2021, how much did temperatures in Sudan deviate from the predicted value?",
        "plan": "1. Define the Boundary for Sudan:\n   - Create a Common Table Expression (CTE) to select geographic boundary data for Sudan.\n   - Filter the `planet_layers` table to include only rows where `layer_class` is 'boundary' and `layer_name` is 'national'.\n   - Use an `EXISTS` clause to ensure the boundary is for Sudan by checking the `ISO3166-1` tag in `all_tags`.\n\n2. Select Relevant Forecast Data:\n   - Create another CTE to select Global Forecast System (GFS) data from the `NOAA_GFS0P25` table.\n   - Filter the GFS data to include only those records created on '2021-11-28T00:00:00'.\n   - Ensure the selected forecast points are within the geographic boundary of Sudan using `ST_WITHIN`.\n\n3. Calculate Predicted Temperatures:\n   - Create a CTE named to calculate average temperatures predicted for each day within the two-week period.\n   - Join with the `forecast` array, and filter the forecast to include only daily data (`MOD(hours, 24) = 0`) within the first 14 days (`hours / 24 <= 14`).\n   - Group by `creation_time` and `time` to compute the average daily temperature.\n\n4. Fetch Observed Temperatures:\n   - Create a CTE named to compare predicted temperatures against observed temperatures.\n   - Join the predicted temperatures with the GFS data (`NOAA_GFS0P25` table) to find actual observations at the predicted times.\n   - Use `ST_WITHIN` to ensure observed points are within the Sudan boundary.\n   - Group the results to calculate the average observed temperature for each predicted time.\n\n5. Calculate Prediction Error and Return Query Execution:\n   - Select the deviation (error) between observed and predicted temperatures by subtraction.\n   - Sort the results by observed time.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq293",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.new_york",
        "question": "What were the top 5 busiest pickup times and locations (by ZIP code) for yellow taxi rides in New York City on January 1, 2015? Additionally, provide detailed metrics for each of these top 5 records, including the count of rides, hourly, daily, and weekly lagged counts, as well as 14-day and 21-day average and standard deviation of ride counts.",
        "plan": "1. **Define `base_data` CTE:**\r\n   - **Select Data:** Fetch data from the `bigquery-public-data.new_york.tlc_yellow_trips_2015` table for trips on '2015-01-01' where `pickup_latitude` is between -90 and 90.\r\n   - **Filter and Join:** Join the filtered taxi data (`nyc_taxi`) with the `bigquery-public-data.geo_us_boundaries.zip_codes` table for New York state (`state_code = 'NY'`), using spatial containment (`ST_CONTAINS`) to match pickups to zip codes, excluding the `zip_code_geom` field from the `gis` table.\r\n\r\n2. **Define `distinct_datetime` CTE:**\r\n   - **Extract Unique Hours:** Select distinct `pickup_datetime` truncated to the hour from `base_data`.\r\n\r\n3. **Define `distinct_zip_code` CTE:**\r\n   - **Extract Unique Zip Codes:** Select distinct `zip_code` from `base_data`.\r\n\r\n4. **Define `zip_code_datetime_join` CTE:**\r\n   - **Cartesian Product:** Perform a CROSS JOIN between `distinct_zip_code` and `distinct_datetime` to create combinations of all zip codes and hours.\r\n   - **Extract Date Parts:** Add columns to extract month, day, weekday, hour, and a flag (`is_weekend`) indicating if the day is a weekend.\r\n\r\n5. **Define `agg_data` CTE:**\r\n   - **Aggregate Data:** Aggregate `base_data` by `zip_code` and `pickup_hour`, counting the number of pickups per hour per zip code.\r\n\r\n6. **Define `join_output` CTE:**\r\n   - **Left Join Aggregated Data:** Perform a LEFT JOIN between `zip_code_datetime_join` and `agg_data` to include pickup counts (`cnt`), filling missing counts with 0 using `IFNULL`.\r\n\r\n7. **Define `final_output` CTE:**\r\n   - **Calculate Lag and Rolling Stats:** Compute lag values and rolling averages/std deviations over different time windows (1 hour, 1 day, 7 days, 14 days, 21 days) for each `zip_code` and `pickup_hour`.\r\n\r\n8. **Final Selection and Ordering:**\r\n   - **Select and Limit:** Select all columns from `final_output`, order by `cnt` in descending order, and limit the result to the top 5 records.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq056",
        "db": "bigquery-public-data.geo_openstreetmap\nbigquery-public-data.geo_us_boundaries",
        "question": "How many different motorway road pairs are there in Minnesota that overlap each other but do not share nodes and do not have a bridge?",
        "plan": "1. Filter by State and Road Type: Identify the specified state and road type from the geographical boundaries and road data tables.\n\n2. Select Relevant Roads: Extract unique identifiers for roads that match the specified type (e.g., motorway) from the road data.\n\n3. Identify Roads Within State: Match the roads with the geographical boundary of the specified state to ensure only roads within the state are considered.\n\n4. Find Overlapping Roads: Determine pairs of roads that geographically overlap each other but do not share nodes.\n\n5. Exclude Roads with Bridges: Filter out road pairs where either road has a bridge attribute.\n\n6. Count Overlapping Pairs: Count the number of unique road pairs that overlap but do not share nodes and do not have a bridge.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq184",
        "db": "bigquery-public-data.crypto_ethereum",
        "question": "I want to compute and compare the cumulative count of Ethereum smart contracts created by users versus created by other contracts. Please list out the daily cumulative tallies in August 2021.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq195",
        "db": "spider2-public-data.crypto_ethereum",
        "question": "What are the top 10 Ethereum addresses by balance, considering both value transactions and gas fees, before September 1, 2021?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq186",
        "db": "bigquery-public-data.san_francisco.bikeshare_trips",
        "question": "Please help me calculate the first, last, highest, and lowest bike trip durations in minutes for each month.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq339",
        "db": "bigquery-public-data.san_francisco_bikeshare",
        "question": "Which month in 2017 had the largest absolute difference between cumulative bike usage minutes for customers and subscribers?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq059",
        "db": "bigquery-public-data.san_francisco_bikeshare",
        "question": "What is the highest average speed (rounded to 1 decimal, in metric m/s) for bike trips in Berkeley with trip distance greater than 1000 meters?",
        "plan": "1. Identify Relevant Locations: Select station identifiers for bike stations located in a specific region (Berkeley) by filtering based on the region's name.\n\n2. Calculate Distances and Speeds: For trips starting and ending at the identified stations, calculate the trip distance and the average speed. Ensure the coordinates for start and end stations are available.\n\n3. Filter Trips: Only include trips that meet the distance requirement (greater than 1000 meters) and start/end station restriction (within the region Berkeley).\n\n4. Find Maximum Speed: From the filtered trips, determine the highest average speed, rounded to one decimal place, and return this value.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq006",
        "db": "bigquery-public-data.austin_incidents",
        "question": "What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.",
        "plan": "Analyze daily occurrences of public intoxication incidents in Austin for the year 2016. \nCalculate the total number of incidents per day and compute the Z-score for each day's incidents to identify days with significantly higher or lower incident counts. \nThe output should include the date, total number of incidents, and the Z-score.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq187",
        "db": "bigquery-public-data.ethereum_blockchain",
        "question": "What is the total circulating supply balances of the 'BNB' token for all addresses (excluding the zero address), based on the amount they have received (converted by dividing by 10^18) minus the amount they have sent?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq014",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?",
        "plan": "1. Get the first order of each user, excluding the cancelled and returned orders. (from 'orders' table)\n2. Calculate the total revenue and distinct user count for each product category. (combine the 'orders' 'order_items' and 'products' tables)\n3. Select the top category with the most user count.\n4. Based on the top category, get its revenue.",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq188",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average time in minutes that users spend per visit on the product category with the highest total quantity purchased?",
        "plan": "",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq189",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average monthly revenue growth rate for the product category with the highest average monthly order growth rate based on completed orders?",
        "plan": "1. Calculate the total sale prices and total order amount (TPV) for each product category (TPO) every month.\n2. Get the TPV and TPO in the previous month as the 'Lagged_TPV' and 'Lagged_TPO'.\n3. Calculate the revenue growth rate and the order growth rate for each product monthly.\n4. Find out the product category with the max average order growth rate.\n5. Calculate the average revenue growth rate for that product category.",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq262",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Help me generate a monthly analysis report on e-commerce sales in the second half of 2019, which should contain the total sum of order count/revenue/profit as well as their growth rates for each product category monthly. Please sort the results by months (e.g., 2019-07) and product categories in ascending order.",
        "plan": "1. Filter and Aggregate Orders Data:\n   - Objective: Gather monthly order data for each product.\n   - Action: Create a CTE `orders_data` to:\n     - Extract the year-month from `created_at` timestamp in the `orders` table.\n     - Count the total orders and sum the sales price for each product per month.\n     - Filter orders within the months from 2019-06 to 2019-12.\n\n2. Fetch Product Category and Cost:\n   - Objective: Associate each product with its category and cost.\n   - Action: Create a CTE `product_data` to:\n     - Select product ID, category, and cost from the `products` table.\n\n3. Calculate Monthly Metrics:\n   - Objective: Compute monthly order count, total revenue, and total profit for each product category.\n   - Action: Create a CTE `monthly_metrics` to:\n     - Join `orders_data` with `product_data` on product ID.\n     - Aggregate data by month and product category.\n     - Calculate total orders, total revenue, and total profit (revenue minus cost of goods sold).\n\n4. Calculate Growth Rates:\n   - Objective: Determine month-over-month growth rates for order count, revenue, and profit.\n   - Action: Create a CTE `growth_metrics` to:\n     - Use window functions (`LAG`) to calculate the previous month's values for each metric within the same product category.\n     - Compute growth rates for orders, revenue, and profit as percentages.\n\n5. Generate Final Report:\n   - Objective: Present the monthly analysis report with growth rates.\n   - Action: Select all columns from `growth_metrics` except the month `2019-06`.\n     - Order the result by month and product category for organized viewing.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq190",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the count of the youngest and oldest users respectively, broken down by gender from January 2019 to April 2022?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq197",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What are the top-selling products by sales volume and revenue for June 2024 and each month before, considering only completed orders?",
        "plan": "",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq020_1",
        "db": "bigquery-public-data.genomics_cannabis",
        "question": "What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq020_2",
        "db": "bigquery-public-data.genomics_cannabis",
        "question": "What is the variant density of the cannabis reference with the longest reference length?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq025",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.",
        "plan": "1. **Define the Main Query Objective**:\r\n   - Retrieve and analyze the population data for countries, specifically focusing on the population under the age of 20 and its percentage relative to the total population.\r\n\r\n2. **Subquery 1 (Alias `age`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n     - `age < 20`: Only include population data for individuals under the age of 20.\r\n   - **Selected Columns**:\r\n     - `country_name`: Name of the country.\r\n     - `population`: Population of the specific age group.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n3. **Subquery 2 (Alias `pop`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n   - **Selected Columns**:\r\n     - `midyear_population`: Total midyear population for the country.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n4. **Join Operation**:\r\n   - **Type**: INNER JOIN\r\n   - **Condition**: `age.country_code = pop.country_code`\r\n   - **Purpose**: Combine age-specific population data with total population data for each country based on matching country codes.\r\n\r\n5. **Aggregation and Calculations**:\r\n   - **Grouped By**: \r\n     - `age.country_name` (Column index 1 in the SELECT clause)\r\n     - `pop.midyear_population` (Column index 3 in the SELECT clause)\r\n   - **Aggregations**:\r\n     - `SUM(age.population) AS under_25`: Calculate the total population under the age of 20 for each country.\r\n     - `ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25`: Calculate and round to two decimal places the percentage of the population under 20 relative to the total midyear population for each country.\r\n\r\n6. **Ordering**:\r\n   - **Order By**: `pct_under_25 DESC` (Column index 4 in the SELECT clause)\r\n   - **Purpose**: Sort the results in descending order based on the percentage of the population under 20.\r\n\r\n7. **Limit**:\r\n   - **Limit**: `10`\r\n   - **Purpose**: Restrict the result set to the top 10 countries based on the sorted percentage of the population under 20.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq025_1",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which country has the highest percentage of population under the age of 25 in 2017?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq338",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.geo_census_tracts",
        "question": "Which Brooklyn census tract (by geo ID) had the largest increase in median income from 2010 to 2017?",
        "plan": "What is the difference in median income in Brooklyn by Census tract from 2010 to 2017?\nFind how the median household changed for each Census tract in Brooklyn between 2010 and 2017. See the visualization below for more details",
        "external_knowledge": null
    },
    {
        "instance_id": "bq088",
        "db": "bigquery-public-data.covid19_symptom_search",
        "question": "Can you provide the average levels of anxiety and depression symptoms in the United States for the years 2019 and 2020, and calculate the percentage increase in these symptoms from 2019 to 2020?",
        "plan": "1. **Data Source Selection**:\r\n   - The query utilizes the `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly` dataset from BigQuery's public data.\r\n\r\n2. **Subquery for 2020 Data**:\r\n   - A subquery (`table_2020`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2020.\r\n   - The `WHERE` clause filters the records to include only those from the US (`country_region_code = \"US\"`) and within the date range from January 1, 2020, to December 31, 2020.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n3. **Subquery for 2019 Data**:\r\n   - Another subquery (`table_2019`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2019.\r\n   - Similar to the 2020 subquery, the `WHERE` clause filters the records to include only those from the US and within the date range from January 1, 2019, to December 31, 2019.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n4. **Main Query**:\r\n   - The main query selects the average values calculated in the subqueries:\r\n     - `table_2019.avg_symptom_Anxiety_2019`\r\n     - `table_2020.avg_symptom_Anxiety_2020`\r\n     - `table_2019.avg_symptom_Depression_2019`\r\n     - `table_2020.avg_symptom_Depression_2020`\r\n   - Additionally, it calculates the percentage increase in anxiety and depression symptoms from 2019 to 2020 using the formula:\r\n     - `((table_2020.avg_symptom_Anxiety_2020 - table_2019.avg_symptom_Anxiety_2019)/table_2019.avg_symptom_Anxiety_2019) * 100` as `percent_increase_anxiety`\r\n     - `((table_2020.avg_symptom_Depression_2020 - table_2019.avg_symptom_Depression_2019)/table_2019.avg_symptom_Depression_2019) * 100` as `percent_increase_depression`\r\n\r\n5. **Output**:\r\n   - The final output includes the average levels of anxiety and depression for both 2019 and 2020, along with the calculated percentage increases for both symptoms between the two years.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq137",
        "db": "bigquery-public-data.census_bureau_usa\nbigquery-public-data.utility_us",
        "question": "Find details about zip code areas within 10 kilometers of the coordinates (-122.3321, 47.6062), including their geographic polygons, land and water area in meters, latitude and longitude points, state code, state name, city, county, and population from the 2010 census data.",
        "plan": "1. **Common Table Expression (CTE) - `zip_pop` Creation**:\r\n   - Query `bigquery-public-data.census_bureau_usa.population_by_zip_2010` table.\r\n   - Select `zipcode` and `population` columns.\r\n   - Filter rows where `gender` is either 'male' or 'female'.\r\n   - Ensure `minimum_age` and `maximum_age` are `NULL` (considering the entire population without age restrictions).\r\n\r\n2. **Main Query - Data Selection and Joining**:\r\n   - Query `bigquery-public-data.utility_us.zipcode_area` table.\r\n   - Select the following columns:\r\n     - `zipcode_geom` as `zipcode_polygon` (geometry of the zip code area),\r\n     - `zipcode` (zip code),\r\n     - `area_land_meters` (land area in square meters),\r\n     - `area_water_meters` (water area in square meters),\r\n     - `ST_GeogPoint(longitude, latitude)` as `lat_lon` (geographical point of the zip code),\r\n     - `state_code` (state abbreviation),\r\n     - `state_name` (full state name),\r\n     - `city` (city name),\r\n     - `county` (county name),\r\n     - `population` (population data from the CTE).\r\n\r\n3. **Join Operation**:\r\n   - Perform an inner join between `zip_area` and `zip_pop` on the `zipcode` column to combine area and population data.\r\n\r\n4. **Distance Filtering**:\r\n   - Filter results where the geographical point of the zip code (`ST_GeogPoint(longitude, latitude)`) is within 10,000 meters (10 kilometers) of the point (-122.3321, 47.6062) using the `ST_DWITHIN` function.\r\n\r\n5. **Final Output**:\r\n   - The final result includes data about zip code areas, such as their geographical polygons, land and water areas, geographical points, and associated population information, filtered to only include zip codes within 10 kilometers of the specified coordinates in Seattle, WA.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq023",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.fec\nbigquery-public-data.hud_zipcode_crosswalk\nbigquery-public-data.geo_census_tracts",
        "question": "What are the average political donation amounts and median incomes for each census tract identifier in Kings County (Brooklyn), NY, using data from the 2018 ACS and 2020 FEC contributions?",
        "plan": "1. Extract Median Income Data:\n    - Retrieve the median income for each geographical area from the census data.\n\n2. Filter Political Donations by State:\n    - Select donation amounts and corresponding ZIP codes for a specific state from political contribution records.\n\n3. Map ZIP Codes to Census Tracts:\n    - Create a mapping between ZIP codes and census tracts using a crosswalk table.\n\n4. Calculate Average Donations per Census Tract:\n    - Aggregate the donation amounts by census tract, using the mapping from ZIP codes to census tracts, to compute the average donation per tract.\n\n5. Identify Census Tracts in Target County:\n    - Select the census tracts that belong to the specific county of interest, filtering by county and state codes.\n\n6. Combine Data:\n    - Merge the census tract information with the computed average donations and the median income data, ensuring that all tracts in the target county are included, even if there are no corresponding donations or income records.\n\n7. Format and Order Results:\n    - Select the relevant fields for the final output, specifically the census tract identifier, average donation amount, and median income. Sort the results by the census tract identifier for orderly presentation.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq060",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?",
        "plan": "1. **Filter and Select Data for 2017**: First, select the relevant columns (country name, net migration, and country code) from the dataset containing information about growth rates for the year 2017.\n\n2. **Filter and Join on Country Area**: Next, filter another dataset to include only countries with an area greater than 500 square kilometers. Join this filtered dataset with the previous one on the country code to combine the net migration data with the area data.\n\n3. **Sort and Retrieve Top Country**: Sort the combined data by net migration in descending order and limit the result to the top 3 entry. Finally, select the country name and net migration fields from this result to identify the country with the highest net migration.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq061",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.geo_census_tracts",
        "question": "Which census tract has witnessed the largest increase in median income between 2015 and 2018 in California? Tell me the tract code.",
        "plan": "1. **Extract Data for 2018**: Create a temporary dataset containing the geographical IDs and median incomes for the year 2018.\n\n2. **Extract Data for 2015**: Create another temporary dataset containing the geographical IDs and median incomes for the year 2015.\n\n3. **Calculate Income Difference**: Join the 2018 and 2015 datasets based on geographical IDs and calculate the difference in median income between these two years for each geographical ID.\n\n4. **Identify Maximum Increase**: From the dataset of income differences, filter out records that are not relevant to California and then identify the geographical ID with the highest increase in median income.\n\n5. **Retrieve Tract Code**: Join the geographical ID with the highest income increase back to the dataset containing California tract codes to retrieve the specific tract code associated with that geographical ID.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq198",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "What are the top 5 most successful college basketball teams over the seasons from 1900 to 2000, based on the number of times they had the maximum wins in a season?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq016",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the highest release versions of PYPI packages, which one and its version has the most dependent packages?",
        "plan": "1. Firstly, we declare a system variable to denote the PYPI system.\n2. Identify the highest released versions for each package.\n   - Use a subquery to:\n     - Partition data by `Name`.\n     - Order the partitions by `VersionInfo.Ordinal` in descending order to rank versions.\n     - Assign row numbers to each version within their respective partitions.\n   - Filter to include only the first row (highest version) for each package where:\n     - The `System` is 'PYPI' (using the variable `Sys`).\n     - The `VersionInfo.IsRelease` is true (indicating it is a release version).\n3. Join the table Dependencies with the table defined in Step 2.\n4. Aggregate and order dependencies by name and version.\n5. Restrict the output to only one record, which is the dependency with the highest count of dependent packages.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq062",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the most frequently used license by packages in each system?",
        "plan": "1. Generate a Common Table Expression (CTE) that aggregates the number of distinct packages using each license within each system.\n- Use `CROSS JOIN UNNEST` to handle the array of licenses, effectively normalizing the data for easier counting.\n- Group the results by `System` and `License`.\n2. Generate another CTE that ranks the licenses within each system based on the number of distinct packages using them.\n- Use the `ROW_NUMBER()` window function partitioned by `System` and ordered by `NPackages` in descending order to assign a rank to each license within each system.\n3. Select and output only top ranked licenses for each system.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq063",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the github URL of the latest released package from the NPM system that have the highest number of dependencies? Exlcude those package whose names contain character '@' or the URL label is not 'SOURCE_REPO'.",
        "plan": "1. Initialize a variable `Sys` with the value 'NPM' to filter data related to the NPM system.\n2. Create a Common Table Expression (CTE) to identify the latest released version of each package in the NPM system.\n- Select `Name` and `Version` from the `PackageVersions` table.\n- Use `ROW_NUMBER` to assign a unique sequential integer to rows within a partition of the dataset, partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order.\n- Filter to include only rows where `VersionInfo.IsRelease` is true.\n- Retrieve only the rows where `RowNumber` equals 1, ensuring only the latest released version is selected for each package.\n3. Create another CTE to find packages with the highest number of dependencies.\n- Select `Name`, `Version`, and the count of dependencies from the `Dependencies` table after joining it with the table created in Step 2.\n- Order the results by the count of dependencies (`NDependencies`) in descending order.\n4. Select the `URL` from the `PackageVersions` table, unnesting the `Links` array to access individual URLs.\n- Filter to include only rows where:\n   - `System` equals `Sys`.\n   - `Name` does not contain the character '@'.\n   - `Label` is 'SOURCE_REPO'.\n   - `lnk.URL` contains 'github.com'.\n5. Limit the result to the top 1, which gives the GitHub URL of the package with the highest number of dependencies among the latest releases.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq028",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the latest release versions of NPM package, which packages are the top 3 most popular based on the Github star number, as well as their versions?",
        "plan": "1. Declare system variable: define a variable `Sys` to filter records for the NPM system.\n2. Identify latest release versions:\n- Create a common table expression (CTE) to determine the latest release version for each package.\n- Use `ROW_NUMBER()` window function partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order to rank the versions.\n- Filter to keep only the top-ranked version (`RowNumber = 1`) which represents the latest release.\n3. Join latest versions with projects to extract the project name and type and constrain the project type to GITHUB.\n4. Select package details and rank by the number of github stars count:\n5. Limit the final result to the top 3 packages based on the number of GitHub stars and return the package names and versions.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq022",
        "db": "bigquery-public-data.chicago_taxi_trips",
        "question": "Given the taxi trip data in Chicago, tell me the total number of trips and average fare for each 10-minute interval (using rounded-off minutes) of trip durations no more than 1 hour.",
        "plan": "Step 1: Filter and Calculate Duration\n- First, filter and calculate duration. Only include trips where the duration is less than 1 hour (=3600 seconds).\n- Calculate duration and quantiles. Calculate the duration of each trip in minutes and divide the trips into 6 quantiles based on their duration using `NTILE(6) OVER (ORDER BY trip_seconds / 60)`.\n- Aggregate trips and fare. For each duration, count the number of trips and sum the total fare. Then, group the results by `trip_seconds` and `duration_in_minutes`.\nStep 2: Calculate Min and Max Duration for Each Quantile\n- For each quantile, calculate the minimum and maximum duration in minutes using window functions.\n- Sum the trips and total fare for each quantile.\nStep 3: Final Aggregation and Formatting\n- Format the minimum and maximum duration for each quantile into a string representing the range (e.g., \"01m to 10m\").\n- Sum the total trips and calculate the average fare per trip for each duration range.\n- Group and order the results by the formatted duration range",
        "external_knowledge": null
    },
    {
        "instance_id": "bq076",
        "db": "bigquery-public-data.chicago_crime",
        "question": "Which month generally has the greatest number of motor vehicle thefts in 2016?",
        "plan": "Which month generally has the greatest number of motor vehicle thefts?\nThe following query summarizes the number of MOTOR VEHICLE THEFT incidents for each year and month, and ranks the month\u2019s total from 1 to 12. Then, the outer SELECT clause limits the final result set to the first overall ranking for each year. According to the data, in 3 of the past 10 years, December had the highest number of car thefts",
        "external_knowledge": null
    },
    {
        "instance_id": "bq077",
        "db": "bigquery-public-data.chicago_crime",
        "question": "For each year from 2010 to 2016, what is the highest number of motor thefts in one month?",
        "plan": "1. Filters the crime dataset for motor vehicle theft incidents between 2010 and 2016.\n2. Extracts the month from the date of each incident and groups the data by year and month.\n3. Counts the number of incidents for each year-month combination and ranks the months within each year based on the number of incidents.\n4. Selects the highest number of incidents for each year by filtering for ranking = 1 and orders the final results by year in ascending order.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq350",
        "db": "open-targets-prod.platform",
        "question": "Please display the drug id, drug type and withdrawal status for approved drugs with a black box warning and known drug type among 'Keytruda', 'Vioxx', 'Premarin', and 'Humira'",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq351_1",
        "db": "open-targets-prod.platform",
        "question": "Which target approved symbol has the overall association score closest to the mean score for psoriasis?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq109",
        "db": "open-targets-genetics.genetics",
        "question": "Find the average, variance, max-min difference, and the QTL source(right study) of the maximum log2(h4/h3) for data where right gene id is \"ENSG00000169174\", h4 > 0.8, h3 < 0.02, reported trait includes \"lesterol levels\", right biological feature is \"IPSC\", and the variant is '1_55029009_C_T'.",
        "plan": "For a given gene, what studies have evidence of colocalisation with molecular QTLs?\n\nWith our variant_disease_coloc and studies, you can find associated GWAS studies with evidence of colocalisation and an H4 greater than 0.8",
        "external_knowledge": null
    },
    {
        "instance_id": "bq352",
        "db": "bigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.census_bureau_acs",
        "question": "Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.",
        "plan": "Is there an association between high commute times [from ACS] and average number of prenatal visits in Wisconsin by County?\nThis query examines the potential correlation between the average number of prenatal visits and length of commutes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq041",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the monthly statistics for new StackOverflow users created in 2021, including the percentage of new users who asked questions and the percentage of those who asked questions and then answered questions within their first 30 days?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq123",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Which day of the week has the third highest percentage of questions answered within an hour?",
        "plan": "Which day of the week has most questions answered within an hour?\nIn this query, we find the best day of the week to ask questions to get an answer very quickly. The query returns day of the week as integers from 1 to 7 (1 = Sunday, 2 = Monday, etc), and the number of questions and answers on each day. We also query how many of these questions received an answer within 1 hour of submission, and the corresponding percentage. The volume of questions and answers is the highest in the middle of the week (Tue, Wed, and Thur), but questions are answered within 1 hour is higher on Saturdays and Sundays",
        "temporal": "Yes",
        "external_knowledge": null
    },
    {
        "instance_id": "bq126",
        "db": "bigquery-public-data.the_met",
        "question": "What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created by known artists, with an object end date of 1839 or earlier?",
        "plan": "What are the earliest photographs in the collection?\nThe Met\u2019s Department of Photographs has some of the earliest photographic works. This query retrieves artwork from the \u201cPhotograph\u201d department and joins the object and image tables to return the results, presented with the oldest items first. In this case, we see the British photography pioneer William Henry Fox Talbot\u2019s botanical images.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq042",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Help me analyze the weather conditions (including temperature, wind speed and precipitation) at NYC's airport LaGuardia for June 12, year over year, starting from 2011 to 2020.",
        "plan": "1. Filter Data for Specific Dates and Location: Extract the records for the specified date (June 12) across the given range of years (2011-2020) and for the specific location (LaGuardia airport).\n\n2. Handle Missing or Invalid Data: Replace any placeholder values indicating missing data with `NULL` for temperature and wind speed, and with `0` for precipitation.\n\n3. Calculate Averages: Compute the average values for temperature, wind speed, and precipitation on June 12 for each year.\n\n4. Organize Results: Create a timestamp for each record, group the results by this timestamp, and sort the output in ascending order of the timestamp for clear year-over-year analysis.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq047",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Could you help me analyze the relationship between each complaint type and daily temperature in New York city, focusing on data in airports LaGuardia and JFK over the 10 years starting from 2008? Calculate the total complaint count, the total day count, and the Pearson correlation coefficient (rounded to 4 decimals) between temperature and both the count and percentage of each common (>5000 occurrences) and strongly correlated (absolute value > 0.5) complaint type.",
        "plan": "1. Temperature Data Preparation:\n    - Extract and clean weather data for the specified 10-year period, focusing on the relevant locations.\n    - Transform date components into a single timestamp and handle any placeholder values for temperature.\n    - Calculate the average temperature for each day from the cleaned weather data.\n\n2. Prepare Complaint Data:\n    - Extract and aggregate complaint data by type and date, counting the number of complaints per day for each type.\n    - Calculate the daily proportion of each complaint type relative to the total complaints on that day.\n\n3. Join Weather and Complaint Data:\n    - Combine the weather data with the complaint data based on matching dates.\n    - Aggregate this joined data by complaint type and temperature to compute the average daily counts and proportions of each complaint.\n\n4. Analyze and Filter Results:\n    - Compute the correlation between temperature and both the average daily count and proportion of each complaint type using function `CORR`.\n    - Filter the results to include only those complaint types that are both common (more than 5000 total occurrences) and strongly correlated (with absolute correlation value greater than 0.5).\n    - Sort the results by the strength of the correlation.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq048",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Which common complaint types have the strongest positive and negative correlation with wind speed respectively, given the data in NYC JFK Airport from year 2011 to 2020? Also, provide the corresponding correlation values (rounded to 4 decimals).",
        "plan": "1. Aggregate Wind Speed Data: Create a dataset with daily average wind speed values for a specific location and time range, replacing erroneous values with nulls.\n\n2. Compute Daily Complaint Data: Generate daily complaint counts and their proportions relative to all complaints for different complaint types over the same time range.\n\n3. Join Weather and Complaint Data: Merge the weather dataset with the complaint dataset on their date fields to align daily wind speed data with daily complaint data.\n\n4. Calculate Correlations: For each complaint type, calculate the correlation between the average daily wind speed and the average daily complaint count, filtering out complaint types with insufficient data.\n\n5. Identify Extremes: Select the complaint types with the strongest positive and negative correlations with wind speed, and report these types along with their correlation values.",
        "external_knowledge": null
    },
    {
        "instance_id": "bq329",
        "db": "bigquery-public-data.austin_bikeshare",
        "question": "Which bike station in the Austin bikeshare system has the lowest average trip duration? I want the station ID.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq320",
        "db": "bigquery-public-data.idc_v11",
        "question": "What is the total count of StudyInstanceUIDs that have a segmented property type of '80891009' and belong to the 'Community' or 'nsclc_radiomics' collections?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq321",
        "db": "bigquery-public-data.idc_v14",
        "question": "How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq325",
        "db": "bigquery-public-data.open_targets_genetics",
        "question": "What are the top 10 genes with the lowest p-values across studies?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "bq326",
        "db": "bigquery-public-data.world_bank_health_population",
        "question": "How many countries experienced both a change in population and a change in Current health expenditure per capita, PPP (current international $) of greater than 1 in the year 2018?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga001",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was most frequently purchased alongside this item?",
        "plan": "1. Focus on the item named \"Google Navy Speckled Tee.\"\n2. Select all purchase-type events from December 2020.\n3. Extract the IDs of individuals who purchased the \"Google Navy Speckled Tee\" during these events.\n4. Calculate all items purchased by these IDs and retain the top 10 items by purchase volume.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga001_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Tell me the most purchased other products and their quantities by customers who bought the Google Red Speckled Tee each month for the three months starting from November 2020.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga003",
        "db": "firebase-public-project.analytics_153293282",
        "question": "I'm trying to evaluate which board types were most effective on September 15, 2018. Can you find out the average scores for each board type from the quick play mode completions on that day?\"",
        "plan": "1. Extract all data for the \"level_complete_quickplay\" mode.\n2. Focus on the board type, grouping by user ID and event timestamp to summarize board type and corresponding scores.\n3. Calculate the average score for each board type.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga004",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you figure out the average difference in pageviews between users who bought something and those who didn\u2019t in December 2020? Just label anyone who was involved in purchase events as a purchaser.",
        "plan": "1. Segment user activities into page views and purchase events for December 2020.\n2. Classify users based on whether they made any purchases.\n3. Calculate average page views for purchasers and non-purchasers.\n4. Determine the difference in average page views between these two groups.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga004_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me the average page views per buyer and total page views for each day in November 2020?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga017",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many distinct users viewed the most frequently visited page during January 2021?",
        "plan": "1. Extract `page_view` events for January 2021 and unnest their parameters to access individual event details.\n2. Aggregate these details to identify the title of each page viewed, grouping by user and event timestamp.\n3. Count occurrences and distinct users per page, then order by the frequency of visits to each page.\n4. Select the number of distinct users for the top visited page.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga007",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please find out what percentage of the page views on January 2, 2021, were for PDP type pages.",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Calculate the total proportion of PDP",
        "external_knowledge": "ga4_page_category.md"
    },
    {
        "instance_id": "ga007_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know all the pages visited by user 1402138.5184246691 on January 2, 2021. Please show the names of these pages and adjust the names to PDP or PLP where necessary.",
        "plan": "",
        "external_knowledge": "ga4_page_category.md"
    },
    {
        "instance_id": "ga018",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I'd like to analyze the appeal of our products to users. Can you calculate the percentage of times users go from browsing the product list pages to clicking into the product detail pages at January 2nd, 2021?",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Applies window functions to the categorized data to calculate the previous and next pages for each session per user, facilitating analysis of navigation paths between pages.\n7. Filters sessions where the current page is a PLP and the next page is a PDP.\n8. Counts the number of sessions transitioning from PLP to PDP and divides this by the total views of PLP pages to calculate the conversion rate.",
        "external_knowledge": "ga4_page_category.md"
    },
    {
        "instance_id": "ga031",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know our user conversion rate for January 2nd, 2021, calculated as the ratio of users who reached the Checkout Confirmation page to those who landed on the Home page.",
        "plan": "1. Extract and prepare event data from the Google Analytics 4 (GA4) dataset, specifically for page views on January 2, 2021.\n2. Unnest the event parameters to retrieve values for session identifiers and page identifiers, such as page title and location.\n3. Identify and count the initial visits to the Home page and the successful visits to the Checkout Confirmation page.\n4. Join the data to match sessions that start on the Home page and end at the Checkout Confirmation page.\n5. Calculate the user conversion rate for January by comparing the number of successful checkout visits to the total home page visits.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga032",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you pull up the sequence of pages our customer 1362228 visited on January 28th 2021, linking them with '>>' between each page? I want to see their navigation flow through our site. Please merge adjacent identical pages into one, and refer to the docs to convert the corresponding pages to PDP, PLP if necessary.",
        "plan": "1. Select and filter page_view events on January 28th, 2021, specifically for user 1362228 from the GA4 dataset.\n2. Unpack the event_params array to extract details such as session ID, session number, page title, and page location.\n3. Determine page categories based on URL structure and keywords to classify pages as either Product Detail Pages (PDPs), Product Listing Pages (PLPs), or other types.\n4. Rank pages based on the event timestamp and use window functions to establish the sequence of page visits within each session.\n5. concatenate the page titles visited in sequence, separated by '>>', representing the user's navigation flow through the site.",
        "external_knowledge": "ga4_page_category.md"
    },
    {
        "instance_id": "ga006",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Provide the IDs and the average spending per session for users who were engaged in multiple purchase sessions in November 2020",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga009",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the average number of engaged sessions per user of December 2020.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga010",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me an overview of our website traffic for December 2020? I'm particularly interested in the channel with the fourth highest number of sessions.",
        "plan": "1.First, read the document to understand how traffic is divided into 18 channel groups, primarily based on the metrics of source, medium, and campaign.\n2.Extract all visits from the database for December, each visit having a unique user ID and session ID. Retrieve the source, medium, and campaign for each visit.\n3.Based on the classification standards for channel groups in the document, write conditional statements to determine which channel each set of data belongs to, mainly using regular expressions. If the data source (source) contains any of the 4.following: 'badoo', 'facebook', 'fb', 'instagram', 'linkedin', 'pinterest', 'tiktok', 'twitter', or 'whatsapp', and the medium (medium) includes 'cp', 'ppc', or starts with 'paid', then categorize it as 'Paid Social'.\n5.Calculate the number of sessions for each channel based on the channel grouping.\n6.Select the name of the channel ranked fourth as the answer.",
        "external_knowledge": "ga4_dimensions_and_metrics.md"
    },
    {
        "instance_id": "ga010_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please tell me the number of sessions for each website traffic channel in December 2020.",
        "plan": "",
        "external_knowledge": "ga4_dimensions_and_metrics.md"
    },
    {
        "instance_id": "ga011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "What is the highest number of page views for any page path in December 2020?",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga012",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Find the transaction IDs, total item quantities, and purchase revenues for the item category with the highest tax rate on November 30, 2020.",
        "plan": "",
        "external_knowledge": null
    },
    {
        "instance_id": "ga019",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Could you determine what percentage of users either did not uninstall our app within seven days or never uninstalled it after installing during August and September 2018?",
        "plan": "1. Extract distinct user IDs and their first open dates from events labeled as 'first_open' for the months of August and September 2020.\n2. Gather distinct user IDs and their app removal dates from events labeled 'app_remove' during the same timeframe.\n3. Join the installation data with the uninstallation data on user ID to calculate the number of days between app installation and removal.\n4. Determine the percentage of users who uninstalled the app within seven days of installation by comparing the number of users who uninstalled within this period to the total number of users who installed the app.",
        "external_knowledge": null
    },
    {
        "instance_id": "ga030",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Can you group users by the week they first used the app starting from July 2, 2018 and show which group has the most active users remained in the next four weeks, with each group named by the Monday date of that week? Please answer in the format of \" YYYY-MM-DD\".",
        "plan": "",
        "external_knowledge": "retention_rate.md"
    },
    {
        "instance_id": "ga030_1",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Can you group users by the week they first used the app, starting from July 9, 2018? I'd like to see the retention rate for each group over the next two weeks.",
        "plan": "",
        "external_knowledge": "retention_rate.md"
    },
    {
        "instance_id": "ga028",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Using the 7-day retention calculation method, find the number of users who first used the app in the week starting on \"2018-07-02\" and tell me the number of retained users for each week from 0 to 4 weeks.",
        "plan": "",
        "external_knowledge": "retention_rate.md"
    },
    {
        "instance_id": "ga020",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Which quickplay event type had the lowest user retention rate during the second week after their initial engagement, for users who first engaged between August 1 and August 15, 2018?",
        "plan": "1. Extract unique users who installed the app in September 2018, recording the date of first use.\n2. Collect data on users who uninstalled the app within a little over a month from installation, noting the uninstallation dates.\n3. Retrieve crash data for the same users during the specified timeframe to determine app stability issues.\n4. Combine the installation, uninstallation, and crash data into a single dataset using user ID as the key.\n5. Calculate the proportion of users who experienced a crash within a week of installation out of those who uninstalled the app within a week, providing insight into potential issues affecting user retention.",
        "external_knowledge": "retention_rate.md"
    },
    {
        "instance_id": "ga020_1",
        "db": "firebase-public-project.analytics_153293282",
        "question": "What is the retention rate for users two weeks after their initial quickplay event within the period from July 2, 2018, to July 16, 2018, calculated separately for each quickplay event type?",
        "plan": "",
        "external_knowledge": "retention_rate.md"
    },
    {
        "instance_id": "ga025",
        "db": "firebase-public-project.analytics_153293282",
        "question": "For all users who first opened the app in September 2018 and then uninstalled within seven days, I want to know what percentage of them experienced an app crash.",
        "plan": "1. Extract users who installed the app in September 2018.\n2. Extract users who uninstalled the app between September 1 and October 7, 2018.\n3. Extract users who experienced app crashes between September 1 and October 7, 2018.\n4. Combine these datasets to analyze the relationship between crashes and uninstallations.\n5. Calculate the percentage of users who uninstalled within a week and experienced crashes to determine the impact of app crashes on user retention.",
        "external_knowledge": null
    }
]