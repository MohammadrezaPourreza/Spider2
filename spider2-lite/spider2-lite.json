[
    {
        "instance_id": "bq011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many pseudo users were active in the last 7 days but inactive in the last 2 days as of January 7, 2021?",
        "external_knowledge": "ga4_obfuscated_sample_ecommerce.events.md",
        "plan": "1. Identify pseudo users (`user_pseudo_id`) active in the last 7 days: query the `events_*` tables to find users who were active in the last 7 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210101` to `20210107`).\n2. Identify pseudo users (`user_pseudo_id`) active in the last 2 days: query the `events_*` tables to find users who were active in the last 2 days based on engagement time and filter them by the fixed timestamp and relevant table suffixes (from `20210105` to `20210107`).\n3. Combine results and filter:\n- Use a `LEFT JOIN` to combine the two sets of users and filter out users who were active in the last 2 days.\n- Count the distinct user IDs who meet the criteria of being active in the last 7 days but not in the last 2 days.",
        "special_function": [
            "timestamp-functions/TIMESTAMP",
            "timestamp-functions/TIMESTAMP_SUB",
            "timestamp-functions/UNIX_MICROS",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq010",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Find the top-selling product among customers who bought 'Youtube Men\u2019s Vintage Henley' in July 2017, excluding itself.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Extract a distinct list of customers (`fullVisitorId`) who purchased the \"YouTube Men's Vintage Henley\" in July 2017.\n2. Find other products purchased by these customers in July 2017.\n3. Filter out the \"YouTube Men's Vintage Henley\" product itself and aggregate other products purchased by the same customers.\n4. Sort to find the most purchased product.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq009",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Which traffic source receives the top revenue in 2017 and what is the difference (millions, rounded to two decimal places) between its highest and lowest revenue months?",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Calculate monthly revenue for each traffic source.\n2. Aggregate the monthly revenues to compute the total yearly revenue for each traffic source.\n3. Determine which traffic source has the highest total revenue for the year 2017.\n4. Retrieve the monthly revenue data for the top traffic source identified in the previous step.\n5. Calculate the difference between the highest and lowest monthly revenues for the top traffic source.\n6. Retrieve the traffic source and the revenue difference.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq001",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "I wonder how many days between the first transaction and the first visit for each transacting visitor in Feburary 2017, along with the device used in the transaction.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Firstly, extract the first visit date for each visitor in the specified range `201702`.\n2. Next, extract the first transaction date for each visitor in Feb 2017.\n3. Then, extract the device categories used for transactions.\n4. Combine the visit, transaction and device data.\n5. Calculate the number of days between the first transaction and the first visit date for each visitor using `DATE_DIFF`.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "time-functions/TIME",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq002",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the maximum monthly, weekly, and daily product revenues (in millions) generated by the top-performing traffic source in the first half of 2017?",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Firstly, we define the date range to be the first half of year 2017: 20170101 to 20170630.\n2. Next, calculate daily revenues for each traffic source.\n3. Similarly, calculate weekly and monthly revenues for each traffic source.\n4. Determine the top-performing traffic source through aggregation and sorting.\n5. Calculate the maximum revenues for this traffic source on daily/weekly/monthly basis respectively.\n6. Return the final results.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "time-functions/TIME",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq003",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Compare the average pageviews per visitor between purchase and non-purchase sessions for each month from April to July in 2017.",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Calculate average pageviews for non-purchase sessions:\n- Extracts the year and month from the `date` field.\n- Filters sessions with no transactions and no product revenue.\n- Aggregates data by month and calculates the average pageviews per visitor.\n2. Similarly, calculate average pageviews for purchase sessions. The difference is that we only include sessions with at least one transaction and product revenue.\n3. Combine and compare the results, that is select and order results by month.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq004",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most popular other purchased product in July 2017 with consumers who bought products relevant to YouTube?",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Identify visitors who purchased any YouTube product in July 2017.\n2. Calculate the total quantity of each product (excluding YouTube products) purchased by visitors who bought any YouTube product in July 2017.\n3. Retrieve the product name with the highest total quantity.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq008",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What's the most common next page for visitors who were part of \"Data Share\" campaign and after they accessed the page starting with '/home' in January 2017. And what's the maximum duration time (in seconds) when they visit the corresponding home page?",
        "external_knowledge": "google_analytics_sample.ga_sessions.md",
        "plan": "1. Identify relevant visits and pages: extract sessions from the `ga_sessions_*` table within January 2017 that contain page hits with paths starting with '/home' and were part of the \"Data Share\" campaign.\n2. Generate visitor page sequence: combine the filtered sessions with their corresponding page hits to get the full visitor ID, visit ID, timestamp, and page path for each page visit. And order the pages by their visit timestamps.\n3. Calculate the next page and duration:\n   - Use the `LEAD` window function to determine the next page and calculate the duration spent on the current page (by subtracting the current timestamp from the next timestamp).\n   - Rank the pages within each session to maintain the sequence of page visits.\n4. Create the page visit sequence CTE: combine the results into a Common Table Expression (CTE) called that includes the visitor ID, visit ID, page path, duration on the page, next page path, and visit step number.\n5. Determine the most common next page after visiting '/home' page. From the table in Step 4, filter for entries where the current page path starts with '/home', group by the next page, and count occurrences to find the most common next page.\n6. Calculate the maximum duration on home pages. Filter for entries where the current page path starts with '/home' and return the maximum duration spent on it.\n7. Combine and return the results in Step 5 and Step 6, which include the most common next page and the maximum duration.",
        "special_function": [
            "navigation-functions/LEAD",
            "numbering-functions/RANK",
            "string-functions/REGEXP_CONTAINS",
            "timestamp-functions/TIMESTAMP",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq029",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "Get the number of patent publications and the average number of inventors per patent in the US every five years from 1945 to 2020?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1.Extract information on patent applications in the United States since 1945.\n2.Divide the data into five-year intervals.\n3.Within each interval, count the number of applicants for each patent, ensuring each patent has more than zero applicants.\n4.Calculate both the total number of patents and the average number of applicants per patent for each interval.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "array-functions/ARRAY_LENGTH",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR"
        ]
    },
    {
        "instance_id": "bq026",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "For the assignee who has been the most active in the patent category 'A61K39', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the top five countries with the most applications for each assignee per year.\n4. Sort to find out which assignee has the highest total number of applications.\n5. Select the year with the most applications for this assignee.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/IF",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq026_1",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "In which year did the assignee with the most applications in the patent category 'A61K39' file the most?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. First, access the patents database to retrieve all patent application data where the CPC code matches \"A61K39\".\n2. For each assignee, categorize the data by year and country of application.\n3. Identify the assignee with the most total applications for the \u201cA61K39\u201d patent.\n4. Select the year with the most applications for this assignee",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/GROUPING",
            "array-functions/ARRAY",
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq026_2",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "For patent class A01B3, I want to analyze the information of the top 20 assignees based on the total number of applications. Please provide the following five pieces of information: the names of these assignees, their total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Filter Patent Data**: \n   - Identify and collect patent applications related to a specific patent class.\n   - Ensure each application is uniquely identified and includes relevant assignee, filing date, and country information.\n\n2. **Count Applications by Assignee, Year, and Country**:\n   - For each application, determine the number of filings per assignee, broken down by year and country.\n   - Group the data by assignee name, year, and country to prepare for aggregation.\n\n3. **Aggregate Yearly Application Data**:\n   - Summarize the total number of applications for each assignee by year.\n   - Identify the country with the highest number of applications for each assignee and year.\n\n4. **Determine Peak Application Year for Each Assignee**:\n   - Calculate the overall number of applications for each assignee.\n   - Identify the year with the highest application count and the corresponding country with the most applications for each assignee.\n\n5. **Select and Rank Top Assignees**:\n   - Order the assignees by their total number of applications in descending order.\n   - Limit the results to the top 20 assignees, including their total application count, the year with the highest applications, the number of applications in that year, and the most frequent country code during that year.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "string-functions/REGEXP_CONTAINS",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq033",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "How many US patent applications about IoT applications were filed each month from 2008 to 2022?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Extract patents with abstracts containing \"Internet of Things\" that were applied for in the United States.\n2. Generate a record set starting from January 2008 to December 2022.\n3. Count and record the number of \"Internet of Things\" patents applied for each month.\n4. Sort and return the monthly totals with the highest number of patent applications.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "array-functions/GENERATE_DATE_ARRAY",
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq209",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "Could you find out which US utility patent granted in January 2010, with a published application, has the most forward citations over the ten years following its application date?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Retrieve all patents granted in January 2010, considering only those with the kind code B2, which are utility patents issued with a published application.\n2. Query other patents to extract forward citation information from patent publications, ensuring that the citation date of the citing patent falls within ten years of the filing date of the cited patent.\n3. Calculate the number of distinct citation applications for each patent to identify the patent with the most citations.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_ADD",
            "date-functions/PARSE_DATE",
            "json-functions/STRING",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq027",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "For US B2 patents granted in the first seven days of January 2018, tell me the publication number of each patent and the number of backward citations it has received in the SEA category.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Define the Initial Dataset**: Create a temporary dataset that includes the publication number and application number of patents.\n    - Filter for patents from a specific country.\n    - Ensure the patents have a grant date within the first week of January 2018.\n    - Exclude any entries without a grant date.\n    - Include only patents with a specific kind code.\n\n2. **Create the Main Query**: Use the temporary dataset to find the required information.\n    - Select the publication number from the temporary dataset.\n\n3. **Join with Citation Data**: Perform a left join with another table that contains citation details.\n    - Extract the citing publication number, cited publication number, and category from the citation details.\n    - Ensure that the citation category contains a specific keyword.\n\n4. **Join with Publication Data Again**: Perform another left join to link the cited publication number with the application number from the publication data.\n\n5. **Aggregate the Results**: Group the results by the publication number from the initial dataset.\n    - Count the distinct application numbers that are cited by the patents in the specified category.\n\n6. **Order the Results**: Sort the final results by the publication number.",
        "special_function": [
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LEFT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq210",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "How many US B2 patents granted between 2015 and 2018 contain claims that do not include the word 'claim'?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Extract Relevant Patent Data**: Create a temporary dataset by selecting publication numbers and their associated claims from the main data source. Ensure that only patents granted in the US between the years 2015 and 2018 are included. Also, filter to include only those patents whose publication numbers end with 'B2'.\n\n2. **Unnest Claims**: Expand the nested claims data so that each claim associated with a patent is treated as a separate row in the temporary dataset.\n\n3. **Filter Claims Text**: Check each claim text to see if it does not contain the word 'claim'. Count the number of such claims for each publication number.\n\n4. **Group by Publication Number**: Aggregate the results by publication number, counting how many claims per patent do not contain the word 'claim'.\n\n5. **Filter Non-zero Counts**: From the aggregated data, select only those patents that have at least one claim not containing the word 'claim'.\n\n6. **Final Count**: Count the number of patents that meet the criteria established in the previous steps. This final result represents the number of US B2 patents granted between 2015 and 2018 that have at least one claim not containing the word 'claim'.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq211",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "Among the US utility B2 patents granted in January 2008, how many of them belong to families that have a total of over 300 distinct applications?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1.Extract information on patents that were applied for and granted in the United States in January 2008.\n2.Obtain the family IDs associated with these patents.\n3.Retrieve the total number of patents within these families.\n4.Keep the application IDs of patents where the family size exceeds 300 and count their total number.",
        "special_function": null
    },
    {
        "instance_id": "bq213",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "What is the most common 4-digit IPC code among US B2 utility patents granted in the first seven days of June 2018?",
        "temporal": "Yes",
        "external_knowledge": "patents_info.md",
        "plan": "1.Data Retrieval: Access patents-public-data.patents.publications, unnest IPC codes, and filter for US patents granted in April 2015 with a 'B2' kind code.\n2.IPC Analysis: Extract the first four characters of each IPC code, count their occurrences within each patent, and group by publication number and IPC subcategory.\n3.Determine Dominance: For each patent, identify the most frequent IPC subcategory using a combination of concatenation and maximum count comparison within the grouped data.\n4.Result Extraction: Order the results by the count of IPC occurrences and limit the output to the top entry to find the single most prevalent IPC subcategory.",
        "special_function": [
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq213_1",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "For US B2 utility patents granted in April 2015, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 20 or more times.",
        "temporal": "Yes",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Create an Interim Table:**\n   - Select the publication number and extract the first 4 characters of the IPC code.\n   - Count the occurrences of each 4-character IPC code for each publication.\n   - Filter records to include only those from the specified country and date range, ensuring the grant date is valid and the publication format matches the specified pattern.\n   - Group the results by publication number and the 4-character IPC code.\n\n2. **Identify Most Frequent IPC Code:**\n   - In the interim table, determine the most frequent 4-character IPC code for each publication.\n   - Concatenate the publication number with the count of the most frequent IPC code.\n   - Use this concatenated value to filter the interim table, ensuring you select rows where the IPC code count matches the maximum count for each publication.\n\n3. **Filter by Frequency:**\n   - Further filter the results to include only those rows where the most frequent IPC code appears 20 or more times.\n\n4. **Final Selection:**\n   - From the filtered results, select and list the publication numbers and their corresponding 4-character IPC codes.",
        "special_function": [
            "string-functions/CONCAT",
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq214",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "For the 'B2' publication granted in January 2017 in the US that received the most forward citations within a month of its filing date, determine the publication number of the most similar patent from the same filing year.",
        "temporal": "Yes",
        "external_knowledge": "patents_info.md",
        "plan": "1.Extract Relevant Patent Data: Access the patents database to retrieve information for patents granted in the US on June 2nd, 2015, with the kind code B2, indicating they are granted utility patents.\n2.Determine the IPC Codes: For each patent, extract and count occurrences of four-digit IPC codes from the abstracts, identifying how frequently each IPC code appears within a given patent.\n3.Identify Backward Citations: For each selected patent, retrieve backward citations (i.e., earlier patents cited by the patent in question) and join these with the IPC data to analyze the diversity of IPC codes from these citations.\n4.Calculate Originality Score: Compute an originality score for each patent based on the diversity of IPC codes cited. This involves calculating a formula where the diversity is inversely related to the sum of the squares of IPC code occurrences.\n5.Select the Patent with the Highest Originality: From the calculated originality scores, identify the patent with the highest score, which indicates it has the broadest range of influences from prior art, suggesting a high level of innovation.\n6.Output the Result: Return the publication number of the patent with the highest originality score, highlighting it as the most original patent granted on that specific day.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_ADD",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq214_1",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "d30b2fcf-037c-4e90-9471-9d4a0a9f5e7b",
        "temporal": "Yes",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Extract the Target Patent**: Create a subset containing only the target patent's unique identifiers.\n\n2. **Calculate Filing Year for Target Patent**: Determine the filing year of the target patent by extracting the year from its filing date.\n\n3. **Identify Same-Year Patents**: Retrieve all patents filed in the same year as the target patent, excluding the target patent itself.\n\n4. **Calculate Technological Similarity**: For each patent from the same year, compute the technological similarity with the target patent using a predefined similarity metric based on their respective embeddings.\n\n5. **Rank Similar Patents**: Order the patents by their similarity scores in descending order.\n\n6. **Select Top Similar Patents**: From the ordered list, select the top five patents with the highest similarity scores for each target patent.\n\n7. **Output Similar Patents**: Return the publication numbers and similarity scores of the top five most similar patents.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq215",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "What is the publication number of US patent granted at January 2018, with the highest originality score based on the diversity of 4-digits IPC codes from its backward citations?",
        "temporal": "Yes",
        "external_knowledge": "patents_info.md",
        "plan": "1. **Filter US Patents:**\n   - Select publication numbers and application numbers from the dataset.\n   - Only include records where the country code is 'US'.\n   - Ensure the grant date is within January 2018.\n   - Exclude records with a grant date of 0.\n   - Only consider patents with a specific kind code pattern.\n\n2. **Extract IPC Codes:**\n   - For each selected patent, extract and count the unique 4-digit IPC codes from the associated IPC codes.\n\n3. **Identify Maximum IPC Code Count:**\n   - Create a subset of records that have the maximum count of a specific 4-digit IPC code for each patent.\n\n4. **Calculate IPC Occurrences in Backward Citations:**\n   - Join the filtered patents with their backward citations.\n   - For each backward citation, join with the subset of records to get the 4-digit IPC codes.\n   - Count the occurrences of each 4-digit IPC code in the backward citations for each patent.\n\n5. **Compute Originality Score:**\n   - For each patent, calculate an originality score based on the diversity of the 4-digit IPC codes from the backward citations.\n   - Use a formula that considers the sum of squared occurrences of each IPC code, normalized by the total number of occurrences.\n\n6. **Select Highest Originality Score:**\n   - From the computed originality scores, select the patent with the highest score.\n\n7. **Return Result:**\n   - Output the publication number of the patent with the highest originality score.",
        "special_function": [
            "mathematical-functions/POWER",
            "string-functions/CONCAT",
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq221",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "Identify the CPC technology areas with the highest exponential moving average of patent filings each year (smoothing factor 0.2), and provide the full title and the best year for each CPC group at level 5.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. First, we define a temporary JavaScript function, which is used to calculate the year with the highest moving average of patent counts.\n2. Use common table expression to extract the CPC codes and filing years for patents, ensuring each patent has an application number and valid filing date. Also filter to include only the primary CPC code (`first = TRUE`).\n3. Calculate the most common patenting technology areas by year and identify the year with the highest moving average of patents for each CPC group. Concretely,\n- Aggregate patent counts by CPC group and filing year.\n- Use the defined function in Step 1. to find the year with the highest moving average of patents for each CPC group.\n- Join the results with the CPC definition table to get the full title for each CPC group. Remember, only include level 5 CPC groups.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "conditional-functions/IF",
            "other-functions/CREATE_FUNCTION",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq222",
        "db": "patents-public-data.patents",
        "question": "Find the CPC technology areas in Germany with the highest exponential moving average of patent filings each year (smoothing factor 0.1) for patents granted in December 2016. Show me the full title, CPC group and the best year for each CPC group at level 4.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Firstly, we create a temporary function to calculate the highest moving average of patent counts.\n2. Use a common table expression to extract CPC codes and filing years for patents granted in Germany between 2016-12-01 to 2016-12-31, including only the primary CPC code.\n3. Calculate the highest moving average of patent counts for each CPC group, record the filing year information.\n4. Combine the results with CPC definitions to get the full title and also return the results with CPC group and filing year.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "aggregate-functions/ARRAY_AGG",
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "conditional-functions/IF",
            "other-functions/CREATE_FUNCTION",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq223",
        "db": "patents-public-data.patents\npatents-public-data.cpc",
        "question": "Which assignee and primary CPC subclass full title most frequently cite patents assigned to 'AAAA'?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Firstly, extract citing publication and cited publication details, including only the primary CPC code.\n2. Combine the citing and cited data according the publication number.\n3. Join with CPC definition information to extract CPC subclass title.\n4. Add filter to include only relevant citations.\n5. Group the results by citing assignee and CPC full title, count the citations and order the result by count.\nWe only return the most frequent assignee and CPC subclass title.",
        "special_function": [
            "string-functions/SUBSTR",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq247",
        "db": "patents-public-data.patents\npatents-public-data.google_patents_research",
        "question": "Which valid family has the most publications? Just provide their publication abstracts.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Aggregate the data to count the number of publications per unique family identifier.\n2. Select the family identifier that has the highest publication count, ensuring that only valid identifiers are considered.\n3. Filter the dataset to include only those records that have a non-empty abstract. \n4. Join the filtered list of publications with the most published family identifier obtained earlier to get a dataset containing only the abstracts from publications that belong to the family with the highest publication count.\n5. Retrieve and display the abstracts from publications associated with the family having the most publications. \n",
        "special_function": null
    },
    {
        "instance_id": "bq246",
        "db": "patents-public-data.patentsview",
        "question": "Can you figure out the number of forward citations within 3 years from the application date for the patent that has the most backward citations within 3 years from application among all U.S. patents?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Correlate patent applications with patent citations based on patent identifiers. Ensure that only patents registered in the US are considered.\n2. Compute citations where the citation date is between the application date and three years after.\n3. Compute citations where the citation date is between three years prior to the application date and the application date itself.\n4. Join the information about current patent classifications with the aggregated citation data.\n5. Merge the detailed citation data with the patent applications data.\n6. Sort the resulting data set by the count of backward citations in descending order and restrict the output to the top record to find the patent with the highest count of backward citations within the specified period.\n7. From the final sorted and limited dataset, extract the count of forward citations for the patent with the highest number of backward citations. \n",
        "special_function": [
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_SUB",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq052",
        "db": "patents-public-data.patentsview",
        "question": "I wonder which patents within CPC subsection 'C05' or group 'A01G' in the USA have at least one forward or backward citations within one month of their application dates. Give me the ids, titles, application date, forward/backward citation counts and summary texts.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Identify Relevant Patents:\n   - Select patents in the USA that belong to specific CPC categories (either a particular subsection or group).\n\n2. Citation Filtering:\n   - Determine patents with at least one forward or backward citation within one month of their application dates. Count these citations separately for forward and backward directions.\n\n3. Data Aggregation:\n   - For each relevant patent, gather the patent ID, title, application date, forward and backward citation counts, and summary text.\n\n4. Combine and Order Results:\n   - Join the relevant patent data with citation counts and summary texts, and then order the results by the application date.",
        "special_function": [
            "conversion-functions/SAFE_CAST",
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_SUB",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq207",
        "db": "patents-public-data.patents\nbigquery-public-data.uspto_oce_assignment\nbigquery-public-data.uspto_oce_cancer\nbigquery-public-data.uspto_oce_claims\nbigquery-public-data.uspto_oce_litigation\nbigquery-public-data.uspto_ptab",
        "question": "Can you provide the initial publication numbers for our top 100 independent patent claims with the highest word count?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1.Select Independent Claims: Extract patent number, claim number, and word count for independent claims.\n2.Match Publication Numbers: Join with the match table to get the corresponding publication numbers.\n3.Match Application Numbers and Publication Information: Join with the publications table to get application details and select the earliest publication for each application.\n4.Select and Order Results: Retrieve all fields, order by word count in descending order, and limit to the top 100 records.",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq036",
        "db": "bigquery-public-data.github_repos",
        "question": "What was the average number of GitHub commits made per month in 2020 for repositories containing Python code?",
        "external_knowledge": null,
        "plan": "1. Find out the table containing all commit info: `bigquery-public-data.github_repos.commits`\n2. Filter out the commit info in 2020, and for each commit-repo pair save its YearMonth timestamp.\n3. Find `bigquery-public-data.github_repos.languages` table, and range the data in commit - language entries.\n4. Combine them to find the commits which contain Python language and were made in  2020. Group the data according to YearMonth and calculate the number of commits for each month.\n5. Calculate the average monthly number of commits.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "timestamp-functions/TIMESTAMP_SECONDS",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq100",
        "db": "bigquery-public-data.github_repos",
        "question": "Find out the most frequently used package in all Go source files.",
        "external_knowledge": null,
        "plan": "1. Extract Imports from File Contents (imports CTE):\n- Selects id and lines containing imports from files where the content matches the regex pattern r'import\\s*\\([^)]*\\)'.\n- SPLIT(REGEXP_EXTRACT(content, r'import\\s*\\(([^)]*)\\)'), '\\n') extracts the content inside the import(...) block and splits it into individual lines.\n2. Filter Go Files (go_files CTE):\n- Selects the id of files that have paths ending with .go (Go source files).\n- Uses GROUP BY id to ensure unique file IDs.\n3. Unnest Lines of Imports (filtered_imports CTE): Unnests the lines from the imports CTE to get each line as a separate row, resulting in rows with id and line.\n4. Join Imports with Go Files (joined_data CTE): Joins the filtered_imports with go_files on the id to filter only those import lines that belong to Go files.\n5. Extract the Package (SELECT statement):\n- Extracts the imported package using REGEXP_EXTRACT(line, r'\"([^\"]+)\"').\n- Filters out NULL string.\n- Groups the results by package, counts the occurrences, orders by the count in descending order, and limits the result to the most frequently imported package.",
        "special_function": [
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq101",
        "db": "bigquery-public-data.github_repos",
        "question": "Identify the top 10 most frequently imported packages and their counts in Java source files.",
        "external_knowledge": null,
        "plan": "1. Filter java source files: select Java files containing the \"import\" keyword.\n- condition: sample_path LIKE '%.java' to filter Java files.\n- condition: REGEXP_CONTAINS(content, r'import') to ensure the file contains \"import\" statements.\n2. Split the file content into lines and extract the import lines.\n3. Extract the package names with regex expression `r'([a-z0-9\\._]*)\\.'` to capture the package names.\n4. Count the number of imports for each package using GROUP BY clause.\n5. Order the results by count in descending order and limits to the top 10 packages.",
        "special_function": [
            "string-functions/LEFT",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq182",
        "db": "bigquery-public-data.github_repos\ngithubarchive.day",
        "question": "Which primary programming languages, determined by the highest number of bytes in each repository, have its repositories having at least a total of 100 pull requests on January 18, 2023?",
        "external_knowledge": null,
        "plan": "1. Extract event data from `githubarchive.day.20230118` db and get repo name from url.\n2. combine with the db `bigquery-public-data.github_repos.languages` to group the event data by the language of its repo.\n3. Filter to keep the data that has a type of 'PullRequestEvent' and a count of more than 100.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "navigation-functions/FIRST_VALUE",
            "string-functions/REGEXP_REPLACE",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq182_1",
        "db": "bigquery-public-data.github_repos\ngithubarchive.day",
        "question": "How many pull requests in total were created in repositories that include JavaScript as one of their languages, considering data from January 18, 2023?",
        "external_knowledge": null,
        "plan": "1. **Subquery `a` - Extract and Transform Data from `githubarchive` Table:**\r\n   - **Select Fields:** Extract the `type`, the year (`y`), and the quarter (`q`) from the `created_at` column.\r\n   - **Regular Expression Replace:** Clean the `repo.url` to extract the repository name, removing specific URL patterns.\r\n   - **Data Source:** Pull data from the `githubarchive.day.20230118` table.\r\n\r\n2. **Subquery `b` - Filter and Aggregate Data from `github_repos.languages`:**\r\n   - **Nested Subquery:** \r\n     - **Unnest Languages:** Expand the `languages` array to individual rows.\r\n     - **Select Fields:** Extract `repo_name` and `language` details.\r\n   - **Filter Language:** Retain only rows where `language` is 'JavaScript'.\r\n   - **Group By:** Aggregate by `repo_name` and `language` to ensure unique combinations.\r\n\r\n3. **Join Subquery `a` and Subquery `b`:**\r\n   - **Join Condition:** Match `name` from subquery `a` with `name` from subquery `b`.\r\n\r\n4. **Filter by Event Type:**\r\n   - **Condition:** Retain only rows where `type` is 'PullRequestEvent'.\r\n\r\n5. **Count the Results:**\r\n   - **Aggregate Function:** Count the total number of resulting rows and alias the result as `total_pull_requests`.\r\n\r\n6. **Final Output:**\r\n   - **Select Statement:** Return the total count of pull request events for repositories where the language is 'JavaScript'.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/REGEXP_REPLACE",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq191",
        "db": "bigquery-public-data.github_repos\ngithubarchive.year",
        "question": "Find the top 5 repositories from 2017, which have more than 300 unique users watching them, that also have a `pom.xml` file containing the text `<artifactId>junit</artifactId>`. List these repositories in descending order of the watches they have.",
        "external_knowledge": null,
        "plan": "1. **Define CTE `repos`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct repository names from the `sample_files` table, assigning the result to `repo_in_mirror`.\r\n   - **Subquery `b`:**\r\n     - Select repository names and approximate count of distinct actors who performed 'WatchEvent' in 2017 from `githubarchive.year.2017`, filtering for repositories with more than 300 stars.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using `repo_in_mirror` and `repo_with_stars` to keep all repositories with stars from subquery `b`.\r\n     - Filter out rows where `repo_in_mirror` is NULL.\r\n   - **Result:**\r\n     - CTE `repos` contains repository names and their star counts for repositories with more than 300 stars and which exist in `sample_files`.\r\n\r\n2. **Define CTE `contents`:**\r\n   - **Subquery `a`:**\r\n     - Select distinct entries from `sample_files` where the repository name exists in the `repos` CTE.\r\n   - **Subquery `b`:**\r\n     - Select content ID and content from `sample_contents`.\r\n   - **Join `a` and `b`:**\r\n     - Perform a RIGHT JOIN on `a` and `b` using the content ID.\r\n   - **Result:**\r\n     - CTE `contents` contains all content entries linked to repositories listed in the `repos` CTE.\r\n\r\n3. **Final Query:**\r\n   - **Join `repos` and `contents`:**\r\n     - Join `repos` with `contents` on `repo_name`.\r\n   - **Filter Results:**\r\n     - Filter for rows where the content contains the string '%junit</artifactId>%'.\r\n     - Filter for rows where the path is 'pom.xml'.\r\n   - **Order and Limit Results:**\r\n     - Order the results by the number of stars in descending order.\r\n     - Limit the output to the top 5 rows.\r\n\r\n4. **Output:**\r\n   - Select repository names and star counts from the filtered and joined results.",
        "special_function": [
            "approximate-aggregate-functions/APPROX_COUNT_DISTINCT"
        ]
    },
    {
        "instance_id": "bq224",
        "db": "bigquery-public-data.github_repos\ngithubarchive.month",
        "question": "Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?",
        "external_knowledge": null,
        "plan": "1. Filter only those repositories whose licenses are listed in the provided array of approved licenses.\n2. Select repository names and count distinct users who watched the repositories, that is to include only `WatchEvent` types.\n3. Similarly, select repository names and count the number of issues (`IssuesEvent`) and forks (`ForkEvent`).\n4. Combine the data and calculate the total counts of forks, issue events, and watches for each repository.\n5. Sort the results in descending order and only return the top repository name.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq192",
        "db": "bigquery-public-data.github_repos",
        "question": "Which repository that has a license of either \"artistic-2.0\", \"isc\", \"mit\", or \"apache-2.0\", contains Python files in the master branch, and has the highest combined count of forks, issues, and watch events in April 2022?",
        "external_knowledge": null,
        "plan": "1. **Define Allowed Repos:**\r\n   - Create a Common Table Expression (CTE) named `allowed_repos` to filter repositories that have specific licenses (`artistic-2.0`, `isc`, `mit`, `apache-2.0`).\r\n   - Select `repo_name` and `license` from the `bigquery-public-data.github_repos.licenses` table where the license matches one of the specified values.\r\n\r\n2. **Calculate Watch Counts:**\r\n   - Create a CTE named `watch_counts` to calculate the number of unique watch events per repository.\r\n   - Select the repository name and count the distinct `actor.login` from the `githubarchive.month.202204` table where the event type is `WatchEvent`.\r\n   - Group by repository name.\r\n\r\n3. **Calculate Issue Counts:**\r\n   - Create a CTE named `issue_counts` to calculate the number of issue events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `IssuesEvent`.\r\n   - Group by repository name.\r\n\r\n4. **Calculate Fork Counts:**\r\n   - Create a CTE named `fork_counts` to calculate the number of fork events per repository.\r\n   - Select the repository name and count the total number of events from the `githubarchive.month.202204` table where the event type is `ForkEvent`.\r\n   - Group by repository name.\r\n\r\n5. **Combine Metadata:**\r\n   - Create a CTE named `metadata` to combine the information from `allowed_repos`, `fork_counts`, `issue_counts`, and `watch_counts`.\r\n   - Perform INNER JOIN operations between `allowed_repos` and the other CTEs (`fork_counts`, `issue_counts`, and `watch_counts`) on `repo_name`.\r\n   - Select `repo_name`, `license`, `forks`, `issue_events`, and `watches` from the combined data.\r\n\r\n6. **Identify Repos with Python Files:**\r\n   - Create a CTE named `github_files_at_head` to identify repositories that have Python files at the head of the master branch.\r\n   - Select `repo_name` from the `bigquery-public-data.github_repos.sample_files` table where `ref` is `\"refs/heads/master\"`, the file path ends with `.py`, and `symlink_target` is `NULL`.\r\n   - Group by `repo_name`.\r\n\r\n7. **Select Top Repository:**\r\n   - Perform an INNER JOIN between `metadata` and `github_files_at_head` on `repo_name`.\r\n   - Select the `repo_name` from `metadata` as `repository`.\r\n   - Order the results by the sum of `forks`, `issue_events`, and `watches` in descending order.\r\n   - Limit the result to the top repository.\r\n\r\n8. **Return Result:**\r\n   - The final query returns the repository name of the top repository with the highest combined count of forks, issue events, and watches among those that have Python files at the head of the master branch and have an allowed license.",
        "special_function": [
            "string-functions/ENDS_WITH",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq225",
        "db": "bigquery-public-data.github_repos",
        "question": "What's the top 3 widely used languages according to file counts?",
        "external_knowledge": "lang_and_ext.md",
        "plan": "1. Extract languages for each file according to file extensions.\n    - Determine the programming language based on the file extension extracted from the `path` field of the `files` table.\n    - Use the `REGEXP_EXTRACT` function to extract the file extension.\n    - Use a `CASE` statement to map file extensions to their respective languages based on the provided document `lang_and_ext.md`.\n2. Perform an inner join between the `languages` CTE and the `sample_contents` table on the `id` column. This join ensures that only files which have a corresponding content entry are considered.\n3. Filter out rows where `language` or `content` is `NULL`. This ensures that only valid and meaningful entries are included in the final result.\n4. Group and count by languages, and limit the final results to the top 3 entries.",
        "special_function": [
            "string-functions/REGEXP_EXTRACT"
        ]
    },
    {
        "instance_id": "bq180",
        "db": "bigquery-public-data.github_repos",
        "question": "Please help me retrieve the top 3 most frequently used module names from Python and R scripts.",
        "external_knowledge": null,
        "plan": "1. Get all the sample github file data and unnest the lines. Record the file path for each line.\n2. Extract the module names from the \"import\" and \"from\" statements of Python files, and the \"library(...)\" lines of R files.\n3. Count the number of occurences for each module and limit to the top 10 most used ones.",
        "special_function": [
            "array-functions/ARRAY_CONCAT",
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq180_1",
        "db": "bigquery-public-data.github_repos",
        "question": "Can you find the top 5 most frequently imported Python modules and R libraries from the GitHub sample files and list them along with their occurrence counts? Please sort the results by language and then by the number of occurrences in descending order.",
        "external_knowledge": null,
        "plan": "1. **Extract File Information:**\r\n   - Join `sample_files` and `sample_contents` tables on `id` to get `file_id`, `repo_name`, `path`, and `content`.\r\n   - Split `content` into individual lines, treating each line as a separate record.\r\n\r\n2. **Identify and Extract Module Imports:**\r\n   - Create a CTE (`extracted_modules`) to filter and process lines containing import statements for Python (`import` or `from`) and R (`library`).\r\n   - For Python files (`.py`):\r\n     - Extract modules imported using `import` and `from` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names and concatenate results.\r\n   - For R files (`.r`):\r\n     - Extract modules imported using `library()` statements.\r\n     - Use `REGEXP_EXTRACT_ALL` to extract module names.\r\n   - Store extracted modules along with file information and detected language (Python or R).\r\n\r\n3. **Count Module Occurrences:**\r\n   - Create another CTE (`module_counts`) to count occurrences of each module per language.\r\n   - Unnest the modules array to have one module per row.\r\n   - Group by `language` and `module` and count occurrences.\r\n\r\n4. **Select Top 5 Modules for Python:**\r\n   - Create a CTE (`top5_python`) to select the top 5 most frequently used Python modules.\r\n   - Filter `module_counts` for `python` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n5. **Select Top 5 Modules for R:**\r\n   - Create a CTE (`top5_r`) to select the top 5 most frequently used R modules.\r\n   - Filter `module_counts` for `r` language.\r\n   - Order by `occurrence_count` in descending order.\r\n   - Limit results to 5.\r\n\r\n6. **Combine Results and Order:**\r\n   - Use `UNION ALL` to combine results from `top5_python` and `top5_r`.\r\n   - Order the final results by `language` and `occurrence_count` in descending order to display the top modules for each language.",
        "special_function": [
            "array-functions/ARRAY_CONCAT",
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq248",
        "db": "bigquery-public-data.github_repos",
        "question": "Among all 'requirements.txt' files in repositories containing Python code, how much percentage of them include the 'requests' package?",
        "external_knowledge": null,
        "plan": "1. Get all ids and contents from table sample_contents.\n2. Extract ids, names and paths of github repositories which have requirements.txt.\n3. Extract names of github repositories whose language_name contains python.\n4. Identify the repositories whose requirements.txt contains \u2018requests\u2019.\n5. Count the number of python repositories whose requirements.txt contains \u2018requests\u2019.\n6. Count the number of python repositories which contain requirements.txt.\n7. Divide the above statistics to get the corresponding proportion.\n",
        "special_function": [
            "string-functions/LOWER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq193",
        "db": "bigquery-public-data.github_repos",
        "question": "Help me retrieve the top 5 most frequently occurring non-empty, non-commented lines of text in `requirements.txt` files from GitHub repositories that primarily use Python for development.",
        "external_knowledge": null,
        "plan": "1. **Extract Content and Metadata**:\n   - Start by retrieving the repository content and metadata from the repository data source.\n   - Filter to include only files whose paths indicate they are `requirements.txt`.\n\n2. **Filter for Python Repositories**:\n   - Retrieve the list of repositories that primarily use Python for development.\n   - Join this list with the filtered files to ensure only files from Python repositories are considered.\n\n3. **Split File Content into Lines**:\n   - For each `requirements.txt` file, split its content into individual lines.\n\n4. **Filter Out Empty Lines**:\n   - Remove any lines that are empty or consist only of whitespace.\n\n5. **Calculate Line Frequencies**:\n   - Count the occurrences of each non-empty line across all the files.\n   - Sort these lines by their frequency of occurrence in descending order.\n\n6. **Retrieve the Third Most Frequent Line**:\n   - Select the line that is the third most frequently occurring from the sorted list.",
        "special_function": [
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "string-functions/STARTS_WITH",
            "string-functions/TRIM",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq295",
        "db": "bigquery-public-data.github_repos",
        "question": "Among the repositories from the GitHub Archive which include a Python file with less than 15,000 bytes in size and a keyword 'def' in the content, find the top 3 that have the highest number of watch events in 2017?",
        "external_knowledge": null,
        "plan": "1. **Common Table Expression (CTE) - `watched_repos`**:\n    - **Objective**: Extract repository names that have been watched.\n    - **Action**: \n        - Select `repo.name` as `repo` from the dataset `githubarchive.month.2017*` where the `type` is `\"WatchEvent\"`.\n\n2. **Common Table Expression (CTE) - `repo_watch_counts`**:\n    - **Objective**: Calculate the watch count for each repository.\n    - **Action**:\n        - Select `repo` and `COUNT(*)` as `watch_count` from the `watched_repos` CTE.\n        - Group the results by `repo` to aggregate watch counts.\n\n3. **Main Query - Data Source**:\n    - **Objective**: Fetch Python files with specific characteristics from GitHub repositories.\n    - **Action**:\n        - Select from `bigquery-public-data.github_repos.sample_files` as `f` and `bigquery-public-data.github_repos.sample_contents` as `c`.\n        - Join `f` and `c` on `f.id = c.id`.\n\n4. **Main Query - Additional Join**:\n    - **Objective**: Incorporate the watch counts into the main query.\n    - **Action**:\n        - Join the results from the previous step with `repo_watch_counts` as `r` on `f.repo_name = r.repo`.\n\n5. **Main Query - Filtering**:\n    - **Objective**: Filter the joined data to match specific criteria for Python files.\n    - **Action**:\n        - Apply a filter to select files where `f.path` ends with `.py`.\n        - Ensure the content size `c.size` is less than 15000 bytes.\n        - Check if the content `c.content` contains the pattern 'def ' using `REGEXP_CONTAINS`.\n\n6. **Main Query - Grouping and Ordering**:\n    - **Objective**: Prepare the final result set with grouping and sorting.\n    - **Action**:\n        - Group the results by `r.repo` and `r.watch_count`.\n        - Order the results by `r.watch_count` in descending order.\n\n7. **Main Query - Limiting Results**:\n    - **Objective**: Restrict the output to the top 3 repositories.\n    - **Action**:\n        - Use `LIMIT 3` to return only the top 3 repositories based on watch count.\n\n8. **Final Output**:\n    - **Objective**: Present the result.\n    - **Action**:\n        - Select and display the repository names and their corresponding watch counts.",
        "special_function": [
            "string-functions/REGEXP_CONTAINS"
        ]
    },
    {
        "instance_id": "bq249",
        "db": "bigquery-public-data.github_repos",
        "question": "Can you provide a report showing the counts of different types of lines in SQL files from the GitHub repository in descending order, specifically categorizing them into 'trailing' if there is at least one blank character at the end of the line, 'Space' if the line starts with at least one space, and 'Other' for any other types? ",
        "external_knowledge": null,
        "plan": "1. Filter records from a dataset where the file path ends with `.sql`.\n2. Use the `SPLIT` function to divide the content of each file into individual lines. This transformation creates an array of lines for each file.\n3. Each element (line) of the array is processed individually using the `CROSS JOIN UNNEST` technique. This flattens the array into a table format where each line is treated as a separate record.\n4. For each line, determine its type according to the definition of three types.\n5. Count how many lines fall into each indentation category ('trailing', 'Space', 'Other') across all files.\n6. Order the results by the count of occurrences in descending order.\n",
        "special_function": [
            "string-functions/CHAR_LENGTH",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq375",
        "db": "bigquery-public-data.github_repos",
        "question": "Determine which file type among Python (.py), C (.c), Jupyter Notebook (.ipynb), Java (.java), and JavaScript (.js) in the GitHub codebase has the most files with a directory depth greater than 10, and provide the file count.",
        "external_knowledge": null,
        "plan": "1. **Identify Files and Calculate Directory Depth**:\n   - Select all relevant files that have extensions of interest (Python, C, Jupyter Notebook, Java, JavaScript).\n   - Calculate the directory depth for each file by counting the number of slashes in the file path.\n\n2. **Classify File Types**:\n   - For each file, determine its type based on the file extension and assign a corresponding label (e.g., Python, C, Jupyter Notebook, Java, JavaScript).\n\n3. **Filter by Directory Depth**:\n   - Filter the files to include only those with a directory depth greater than 10.\n\n4. **Aggregate and Count Files by Type**:\n   - Group the filtered files by their type and count the number of files in each group.\n\n5. **Sort and Limit Results**:\n   - Sort the file counts in descending order.\n   - Select the file type with the highest count.\n\n6. **Output**:\n   - Return the file type with the highest number of files and the corresponding file count.",
        "special_function": [
            "string-functions/LENGTH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REPLACE",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq255",
        "db": "bigquery-public-data.github_repos",
        "question": "How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?",
        "external_knowledge": null,
        "plan": "1. Retrieve repositories which have an 'apache-2.0' license.\n2. Retrieve repositories which have 'Shell' as one of its languages.\n3. Only keep messages whose length is greater than 5 and less than 10000.\n4. Remove messages containing \u2018update\u2019, \u2018test\u2019 or \u2018merge%\u2019.\n5. Count the selected messages and return.\n",
        "special_function": [
            "string-functions/LENGTH",
            "string-functions/LOWER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq194",
        "db": "bigquery-public-data.github_repos",
        "question": "What is the second most frequently used module (imported library) across Python, R, and IPython script (.ipynb) files in the GitHub sample dataset?",
        "external_knowledge": null,
        "plan": "1. **Define CTE `extracted_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `line`\r\n     - Determine `script_type` based on file extension:\r\n       - `.py` -> 'Python'\r\n       - `.r`, `.R`, `.Rmd`, `.rmd` -> 'R'\r\n       - `.ipynb` -> 'IPython'\r\n       - Others -> 'Others'\r\n     - Extract `modules` using regex based on `script_type`:\r\n       - For Python files, extract modules from `import` and `from` statements.\r\n       - For R files, extract modules from `library()` statements.\r\n       - For IPython notebooks, handle `import` and `from` statements within double quotes.\r\n   - **From Subquery:**\r\n     - Join `bigquery-public-data.github_repos.sample_files` and `bigquery-public-data.github_repos.sample_contents` on `id`.\r\n     - Select `file_id`, `repo_name`, `path`, and split `content` into `lines`.\r\n   - **Filter Lines:**\r\n     - For Python files, include lines containing `import` or `from ... import`.\r\n     - For R files, include lines containing `library()`.\r\n     - For IPython notebooks, include lines containing `import` or `from ... import` within double quotes.\r\n\r\n2. **Define CTE `unnested_modules`:**\r\n   - **Select Columns:**\r\n     - `file_id`, `repo_name`, `path`, `script_type`, `module`\r\n   - **From `extracted_modules`:**\r\n     - Unnest `modules` to get individual `module` names.\r\n\r\n3. **Define CTE `module_frequencies`:**\r\n   - **Select Columns:**\r\n     - `module`\r\n     - `script_type`\r\n     - Count occurrences of each `module` and `script_type` combination as `frequency`\r\n   - **From `unnested_modules`:**\r\n     - Group by `module`, `script_type`\r\n     - Order by `frequency` in descending order.\r\n\r\n4. **Final Select:**\r\n   - **Select Column:**\r\n     - `module`\r\n   - **From `module_frequencies`:**\r\n     - Order by `frequency` in descending order.\r\n     - Limit result to the second most frequent module (using `LIMIT 1 OFFSET 1`).",
        "special_function": [
            "string-functions/ENDS_WITH",
            "string-functions/REGEXP_CONTAINS",
            "string-functions/REGEXP_EXTRACT_ALL",
            "string-functions/REPLACE",
            "string-functions/SPLIT",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq359",
        "db": "bigquery-public-data.github_repos",
        "question": "List the repository names and commit counts for the top five GitHub repositories with Python as the primary language and the highest number of commits.",
        "external_knowledge": null,
        "plan": "1. **Identify Primary Language of Repositories:**\n   - Extract repository names and their associated languages.\n   - Rank the languages for each repository based on the size of code written in that language.\n   - Filter to retain only the primary language (the one with the highest code size) for each repository.\n\n2. **Filter for Python Repositories:**\n   - From the list of repositories with their primary languages, select only those repositories where the primary language is Python.\n\n3. **Join with Commit Data:**\n   - Join the filtered list of Python repositories with the commit data to associate each repository with its commits.\n\n4. **Count Commits per Repository:**\n   - For each repository, count the total number of commits.\n\n5. **Order and Limit Results:**\n   - Order the repositories by the count of commits in descending order.\n   - Limit the result to the top five repositories with the highest number of commits.\n\n6. **Select Required Fields:**\n   - From the final ordered list, select and display the repository names and their corresponding commit counts.",
        "special_function": [
            "numbering-functions/RANK",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq251",
        "db": "bigquery-public-data.github_repos\nspider2-public-data.pypi",
        "question": "Could you find the GitHub URL of the Python package that has the highest number of downloads on PyPi and was updated most recently? Please ensure that only the main repository URL is provided, excluding specific subsections like issues, blobs, pull requests, or tree views.",
        "external_knowledge": null,
        "plan": "1. Extract metadata including pypi_name, project_urls for PyPi packages.\n2. Filter the project_urls from PyPiData to extract clean GitHub repository URLs.\n3. Get the most recent version of each PyPi package based on the upload_time.\n4. Compute total downloads, yearly downloads for the past three years, and capture the earliest and latest download timestamps.\n5. Retrieve the number of watchers for each GitHub repository.\n6. Calculate the total bytes of code written in Python and the total bytes of code across all languages to get language data.\n7. Sort all data collected according to pypi_downloads and return the most downloaded package URL.\n",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/REGEXP_REPLACE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq252",
        "db": "bigquery-public-data.github_repos",
        "question": "Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?",
        "external_knowledge": null,
        "plan": "1. Get file details and remove potential duplicates by file id.\n2. Filter the results to include only non-binary files with a `.swift` extension.\n3. Sort and return the name of the file which has the highest number of copies.",
        "special_function": null
    },
    {
        "instance_id": "bq019",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the most common inpatient diagnosis in the US in 2014, what was the citywise average payment respectively in the three cities that had the most cases?",
        "external_knowledge": null,
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.inpatient_charges_2014`\n2. Rank all the diagnostic conditions by national number of cases to find the most common diagnostic condition.\n3. Group the data by diagnostic condition name, city and state, and calculate the citywise number of cases, citywise average payment for each entry.\n4. Accordingly, rank the cities by citywise number of cases for the most common diagnostic condition, and calculate the national avg. payments.\n5. Limit to the top 3 cities.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq019_1",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "What is the most prescribed medication in each state in 2014?",
        "external_knowledge": null,
        "plan": "1. Decide which table to work on:  `bigquery-public-data.cms_medicare.part_d_prescriber_2014`\n2. Group the data by drug name and state, and calculate the total claim count for each drug in each state.\n3. For each state, find out the max claim count number among different drugs. (most prescribed)\n4. List out corresponding most prescribed drug name for each state. (don\u2018t need to be in order",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq019_2",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "Can you tell me which healthcare provider incurs the highest combined average costs for both outpatient and inpatient services in 2014?",
        "external_knowledge": null,
        "plan": "1. find out the corresponding table as described in the instruction `bigquery-public-data.cms_medicare.outpatient_charges_2014` and `bigquery-public-data.cms_medicare.inpatient_charges_2014`\r\n2. Group the outpatient charges data by different medical provider, and calculate the average outpatient cost per service.\r\n3. Do the same to inpatient charges data.\r\n4. Combine them to calculate the sum of two average costs for each medical provider.\r\n5. Rank them to find the peak. (the highest one)",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq172",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the drug with the highest total number of prescriptions in New York State during 2014, could you list the top five states with the highest total claim counts for this drug? Please also include their total claim counts and total drug costs. ",
        "external_knowledge": null,
        "plan": "1. **Identify the Top Drug in NY:**\r\n   - Create a Common Table Expression (CTE) named `ny_top_drug`.\r\n   - Select the `generic_name` of drugs as `drug_name` and calculate the total number of claims (`total_claim_count`) for each drug.\r\n   - Filter the data to include only records where the provider state (`nppes_provider_state`) is 'NY'.\r\n   - Group the results by `drug_name`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top drug (highest `total_claim_count`).\r\n\r\n2. **Identify the Top 5 States for the Top Drug:**\r\n   - Create a second CTE named `top_5_states`.\r\n   - Select the provider state (`nppes_provider_state`) as `state`, and calculate the total number of claims (`total_claim_count`) and total drug cost (`total_drug_cost`) for each state.\r\n   - Filter the data to include only records where the `generic_name` matches the top drug identified in `ny_top_drug`.\r\n   - Group the results by `state`.\r\n   - Order the results by `total_claim_count` in descending order.\r\n   - Limit the results to the top 5 states with the highest `total_claim_count`.\r\n\r\n3. **Return the Final Results:**\r\n   - Select the `state`, `total_claim_count`, and `total_drug_cost` from the `top_5_states` CTE to produce the final output.\r\n\r\nThis sequence ensures that we first determine the most prescribed drug in New York, then find the top 5 states where this drug is most frequently prescribed, and finally, present the relevant statistics for these states.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq172_1",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "For the provider with the highest total inpatient service cost from 2011-2015, tell me its annual inpatient and outpatient costs for each year during that period.",
        "external_knowledge": null,
        "plan": "1. **Identify Provider with Highest Inpatient Service Cost (2011-2015)**\r\n   - **Combine Inpatient Data:** Use `UNION ALL` to merge inpatient charge data from 2011 through 2015.\r\n   - **Calculate Total Inpatient Cost:** For each provider, compute the total inpatient cost as the sum of `average_medicare_payments` multiplied by `total_discharges`.\r\n   - **Find Top Provider:** Group the results by `provider_id`, order them by `total_ip_cost` in descending order, and limit the output to the top provider.\r\n\r\n2. **Extract Provider ID with Highest Inpatient Cost**\r\n   - **Select Provider ID:** From the previous step, extract the `provider_id` of the provider with the highest total inpatient service cost.\r\n\r\n3. **Retrieve Annual Inpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Inpatient Data:** For the identified provider, fetch the inpatient data from `cms_medicare.inpatient_charges_*`.\r\n   - **Calculate Annual Average Inpatient Cost:** Compute the average inpatient cost per year by averaging `average_medicare_payments` multiplied by `total_discharges`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n4. **Retrieve Annual Outpatient Costs for Identified Provider (2011-2015)**\r\n   - **Query Outpatient Data:** For the identified provider, fetch the outpatient data from `cms_medicare.outpatient_charges_*`.\r\n   - **Calculate Annual Average Outpatient Cost:** Compute the average outpatient cost per year by averaging `average_total_payments` multiplied by `outpatient_services`. Group the data by `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n5. **Join Inpatient and Outpatient Data**\r\n   - **Merge Inpatient and Outpatient Data:** Perform a LEFT JOIN on the inpatient and outpatient datasets based on `provider_id`, `provider_state`, `provider_city`, `provider_name`, and year.\r\n\r\n6. **Format and Order Final Results**\r\n   - **Select and Format Fields:** Select relevant fields including `State`, `City`, `Provider_ID`, `Provider_Name`, year, and round the average inpatient and outpatient costs.\r\n   - **Order by Year:** Sort the final results by year for chronological presentation.\r\n\r\nThis plan ensures the correct identification of the top provider based on inpatient costs and retrieves comprehensive annual cost data for both inpatient and outpatient services for detailed analysis.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq032",
        "db": "bigquery-public-data.noaa_hurricanes",
        "question": "Can you provide the latitude of the final coordinates for the hurricane that traveled the second longest distance in the North Atlantic during 2020?",
        "external_knowledge": null,
        "plan": "1. Select hurricane data for the 2020 season located in the North Atlantic (NA).\n2. Obtain position change information for each hurricane based on the time of movement.\n3. Calculate the total distance traveled by each hurricane.\n4. Rank hurricanes based on their total travel distances and select the hurricane that ranks second.\n5. Output the coordinate changes for the movement of this hurricane\n6. Only retain the last coordinate of this hurricane",
        "special_function": [
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_Y",
            "navigation-functions/LAG",
            "numbering-functions/DENSE_RANK"
        ]
    },
    {
        "instance_id": "bq032_1",
        "db": "bigquery-public-data.noaa_hurricanes",
        "question": "Please show information of the hurricane with the third longest total travel distance, including its travel coordinates, the cumulative travel distance at each point, and the maximum sustained wind speed at those times.",
        "external_knowledge": null,
        "plan": "1. **Filter Data**:\n   - Start with a dataset containing records of a specific event type for a particular year and region.\n   - Exclude entries that are not named.\n\n2. **Create Geometric Points**:\n   - For each record, create a geographic point from the given coordinates.\n\n3. **Calculate Distances**:\n   - For each event, calculate the distance between consecutive geographic points in kilometers.\n   - Use a window function to compute the distance between each point and the previous point, ordered by time.\n\n4. **Cumulative Distance Calculation**:\n   - For each event, calculate the cumulative distance traveled up to each point in time.\n   - Also, compute the total distance traveled by each event over its entire duration.\n\n5. **Rank Events**:\n   - Rank all events based on their total travel distance, with the longest distance receiving the highest rank.\n\n6. **Select Specific Rank**:\n   - Select the records for the event that has the third longest total travel distance.\n\n7. **Final Output**:\n   - From the selected event, extract and display the geographic points, cumulative distances at each point, and the maximum wind speed recorded at those points.\n   - Order the results by cumulative distance.",
        "special_function": [
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "navigation-functions/LAG",
            "numbering-functions/DENSE_RANK"
        ]
    },
    {
        "instance_id": "bq117",
        "db": "bigquery-public-data.noaa_historic_severe_storms",
        "question": "What is the total number of severe storm events that occurred in the most affected month over the past 15 years according to NOAA records, considering only the top 100 storm events with the highest property damage?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Define a Subquery for Base Information:**\n   - Create a temporary table to aggregate storm event data.\n   - Extract unique event identifiers and calculate the earliest occurrence time for each event.\n   - Format the month and year of the earliest occurrence into a string.\n   - Aggregate unique county names, state names, and event types, limiting each to a certain number of unique values.\n   - Sum the property damage values and convert the total to billions.\n\n2. **Filter Data for the Past 15 Years:**\n   - Restrict the data to the past 15 years by comparing the year of the event to the current year, using a dynamic range calculated from the current date.\n\n3. **Group and Sort Data:**\n   - Group the data by unique event identifiers.\n   - Sort the aggregated results by the total property damage in descending order.\n   - Limit the results to the top 100 events with the highest property damage.\n\n4. **Count Events by Month:**\n   - From the aggregated results, count the number of events that occurred in each month.\n\n5. **Determine the Most Affected Month:**\n   - Group the counted results by month.\n   - Sort the counts in descending order to identify the month with the highest number of severe storm events.\n   - Limit the results to the top month (the one with the highest count).\n\n6. **Return the Final Result:**\n   - Output the total number of severe storm events that occurred in the most affected month over the past 15 years.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "date-functions/CURRENT_DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq071",
        "db": "bigquery-public-data.noaa_hurricanes\nbigquery-public-data.geo_us_boundaries",
        "question": "What are the top 10 zip codes of the areas in the United States that have been affected by the most named hurricanes?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Select Required Columns**: Begin by selecting the necessary columns which include the city, zip code, state, the count of distinct hurricane names, and a concatenated string of distinct hurricane names.\n\n2. **Join Two Data Sources**: Combine data from the zip codes dataset and the hurricanes dataset. This will allow you to link hurricanes to the zip codes they have affected.\n\n3. **Filter Data**: Use a spatial function to filter and ensure that only hurricanes whose coordinates fall within the geometric boundaries of the zip codes are considered. Additionally, exclude hurricanes that are not named.\n\n4. **Group Data**: Group the filtered data by zip code, city, and state. This allows for aggregation operations to be performed on each group.\n\n5. **Count Distinct Hurricanes**: For each group, count the number of distinct hurricanes that have affected that area.\n\n6. **Concatenate Hurricane Names**: Create a concatenated string of the names of the distinct hurricanes for each group.\n\n7. **Order and Limit Results**: Order the results by the count of distinct hurricanes in descending order to identify the areas most affected by named hurricanes. Limit the output to the top 10 zip codes.\n\n8. **Return Final Output**: Display the final output with columns for city, zip code, state, the count of hurricanes, and the concatenated hurricane names.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_WITHIN",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq117_1",
        "db": "bigquery-public-data.noaa_historic_severe_storms\nbigquery-public-data.geo_us_boundaries",
        "question": "What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "What zip codes have experienced the most hail storms in the last 10 years?\nThis query combines the severe weather events dataset with the zip code boundary data  available as a BigQuery Public Dataset to group hail events by zip code over the last 10 years.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/CURRENT_DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_WITHIN",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/LOWER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq356",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "What is the number of weather stations where the valid temperature record days in 2019 reached 90% or more of the maximum number of recorded days, and have had tracking back to 1/1/2000 or before and through at least 6/30/2019?",
        "external_knowledge": null,
        "plan": "1. **Subquery to Count Valid Temperature Record Days by Station in 2019:**\n    - Create a subquery to count the number of distinct dates with valid temperature data for each station.\n    - Convert year, month, and day information into a date format.\n    - Filter out records with missing temperature data and invalid temperature values.\n    - Group the results by each weather station.\n\n2. **Subquery to Calculate Maximum Number of Valid Temperature Record Days in 2019:**\n    - Create another subquery to find the maximum number of valid temperature record days across all stations for the year 2019.\n\n3. **Main Query to Count Qualifying Weather Stations:**\n    - Select the main weather stations table.\n    - Perform an inner join with the subquery from step 1 to filter only those stations that have temperature data in 2019.\n    - Cross join with the subquery from step 2 to make the maximum number of valid temperature record days available for filtering.\n    - Apply additional filtering conditions:\n        - Include only stations that have been tracking data since at least January 1, 2000.\n        - Include only stations that have continued tracking data through at least June 30, 2019.\n        - Include only stations where the number of valid temperature record days in 2019 is at least 90% of the maximum number of valid temperature record days across all stations for 2019.\n\n4. **Count and Return the Number of Stations Meeting All Criteria:**\n    - Count the number of stations that meet all the specified conditions and return this count as the final result.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/LEAST"
        ]
    },
    {
        "instance_id": "bq357",
        "db": "bigquery-public-data.noaa_icoads",
        "question": "What are the latitude and longitude coordinates and dates between 2005 and 2015 with the top 5 highest daily average wind speeds, excluding records with missing wind speed values?",
        "external_knowledge": null,
        "plan": "1. **Data Aggregation Preparation**:\n   - Create a temporary table to store aggregated daily average values.\n   - Select the necessary date and location columns.\n   - Calculate the average value of the target variable for each unique combination of date and location.\n\n2. **Filtering by Date Range**:\n   - Ensure that only the records within the specified date range are considered for aggregation.\n\n3. **Grouping Data**:\n   - Group the data by date and location to facilitate the calculation of daily averages.\n\n4. **Calculate Daily Averages**:\n   - Compute the average value of the target variable for each group (combination of date and location).\n\n5. **Exclude Missing Values**:\n   - Filter out any records where the computed average is missing.\n\n6. **Sort and Limit Results**:\n   - Sort the resulting dataset by the calculated average in descending order.\n   - Limit the results to the top 5 records with the highest daily averages.\n\n7. **Final Selection**:\n   - Select the required columns from the temporary table to present the final output, including date, location, and the calculated averages.",
        "special_function": null
    },
    {
        "instance_id": "bq181",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "How much percentage of weather stations recorded temperature data for at least 90% of the days in 2022?",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of days in 2022 that it records the temperature data (not missing) for each station.\n2. Find out the total number of valid stations.\n3. Leverage the knowledge that 2022 has 365 days.\n4. Join the table with the stations table to find out the stations which records for at least 90% of days in 2022.\n5. Calculate the percentage of stations with over 90% records .",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq045",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Which weather stations in Washington State had more than 150 rainy days in 2023 but fewer rainy days than in 2022?",
        "external_knowledge": null,
        "plan": "1. Extract data for each weather station for the years 2023 and 2022, focusing on entries that record precipitation levels.\n2. Filter these records to retain only the days with precipitation greater than zero as rainy days.\n3. Merge the records from 2023 and 2022, retaining only those stations where 2023 had more than 150 rainy days and less precipitation than in 2022.\n4. Query station information to obtain the names of these weather stations.",
        "special_function": [
            "aggregate-functions/ANY_VALUE"
        ]
    },
    {
        "instance_id": "bq290",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Can you calculate the difference in maximum temperature, minimum temperature, and average temperature between US and UK weather stations for each day in October 2023, excluding records with missing temperature values?",
        "external_knowledge": null,
        "plan": "1. **Filter Relevant Stations**:\n   - Identify and select weather stations located in the specified countries.\n\n2. **Join Station Data with Weather Data**:\n   - Combine the weather station data with the weather observations for the specified year.\n   - Filter the combined data to include only the records within the specified date range.\n   - Exclude records with missing temperature values.\n\n3. **Calculate Daily Metrics for US**:\n   - For the filtered data, compute the average, minimum, and maximum temperatures for each day in the specified date range for stations in one of the specified countries.\n   - Group the results by date.\n\n4. **Calculate Daily Metrics for UK**:\n   - Repeat the previous step for the other specified country.\n\n5. **Calculate Temperature Differences**:\n   - For each day, compute the difference in maximum, minimum, and average temperatures between the two countries.\n   - Join the daily metrics from both countries on the date to perform these calculations.\n\n6. **Output Results**:\n   - Select the computed temperature differences and the corresponding dates.\n   - Order the results by date for a chronological output.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq031",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Show me the daily weather data (temperature, precipitation, and wind speed) in Rochester for the first season of year 2019, converted to Celsius, centimeters, and meters per second, respectively. Also, include the moving averages (window size = 8) and the differences between the moving averages for up to 8 days prior (all values rounded to one decimal place, sorted by date in ascending order, and records starting from 2019-01-09).",
        "external_knowledge": null,
        "plan": "1. Data Transformation and Filtering:\n   - Convert the date components into a proper date format.\n   - Transform temperature from Fahrenheit to Celsius, precipitation from inches to centimeters, and wind speed from knots to meters per second.\n   - Filter the data to include only the records for the specified location and year.\n\n2. Calculation of Moving Averages:\n   - Calculate the moving averages for temperature, precipitation, and wind speed using a window size of 8 days.\n\n3. Lagging the Moving Averages:\n   - Create lagged versions of the moving averages for up to 8 days prior.\n\n4. Calculating Differences:\n   - Compute the differences between the current moving averages and their lagged versions for up to 8 days.\n\n5. Final Selection and Sorting:\n   - Select the relevant columns, round the values to one decimal place, ensure the lagged values are not null, and sort the results by date in ascending order.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG"
        ]
    },
    {
        "instance_id": "bq050",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.noaa_gsod\nbigquery-public-data.new_york_citibike\nspider2-public-data.cyclistic",
        "question": "Help me look at the total number of bike trips, average trip duration (in minutes), average daily temperature, wind speed, and precipitation when trip starts (rounded to 1 decimal), as well as the month with the most trips (e.g., `4`), categorized by different starting and ending neighborhoods in New York City for the year 2014.",
        "external_knowledge": null,
        "plan": "1. Data Preparation: extract necessary data for each trip, including start and end locations, trip duration, weather conditions, and the month of the trip start.\n   - Join multiple tables to gather geographical and weather information for each trip.\n   - Use function `ST_WITHIN` and `ST_GEOGPOINT` to determine the zip codes of the start and end station.\n   - Extract the borough and neighborhood based on the zip code.\n\n2. Filtering and Transformation:\n   - Filter the data to include only trips that started in the year 2014.\n   - Ensure the weather data corresponds to the specified weather station `New York Central Park`.\n   - Convert trip duration to minutes and round weather metrics to one decimal place.\n\n3. Aggregation:\n   - Group the data by starting and ending locations.\n   - Calculate aggregate metrics: total number of trips, average trip duration, average temperature, average wind speed, and average precipitation.\n\n4. Identify Most Common Month:\n   - Determine the month with the highest number of trips for each start and end location combination.\n   - Combine this information with the aggregated data to provide a comprehensive view.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_WITHIN",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "string-functions/CONCAT",
            "string-functions/LOWER",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq291",
        "db": "spider2-public-data.noaa_global_forecast_system",
        "question": "What is the daily weather forecast summary, including temperature stats (max, min and average), precipitation, cloud cover, snow and rain, for the specified location (latitude 17.5, longitude 23.25) and on November 28, 2021?",
        "external_knowledge": null,
        "plan": "1. **Define Subquery for Daily Forecasts:**\n   - Create a subquery to gather relevant weather data for the specified location and date.\n   - Convert forecast times to local dates by adding 1 hour and extracting the date.\n\n2. **Calculate Temperature Statistics:**\n   - Compute the maximum temperature for the day by selecting the highest value of temperature data.\n   - Compute the minimum temperature for the day by selecting the lowest value of temperature data.\n   - Compute the average temperature for the day by averaging the temperature data.\n\n3. **Calculate Precipitation:**\n   - Sum up all the precipitation data to get the total precipitation for the day.\n\n4. **Calculate Cloud Cover:**\n   - Calculate the average cloud cover during daytime hours (10:00 AM to 5:00 PM).\n\n5. **Calculate Snow and Rain:**\n   - Calculate the total snow by summing up precipitation if the average temperature is below freezing.\n   - Calculate the total rain by summing up precipitation if the average temperature is above or equal to freezing.\n\n6. **Filter and Group Data:**\n   - Filter the data to include only the forecasts created on the specified date.\n   - Ensure the data is for the specified geographical location within a 5 km radius.\n   - Group the data by creation time and local forecast date.\n\n7. **Select Final Output:**\n   - Select the relevant columns from the subquery to be included in the final output.\n   - Order the results by creation time and local forecast date.\n\n8. **Return Results:**\n   - Return the daily weather forecast summary including temperature statistics, precipitation, cloud cover, snow, and rain for the specified location and date.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "datetime-functions/DATETIME_ADD",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "time-functions/TIME",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq208",
        "db": "bigquery-public-data.new_york\nbigquery-public-data.noaa_gsod",
        "question": "Can you provide weather stations within a 20-mile radius of Chappaqua, New York (Latitude: 41.197, Longitude: -73.764), and tell me the number of valid temperature observations they have recorded from 2011 to 2020?",
        "external_knowledge": null,
        "plan": "1. **Define Location and Home**:\n   - Create a temporary reference for the specific location (latitude and longitude) and assign a name to it.\n\n2. **Fetch and Enhance Weather Stations Data**:\n   - Retrieve data about weather stations, adding a geographical point (latitude and longitude) for each station.\n\n3. **Identify Nearby Stations**:\n   - Calculate the distance between the specified location and each weather station.\n   - Filter out stations that are within a 20-mile radius of the specified location.\n   - Group these stations by their proximity to the specified location.\n\n4. **Extract Closest Station Details**:\n   - Flatten the grouped data to get detailed information about each nearby station.\n\n5. **Count Valid Temperature Observations**:\n   - Join the detailed weather station information with temperature observation data.\n   - Filter out invalid temperature readings.\n   - Restrict the data to observations recorded between the years 2011 and 2020.\n   - Count the number of valid temperature observations for each station within the specified radius.\n   - Group the results by station details and order them by proximity.\n\nThis plan ensures that the query retrieves the required weather stations within the given radius and accurately counts their valid temperature observations over the specified period.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "geography-functions/ST_ASTEXT",
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq017",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "What are the five longest types of highways within the multipolygon boundary of Japan (as defined by Wikidata ID 'Q17') by total length?",
        "external_knowledge": null,
        "plan": "1.Select records identified by Wikidata ID Q17 to capture the geographic area of Japan, retrieving geometries of the 'multipolygons' feature type.\n2.Choose records of the 'lines' feature type, which typically represent highways and other linear geographic features, ensuring that the highway data's geometry intersects with the boundaries of Japan.\n3.Extract the type of each highway and calculate the length of these highways.\n4.Group the results by highway type and then order them by the total length of the highways in descending order.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_LENGTH",
            "interval-functions/EXTRACT",
            "string-functions/FORMAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq131",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of San Francisco (as defined by Wikidata ID 'Q62')?",
        "external_knowledge": null,
        "plan": "1. Retrieve the 'multipolygons' geometry for the area identified by Wikidata ID Q62, setting the boundaries for the query.\n2. Choose point features tagged as 'highway' with 'bus_stop' values, ensuring they are within the defined area.\n3. Extract the bus network information from the 'network' key for each stop, and count the number of stops per network.\n4. Group the results by bus network and order them by the descending count of stops to identify the network with the most stops.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "geography-functions/ST_DWITHIN",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq106",
        "db": "bigquery-public-data.geo_openstreetmap\nspider2-public-data.noaa_global_forecast_system",
        "question": "For the two-week forecast made on November 28th, 2021, how much did temperatures in Sudan deviate from the predicted value?",
        "external_knowledge": null,
        "plan": "1. Define the Boundary for Sudan:\n   - Create a Common Table Expression (CTE) to select geographic boundary data for Sudan.\n   - Filter the `planet_layers` table to include only rows where `layer_class` is 'boundary' and `layer_name` is 'national'.\n   - Use an `EXISTS` clause to ensure the boundary is for Sudan by checking the `ISO3166-1` tag in `all_tags`.\n\n2. Select Relevant Forecast Data:\n   - Create another CTE to select Global Forecast System (GFS) data from the `NOAA_GFS0P25` table.\n   - Filter the GFS data to include only those records created on '2021-11-28T00:00:00'.\n   - Ensure the selected forecast points are within the geographic boundary of Sudan using `ST_WITHIN`.\n\n3. Calculate Predicted Temperatures:\n   - Create a CTE named to calculate average temperatures predicted for each day within the two-week period.\n   - Join with the `forecast` array, and filter the forecast to include only daily data (`MOD(hours, 24) = 0`) within the first 14 days (`hours / 24 <= 14`).\n   - Group by `creation_time` and `time` to compute the average daily temperature.\n\n4. Fetch Observed Temperatures:\n   - Create a CTE named to compare predicted temperatures against observed temperatures.\n   - Join the predicted temperatures with the GFS data (`NOAA_GFS0P25` table) to find actual observations at the predicted times.\n   - Use `ST_WITHIN` to ensure observed points are within the Sudan boundary.\n   - Group the results to calculate the average observed temperature for each predicted time.\n\n5. Calculate Prediction Error and Return Query Execution:\n   - Select the deviation (error) between observed and predicted temperatures by subtraction.\n   - Sort the results by observed time.",
        "special_function": [
            "geography-functions/ST_WITHIN",
            "mathematical-functions/MOD",
            "time-functions/TIME",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq293",
        "db": "bigquery-public-data.geo_us_boundaries\nbigquery-public-data.new_york",
        "question": "What were the top 5 busiest pickup times and locations (by ZIP code) for yellow taxi rides in New York City on January 1, 2015? Additionally, provide detailed metrics for each of these top 5 records, including the count of rides, hourly, daily, and weekly lagged counts, as well as 14-day and 21-day average and standard deviation of ride counts.",
        "external_knowledge": null,
        "plan": "1. **Define `base_data` CTE:**\r\n   - **Select Data:** Fetch data from the `bigquery-public-data.new_york.tlc_yellow_trips_2015` table for trips on '2015-01-01' where `pickup_latitude` is between -90 and 90.\r\n   - **Filter and Join:** Join the filtered taxi data (`nyc_taxi`) with the `bigquery-public-data.geo_us_boundaries.zip_codes` table for New York state (`state_code = 'NY'`), using spatial containment (`ST_CONTAINS`) to match pickups to zip codes, excluding the `zip_code_geom` field from the `gis` table.\r\n\r\n2. **Define `distinct_datetime` CTE:**\r\n   - **Extract Unique Hours:** Select distinct `pickup_datetime` truncated to the hour from `base_data`.\r\n\r\n3. **Define `distinct_zip_code` CTE:**\r\n   - **Extract Unique Zip Codes:** Select distinct `zip_code` from `base_data`.\r\n\r\n4. **Define `zip_code_datetime_join` CTE:**\r\n   - **Cartesian Product:** Perform a CROSS JOIN between `distinct_zip_code` and `distinct_datetime` to create combinations of all zip codes and hours.\r\n   - **Extract Date Parts:** Add columns to extract month, day, weekday, hour, and a flag (`is_weekend`) indicating if the day is a weekend.\r\n\r\n5. **Define `agg_data` CTE:**\r\n   - **Aggregate Data:** Aggregate `base_data` by `zip_code` and `pickup_hour`, counting the number of pickups per hour per zip code.\r\n\r\n6. **Define `join_output` CTE:**\r\n   - **Left Join Aggregated Data:** Perform a LEFT JOIN between `zip_code_datetime_join` and `agg_data` to include pickup counts (`cnt`), filling missing counts with 0 using `IFNULL`.\r\n\r\n7. **Define `final_output` CTE:**\r\n   - **Calculate Lag and Rolling Stats:** Compute lag values and rolling averages/std deviations over different time windows (1 hour, 1 day, 7 days, 14 days, 21 days) for each `zip_code` and `pickup_hour`.\r\n\r\n8. **Final Selection and Ordering:**\r\n   - **Select and Limit:** Select all columns from `final_output`, order by `cnt` in descending order, and limit the result to the top 5 records.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/DATETIME_TRUNC",
            "datetime-functions/EXTRACT",
            "datetime-functions/FORMAT_DATETIME",
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_GEOGPOINT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "statistical-aggregate-functions/STDDEV",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq250",
        "db": "bigquery-public-data.geo_openstreetmap\nbigquery-public-data.worldpop",
        "question": "What is the total population living on the geography grid which is the farthest from any hospital in Singapore, based on the most recent population data before 2023? Note that geographic grids and distances are calculated based on geospatial data and GIS related functions.",
        "external_knowledge": "OpenStreetMap_data_in_layered_GIS_format.md",
        "plan": "1. Simply define a single value 'Singapore' as the country of interest.\n2. Find the most recent date when the population data was last updated for Singapore.\n3. Calculate the total population of Singapore and create a bounding box that encompasses the entire country.\n4. Select the geometries of hospitals and doctors that are within the bounding box of Singapore.\n5. Calculate the minimum distance between each populated grid cell and the nearest hospital or doctor.\n6. Aggregate the total population of all grid cells and return the total population that lives farthest away from hospitals and doctors in Singapore. \n",
        "special_function": [
            "geography-functions/ST_CENTROID_AGG",
            "geography-functions/ST_CONVEXHULL",
            "geography-functions/ST_DISTANCE",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_INTERSECTS",
            "geography-functions/ST_UNION_AGG"
        ]
    },
    {
        "instance_id": "bq056",
        "db": "bigquery-public-data.geo_openstreetmap\nbigquery-public-data.geo_us_boundaries",
        "question": "How many different motorway road pairs are there in Minnesota that overlap each other but do not share nodes and do not have a bridge?",
        "external_knowledge": null,
        "plan": "1. Filter by State and Road Type: Identify the specified state and road type from the geographical boundaries and road data tables.\n\n2. Select Relevant Roads: Extract unique identifiers for roads that match the specified type (e.g., motorway) from the road data.\n\n3. Identify Roads Within State: Match the roads with the geographical boundary of the specified state to ensure only roads within the state are considered.\n\n4. Find Overlapping Roads: Determine pairs of roads that geographically overlap each other but do not share nodes.\n\n5. Exclude Roads with Bridges: Filter out road pairs where either road has a bridge attribute.\n\n6. Count Overlapping Pairs: Count the number of unique road pairs that overlap but do not share nodes and do not have a bridge.",
        "special_function": [
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_INTERSECTS",
            "json-functions/STRING",
            "mathematical-functions/GREATEST",
            "mathematical-functions/LEAST",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/DECLARE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq253",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "Can you find the name of the relation from the OpenStreetMap data that has the most associated features within the same bounding area as the multipolygon with Wikidata item Q218, where the relation itself does not have a Wikidata tag but its name is specified, and at least one of its associated features does have a Wikidata tag?",
        "external_knowledge": null,
        "plan": "1. Extract the geometry of multipolygon features having a wikidata tag.\n2. Get planet_relations without a wikidata tag, but whose geometries are within the bounding_area.\n3. Count the number of OpenStreetMaps each planet relation has through osm_id.\n4. Sort and return the planet relation id which has most OSMs.\n",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq254",
        "db": "bigquery-public-data.geo_openstreetmap",
        "question": "Can you find the name of the multipolygon with a valid id that ranks second in terms of the number of points within its boundary, among those multipolygons that do not have a Wikidata tag but are located within the same geographic area as the multipolygon associated with Wikidata item Q218? ",
        "external_knowledge": null,
        "plan": "1. Define a bouding_area which has the geometry of a specific multipolygon with a Wikidata ID 'Q218'.\n2. Retrieve all the features that are within the bounding_area.\n3. Select the multipolygons within the bounding_area that do not have a Wikidata tag and have a non-null osm_id, and retrieve their osm_id, name, and geometry.\n4. Count the number of OpenStreetMaps each polygon has.\n5. Sort and return the name of the polygon which has the second highest OSM number.\n",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq184",
        "db": "bigquery-public-data.crypto_ethereum",
        "question": "I want to compute and compare the cumulative count of Ethereum smart contracts created by users versus created by other contracts. Please list out the daily cumulative tallies in August 2021.",
        "external_knowledge": null,
        "plan": "1. **Filter Data for User-Created Contracts:**\n   - Extract records where the entity type is contract creation and the creator is a user (not another contract).\n   - Group these records by date and count the number of creations per day.\n   \n2. **Calculate Cumulative Sum for User-Created Contracts:**\n   - Compute the running total of contract creations by users, ordered by date.\n   - Determine the next date for each record to help with calendar alignment.\n\n3. **Generate Calendar for August 2021:**\n   - Create a list of all dates in August 2021 to ensure all dates are covered in the final output, even if no contracts were created on some days.\n\n4. **Align User-Created Contracts with Calendar:**\n   - Join the cumulative sum data with the calendar dates to ensure there is a record for each day in August 2021.\n   - Ensure the cumulative sum is carried forward to the next date where appropriate.\n\n5. **Filter Data for Contract-Created Contracts:**\n   - Extract records where the entity type is contract creation and the creator is another contract.\n   - Group these records by date and count the number of creations per day.\n\n6. **Calculate Cumulative Sum for Contract-Created Contracts:**\n   - Compute the running total of contract creations by contracts, ordered by date.\n   - Determine the next date for each record to help with calendar alignment.\n\n7. **Generate Calendar for August 2021 (Again):**\n   - Create a list of all dates in August 2021 to ensure all dates are covered in the final output, even if no contracts were created on some days (separate calendar for the second cumulative sum).\n\n8. **Align Contract-Created Contracts with Calendar:**\n   - Join the cumulative sum data with the calendar dates to ensure there is a record for each day in August 2021.\n   - Ensure the cumulative sum is carried forward to the next date where appropriate.\n\n9. **Combine and Compare Results:**\n   - Join the two cumulative sum datasets (user-created and contract-created) on the date to get a consolidated view.\n   - Select the date, cumulative sum of user-created contracts, and cumulative sum of contract-created contracts for the final output.\n\n10. **Order Final Output:**\n    - Ensure the final result is ordered by date for clear comparison.",
        "special_function": [
            "array-functions/GENERATE_DATE_ARRAY",
            "date-functions/DATE",
            "navigation-functions/LEAD",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq195",
        "db": "spider2-public-data.crypto_ethereum",
        "question": "What are the top 10 Ethereum addresses by balance, considering both value transactions and gas fees, before September 1, 2021?",
        "external_knowledge": null,
        "plan": "1. **Define the Value Table:**\n   - Create a temporary table to aggregate all relevant balance changes for addresses.\n   \n2. **Select Incoming Transactions:**\n   - Extract all transactions where the address is the recipient and the transaction was successful before the specified date.\n   - Include only specific types of calls (exclude 'delegatecall', 'callcode', 'staticcall') or if the call type is null.\n\n3. **Select Outgoing Transactions:**\n   - Extract all transactions where the address is the sender and the transaction was successful before the specified date.\n   - Negate the transaction value to represent outgoing funds.\n   - Similar to incoming transactions, exclude certain call types or if the call type is null.\n\n4. **Calculate Miner Rewards:**\n   - Sum the gas fees received by miners for all blocks mined before the specified date.\n   - Group the results by miner address to aggregate total rewards.\n\n5. **Calculate Gas Fees Spent:**\n   - Extract all transactions where the address is the sender and calculate the total gas fees spent.\n   - Negate the value to represent the outgoing gas fees.\n\n6. **Aggregate All Values:**\n   - Combine the results of the above steps into the temporary table, ensuring all values are correctly signed (positive for incoming, negative for outgoing).\n\n7. **Calculate Final Balances:**\n   - Sum all values for each address from the temporary table.\n   - Convert the summed value from the base unit to the standard unit of measurement.\n\n8. **Order and Limit Results:**\n   - Sort the addresses by their calculated balance in descending order.\n   - Limit the results to the top 10 addresses.\n\nBy following these steps, the query identifies the top 10 Ethereum addresses by balance before the specified date, considering both transaction values and gas fees.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/FLOOR",
            "mathematical-functions/POWER"
        ]
    },
    {
        "instance_id": "bq256",
        "db": "spider2-public-data.crypto_ethereum",
        "question": "What is the balance of the Ethereum address that initiated the highest number of transactions before September 1, 2021?",
        "external_knowledge": null,
        "plan": "1. Gather transaction details like addresses, values, and gas information for Ethereum transactions up to September 1, 2021.\n2. Calculate the net balance for each address by aggregating incoming and outgoing transaction values.\n3. Count the number of transactions received by each address.\n4. Count the number of transactions sent by each address.\n5. Calculate the balance for senders with the most transactions and return.\n",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/POWER"
        ]
    },
    {
        "instance_id": "bq341",
        "db": "spider2-public-data.crypto_ethereum",
        "question": "Which Ethereum address has the smallest positive balance from transactions involving the token at address \"0xa92a861fc11b99b24296af880011b47f9cafb5ab\" after March 17, 2023?",
        "external_knowledge": null,
        "plan": "1. **Filter Transactions**: Extract all transactions involving the specified token address that occurred after the given date. For each transaction, capture the sender, recipient, and value of the transaction.\n\n2. **Calculate Outgoing Balances**: Aggregate the negative values of all transactions sent from each address. This will represent the total value sent by each address.\n\n3. **Calculate Incoming Balances**: Aggregate the positive values of all transactions received by each address. This will represent the total value received by each address.\n\n4. **Combine Balances**: Create a unified list of addresses by combining the outgoing and incoming balances, ensuring that each address appears only once in the list with its corresponding balance.\n\n5. **Filter Positive Balances**: From the combined list, select only those addresses that have a positive balance.\n\n6. **Find Smallest Positive Balance**: Sort the addresses with positive balances in ascending order of their balances and select the address with the smallest positive balance. Limit the result to only one address.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq336",
        "db": "bigquery-public-data.crypto_bitcoin",
        "question": "Which address has the highest net balance after accounting for both debits and credits?",
        "external_knowledge": null,
        "plan": "1. **Create a Common Table Expression (CTE)**:\n   - Combine two datasets: one representing debit transactions and the other representing credit transactions.\n   - For the debit transactions, select the address, transaction type, and the negative value of the transaction.\n   - For the credit transactions, select the address, transaction type, and the positive value of the transaction.\n   - Use `UNION ALL` to merge these two datasets into a single CTE.\n\n2. **Group and Aggregate**:\n   - Select the address from the combined dataset.\n   - Group the data by the address and transaction type.\n\n3. **Order and Limit Results**:\n   - Calculate the net balance for each address by summing up the transaction values.\n   - Order the results by the net balance in descending order to find the address with the highest net balance.\n   - Limit the result to the top entry to identify the address with the highest net balance.\n\nThis process ensures that you can determine which address has the highest net balance after considering both debits and credits.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING"
        ]
    },
    {
        "instance_id": "bq057",
        "db": "spider2-public-data.crypto_bitcoin",
        "question": "Which month (e.g., 3) in 2021 witnessed the highest percent of Bitcoin volume that took place in CoinJoin transactions? Also give me the percentage of CoinJoins transactions, the average input and output UTXOs ratio, and the proportion of CoinJoin transaction volume for that month (all 1 decimal).",
        "external_knowledge": null,
        "plan": "1. Aggregate Monthly Totals:\n   - Calculate the monthly totals for the number of transactions, input/output counts, and input/output values for Bitcoin transactions within a specified year.\n\n2. Identify Potential CoinJoin Transactions:\n   - Identify potential CoinJoin transactions by selecting those with more than two outputs and an output value less than or equal to the input value, within the same year.\n\n3. Filter CoinJoin Transactions:\n   - Filter these potential CoinJoin transactions to only include those with more than one equal-value output, ensuring the transactions are distinct.\n\n4. Aggregate CoinJoin Monthly Totals:\n   - Aggregate the monthly totals specifically for the identified CoinJoin transactions, including their input/output counts and values.\n\n5. Calculate Percentages:\n   - For each month, calculate the percentage of transactions that were CoinJoin transactions, the average percentage of input and output UTXOs that were part of CoinJoin transactions, and the percentage of the total transaction volume that was involved in CoinJoin transactions.\n\n6. Determine the Month with Highest CoinJoin Volume:\n   - Select the month with the highest percentage of transaction volume involved in CoinJoin transactions, and return this month along with the calculated percentages, formatted to one decimal place.",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq068",
        "db": "bigquery-public-data.crypto_bitcoin_cash",
        "question": "What are the maximum and minimum balances across all addresses for different address types on Bitcoin Cash during January 2019?",
        "external_knowledge": null,
        "plan": "1. Setup Temporary Data:\n    - Create a Common Table Expression (CTE) to consolidate debits and credits for each address on January 1st, 2019.\n    - Debits Calculation:\n        - Select addresses and types from the `inputs` array within transactions.\n        - Negate the `inputs.value` to represent the debit.\n        - Filter for transactions that occurred on '2019-01-01'.\n    - Credits Calculation:\n        - Select addresses and types from the `outputs` array within transactions.\n        - Use `outputs.value` directly to represent the credit.\n        - Filter for transactions that occurred on '2019-01-01'.\n    - Combine both debits and credits using `UNION ALL`.\n\n2. Calculate Address Balances:\n    - Create another CTE to calculate the net balance for each address and type.\n    - Select address, type, and the sum of values from the table defined in Step 1.\n    - Group the results by address and type.\n\n3. Determine Maximum and Minimum Balances:\n    - Create a third CTE to determine the maximum and minimum balances for each address type.\n    - Select type, maximum balance, and minimum balance from  it.\n    - Group and return the results by type.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq092",
        "db": "bigquery-public-data.crypto_dash",
        "question": "Tell me the highest and lowest net changes among all addresses and types on Bitcoin Cash as of January, 2019?",
        "external_knowledge": null,
        "plan": "1. Create a Common Table Expression (CTE) to combine all debit and credit transactions for the specified date, January 1st, 2019.\n    - Debits:\n        - Extract addresses and types from the `inputs` field of the transactions.\n        - Negate the value to represent the debit.\n        - Filter transactions where `block_timestamp_month` equals '2019-01-01'.\n    - Credits:\n        - Extract addresses and types from the `outputs` field of the transactions.\n        - Use the value as-is to represent the credit.\n        - Filter transactions where `block_timestamp_month` equals '2019-01-01'.\n    - Use `UNION ALL` to combine both debit and credit data into a single table.\n\n2. Create a second CTE to calculate the net balance for each unique address and type combination.\n    - Group the data by address and type.\n    - Sum the values to get the net balance for each group.\n\n3. Select the maximum and minimum balances:\n    - Compute the highest and lowest balances.\n    - These values represent the highest and lowest net changes among all addresses and types for the specified date.",
        "special_function": [
            "array-functions/ARRAY_TO_STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq093",
        "db": "bigquery-public-data.crypto_ethereum_classic",
        "question": "Tell me the maximum and minimum net changes in balances for Ethereum Classic addresses on January 1, 2019, considering debits, credits, and gas fees, while excluding internal calls like 'delegatecall', 'callcode', and 'staticcall'.",
        "external_knowledge": null,
        "plan": "1. Create a temporary dataset that combines different types of financial transactions (debits, credits, and fees) affecting account balances.\n- Extract Debits: Identify and include transactions where value is being added to an account. Ensure these transactions are successful and exclude certain internal calls. Limit the data to a specific date.\n- Extract Credits: Identify and include transactions where value is being deducted from an account. Similar to debits, ensure these transactions are successful and exclude specific internal calls. Again, limit the data to the same specific date.\n- Calculate Transaction Fee Debits: Identify and calculate the total transaction fees credited to miners. Ensure the data is limited to the specific date.\n- Calculate Transaction Fee Credits: Identify and calculate the transaction fees debited from the originating accounts. Ensure the data is limited to the specific date.\n\n2. Combine Data: Merge the results of debits, credits, and transaction fees into a single dataset representing changes to account balances.\n\n3. Aggregate Net Changes: Group the combined data by account and calculate the net change in balance for each account.\n\n4. Identify Extremes: Determine the maximum and minimum net changes in balances across all accounts for the specific date.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq037",
        "db": "bigquery-public-data.human_genome_variants",
        "question": "About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.",
        "external_knowledge": null,
        "plan": "1. Filter and extract relevant data:\n   - Use a Common Table Expression (CTE) to select the `reference_bases` and `start_position` columns from the `1000_genomes_phase_3_optimized_schema_variants_20150220` table.\n   - Filter the rows where `reference_bases` is either 'AT' or 'TA'.\n\n2. Compute minimum and maximum start positions:\n   - Use another CTE to compute the minimum (`MIN`) and maximum (`MAX`) start positions for each `reference_bases`.\n   - Also, count the total number of occurrences for each `reference_bases`.\n\n3. Count occurrences of minimum start position:\n   - Use a CTE named to find the number of occurrences of the minimum start position for each `reference_bases`.\n   - Group by `reference_bases` and `start_position`.\n   - Use a subquery in the `HAVING` clause to ensure only rows with the minimum start position for each `reference_bases` are selected.\n\n4. Similarly, count occurrences of maximum start position:\n\n5. Combine results and calculate proportions for the minimum and maximum positions per reference base.",
        "special_function": [
            "conversion-functions/CAST"
        ]
    },
    {
        "instance_id": "bq135",
        "db": "spider2-public-data.crypto_zilliqa",
        "question": "Which date before 2022 had the highest total transaction amount in the Zilliqa blockchain data?",
        "external_knowledge": null,
        "plan": "1. Combine Data Sources: Merge data from two different sources into a single dataset that includes the relevant fields, ensuring all transactions are considered.\n2. Filter Data: Extract transactions that occurred before a specific date, focusing only on those within the desired timeframe.\n3. Aggregate and Identify: Group the filtered transactions by date, calculate the total transaction amount for each date, and identify the date with the highest total transaction amount.",
        "special_function": [
            "date-functions/DATE",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq065",
        "db": "public-data-finance.crypto_band",
        "question": "Provide the most recent 10 results of symbols and their corresponding rates, adjusted for the multiplier, from oracle requests with the script ID 3.",
        "external_knowledge": null,
        "plan": "1. **Parse Aggregator Oracle Requests:**\n   - **Objective:** Extract relevant data (symbols, multiplier, rates) from JSON fields within the `oracle_requests` table for requests with `oracle_script_id = 3`.\n   - **Steps:**\n     - Use `JSON_EXTRACT_ARRAY` to get arrays of symbols and rates from the `calldata` and `result` fields, respectively.\n     - Convert the `symbols` array elements to strings and the `rates` array elements to floating-point numbers.\n     - Extract the `multiplier` as a floating-point number from the `calldata`.\n     - Filter out rows where `calldata` or `result` is NULL.\n     - Collect the extracted arrays and additional fields (`block_timestamp`, `oracle_request_id`) into a temporary table `parsed_aggregator_oracle_requests`.\n\n2. **Zip Symbols and Rates:**\n   - **Objective:** Pair each symbol with its corresponding rate.\n   - **Steps:**\n     - Use `UNNEST` to break down the `symbols` array and pair each symbol with its corresponding rate using the `OFFSET` index.\n     - Ensure the lengths of the `symbols` and `rates` arrays match before proceeding.\n     - Create a struct `zipped` containing the symbol and its associated rate.\n     - Include additional fields (`block_timestamp`, `oracle_request_id`, `multiplier`) in the resulting temporary table `zipped_rates`.\n\n3. **Adjust Rates for Multiplier:**\n   - **Objective:** Adjust each rate by dividing it by the `multiplier`.\n   - **Steps:**\n     - Iterate over the `zipped_rates` table.\n     - For each row, create a new struct `zipped` with the symbol and the adjusted rate (rate divided by multiplier using `IEEE_DIVIDE`).\n     - Store the results along with `block_timestamp` and `oracle_request_id` in the temporary table `adjusted_rates`.\n\n4. **Select Final Adjusted Rates:**\n   - **Objective:** Retrieve and display the final adjusted rates.\n   - **Steps:**\n     - Select `block_timestamp`, `oracle_request_id`, `zipped.symbol`, and `zipped.rate` from the `adjusted_rates` table.\n     - Optionally, a filter can be applied to select specific symbols (commented out in the provided query).\n     - Order the results by `block_timestamp` in descending order.\n     - Limit the output to the top 10 rows.\n\nThis reference plan outlines the logical flow and transformations applied to the data to achieve the intended result of parsing, zipping, adjusting, and selecting specific oracle request data.",
        "special_function": [
            "array-functions/ARRAY",
            "conversion-functions/CAST",
            "json-functions/JSON_EXTRACT_ARRAY",
            "json-functions/JSON_EXTRACT_SCALAR",
            "mathematical-functions/IEEE_DIVIDE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq038",
        "db": "bigquery-public-data.new_york",
        "question": "Can you list the top 10 stations with the highest proportion of group rides? The group rides are trips that start and end at the same station within a 2-minute window.",
        "external_knowledge": null,
        "plan": "1. Calculate the number of single and group trips for each end station\n2. For each station, compute the proportion of group trips to the total number of trips\n3. Rank the stations by the proportion of group trips and select the top 10.\n4. Join the top stations with trip data to get distinct station names for the top-ranked stations by group trip proportion",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "mathematical-functions/ROUND",
            "timestamp-functions/UNIX_SECONDS"
        ]
    },
    {
        "instance_id": "bq040",
        "db": "bigquery-public-data.new_york_taxi_trips",
        "question": "For NYC yellow taxi trips between January 1-7, 2016, excluding pickups from 'EWR' and 'Staten Island', calculate the proportion of trips by tip category for each pickup borough. Show the borough, tip category, and proportion, ensuring trips where the dropoff occurs after the pickup, the passenger count is greater than 0, and trip distance, tip, tolls, MTA tax, fare, and total amount are non-negative.",
        "external_knowledge": "taxi_tip_rate.md",
        "plan": "1.Select trips from 2016, calculate trip duration, speed, and tip rate, and extract date-time features.\n2.Attach geographic details to each trip by joining with taxi zone data based on coordinates.\n3.Classify trips into tipping categories and count the number of trips per category in each borough.\n4.Use ROLLUP to aggregate trip data by borough and tipping category, calculating the total and percentage contributions.\n5.Determine the weighted average tip rate for each borough by assigning numerical values to tipping categories.\n6.Select boroughs and their average tip rates, ordering the results to show areas with the highest average tips.",
        "special_function": [
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq040_1",
        "db": "bigquery-public-data.new_york_taxi_trips",
        "question": "For NYC yellow taxi trips between January 1-7, 2016, could you tell me the percentage of no tips in each borough. Ensure trips where the dropoff occurs after the pickup, the passenger count is greater than 0, and trip distance, tip, tolls, MTA tax, fare, and total amount are non-negative.",
        "external_knowledge": "taxi_tip_rate.md",
        "plan": "1. Filter taxi trip records from January 2016, ensuring data quality by validating coordinates and calculating trip durations and amounts.\n2. Merge trip data with borough details by matching each trip's pickup location coordinates to taxi zone geometries.\n3. Calculate the tip rate for each trip as a percentage of the total amount and classify trips into tip categories.\n4.Count the number of trips in each tip category per borough.\n5. For each borough, calculate the percentage of trips with no tips relative to the total trips in that borough.",
        "special_function": [
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq039",
        "db": "bigquery-public-data.new_york_taxi_trips",
        "question": "Which are the top 10 taxi trips in New York City from July 1 to July 7, 2016, with more than 5 passengers, a trip distance of at least 10 miles, and a positive fare, ranked by total fare amount? Display the pickup and dropoff zones, trip duration, driving speed in miles per hour, and tip rate.",
        "external_knowledge": null,
        "plan": "Retrieve detailed data for New York taxi trips between January 1 and January 2, 2016. Calculate trip duration in seconds, driving speed in miles per hour, and the rate of tips given. Include time-related attributes such as year, month, day, and hour for both pickups and dropoffs. \nAugment each record with geographic information like zone IDs, zone names, and boroughs for both pickup and dropoff locations. \nEnsure data accuracy by filtering for valid geographic coordinates and positive financial values for fares, tips, and other charges.",
        "special_function": [
            "mathematical-functions/ROUND",
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq054",
        "db": "bigquery-public-data.new_york",
        "question": "Can you provide the top 10 tree species in New York, based on the growth in their population from 1995 to 2015, including the counts of trees, the counts of alive and dead trees for both years, and the respective growth values?",
        "external_knowledge": null,
        "plan": "1. **Subquery A: Tree Census 2015 Data Aggregation**\n   - **Select and Transform Data**: \n     - Convert `spc_latin` to uppercase (`UPPER(spc_latin) AS upper_latin`).\n     - Select species common name (`spc_common`).\n   - **Aggregate Data**:\n     - Count total trees (`COUNT(*) AS count_2015`).\n     - Count trees with status \"Alive\" (`COUNTIF(status=\"Alive\") AS alive_2015`).\n     - Count trees with status \"Dead\" (`COUNTIF(status=\"Dead\") AS dead_2015`).\n   - **Filter and Group**:\n     - Exclude rows where `spc_latin` is an empty string.\n     - Group by `spc_latin` and `spc_common`.\n\n2. **Subquery B: Tree Census 1995 Data Aggregation**\n   - **Select and Transform Data**:\n     - Convert `spc_latin` to uppercase (`UPPER(spc_latin) AS upper_latin`).\n   - **Aggregate Data**:\n     - Count total trees (`COUNT(*) AS count_1995`).\n     - Count trees with status not equal to \"Dead\" (`COUNTIF(status!=\"Dead\") AS alive_1995`).\n     - Count trees with status \"Dead\" (`COUNTIF(status=\"Dead\") AS dead_1995`).\n   - **Group**:\n     - Group by `spc_latin`.\n\n3. **Full Outer Join on Subqueries A and B**\n   - Join `a` and `b` on the `upper_latin` column to ensure all records from both subqueries are included, even if there's no match in the other subquery.\n\n4. **Select and Calculate Final Fields**\n   - **Select `upper_latin`**:\n     - Use `IFNULL` to handle cases where the species name might be missing in either subquery.\n   - **Calculate Counts**:\n     - Use `IFNULL` to handle missing counts and default them to 0.\n     - Select and alias `count_2015` and `count_1995`.\n   - **Calculate Growth Metrics**:\n     - Calculate `count_growth` as the difference between `count_2015` and `count_1995`.\n     - Calculate `alive_growth` as the difference between `alive_2015` and `alive_1995`.\n     - Calculate `dead_growth` as the difference between `dead_2015` and `dead_1995`.\n\n5. **Order and Limit the Result**\n   - Order the results by `count_growth` in descending order.\n   - Limit the result to the top 10 records.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "string-functions/UPPER",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq186",
        "db": "bigquery-public-data.san_francisco",
        "question": "Please help me calculate the first, last, highest, and lowest bike trip durations in minutes for each month.",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Dataset**:\n    - Generate a temporary dataset where each record is associated with a date string representing the year and month of the start date of the trip.\n    - This dataset includes all original columns plus the newly created date string.\n\n2. **Calculate Trip Durations**:\n    - From the temporary dataset, for each unique month, calculate:\n        - The first trip duration.\n        - The last trip duration.\n        - The highest trip duration.\n        - The lowest trip duration.\n    - Convert these durations from seconds to minutes.\n\n3. **Retrieve and Format Results**:\n    - Select the date string (representing the month) and the calculated trip durations (first, last, highest, and lowest) from the results of the previous step.\n    - Ensure the results are ordered by the date string to maintain chronological order.\n\n4. **Final Output**:\n    - Output the date string and the calculated trip durations (in minutes) for each month, ensuring that the data is grouped and ordered by the date string.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "navigation-functions/FIRST_VALUE",
            "navigation-functions/LAST_VALUE",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq339",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "Which month in 2017 had the largest absolute difference between cumulative bike usage minutes for customers and subscribers?",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery for Monthly Totals**:\n   - Calculate the total usage minutes for each user type (e.g., type A and type B) for each month in the specified year.\n   - Group the results by month to get monthly totals for each user type.\n\n2. **Calculate Cumulative Totals**:\n   - Using the monthly totals, compute the cumulative sum of usage minutes for each user type up to each month.\n   - This involves summing the monthly totals in an ordered manner, from the beginning of the year to the end of each month.\n\n3. **Calculate Absolute Differences**:\n   - Determine the absolute difference between the cumulative totals of the two user types for each month.\n   - This provides a measure of how the cumulative usage minutes differ between the two user types at the end of each month.\n\n4. **Identify the Month with the Largest Difference**:\n   - Sort the months based on the absolute difference calculated in the previous step, in descending order.\n   - Select the month with the highest absolute difference.\n\nBy following these steps, the query identifies the month in the specified year that had the largest absolute difference in cumulative bike usage minutes between the two user types.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ABS",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq059",
        "db": "bigquery-public-data.san_francisco_bikeshare\nbigquery-public-data.san_francisco_neighborhoods\nbigquery-public-data.san_francisco_311\nbigquery-public-data.san_francisco_film_locations\nbigquery-public-data.san_francisco_sffd_service_calls\nbigquery-public-data.san_francisco_sfpd_incidents\nbigquery-public-data.san_francisco_transit_muni\nbigquery-public-data.san_francisco_trees",
        "question": "What is the highest average speed (rounded to 1 decimal, in metric m/s) for bike trips in Berkeley with trip distance greater than 1000 meters?",
        "external_knowledge": null,
        "plan": "1. Identify Relevant Locations: Select station identifiers for bike stations located in a specific region (Berkeley) by filtering based on the region's name.\n\n2. Calculate Distances and Speeds: For trips starting and ending at the identified stations, calculate the trip distance and the average speed. Ensure the coordinates for start and end stations are available.\n\n3. Filter Trips: Only include trips that meet the distance requirement (greater than 1000 meters) and start/end station restriction (within the region Berkeley).\n\n4. Find Maximum Speed: From the filtered trips, determine the highest average speed, rounded to one decimal place, and return this value.",
        "special_function": [
            "conversion-functions/CAST",
            "geography-functions/ST_DISTANCE",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq006",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.",
        "external_knowledge": null,
        "plan": "Analyze daily occurrences of public intoxication incidents in Austin for the year 2016. \nCalculate the total number of incidents per day and compute the Z-score for each day's incidents to identify days with significantly higher or lower incident counts. \nThe output should include the date, total number of incidents, and the Z-score.",
        "special_function": [
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/STDDEV"
        ]
    },
    {
        "instance_id": "bq187",
        "db": "bigquery-public-data.ethereum_blockchain",
        "question": "What is the total circulating supply balances of the 'BNB' token for all addresses (excluding the zero address), based on the amount they have received (converted by dividing by 10^18) minus the amount they have sent?",
        "external_knowledge": null,
        "plan": "1. **Identify the Token:** Select the address of the specified token from the token details table.\n   \n2. **Calculate Received Amounts:**\n   - Filter transactions where the token matches the specified token.\n   - Exclude transactions to the zero address.\n   - Sum the received amounts for each address, converting the values to a standard unit.\n\n3. **Calculate Sent Amounts:**\n   - Filter transactions where the token matches the specified token.\n   - Exclude transactions from the zero address.\n   - Sum the sent amounts for each address, converting the values to a standard unit.\n\n4. **Compute Balances:**\n   - Combine the received and sent amounts for each address.\n   - Subtract the total sent amounts from the total received amounts to determine the balance for each address.\n\n5. **Aggregate Total Circulating Supply:**\n   - Sum the balances of all addresses to compute the total circulating supply of the token.",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/POWER",
            "conditional-functions/COALESCE"
        ]
    },
    {
        "instance_id": "bq014",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Get the first order of each user, excluding the cancelled and returned orders. (from 'orders' table)\n2. Calculate the total revenue and distinct user count for each product category. (combine the 'orders' 'order_items' and 'products' tables)\n3. Select the top category with the most user count.\n4. Based on the top category, get its revenue.",
        "special_function": [
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq188",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average time in minutes that users spend per visit on the product category with the highest total quantity purchased?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Create an Initial Dataset:**\n   - Extract event details including timestamps and session information.\n   - Identify events related to a specific type by transforming a specific URI component into an identifier.\n   - Calculate the timestamp for the next event in the same session.\n\n2. **Aggregate Purchase Data:**\n   - Determine the total number of times products in various categories have been purchased.\n   - Group this data by category.\n\n3. **Calculate Visit Statistics:**\n   - Compute visit details for each category by joining the initial dataset with product data.\n   - Count the number of visits for each category.\n   - Calculate the average time spent per visit by determining the time difference between consecutive events and converting it to minutes.\n   - Include the total quantity of purchases for each category from the aggregated purchase data.\n   - Filter out irrelevant events to ensure only meaningful data is included.\n\n4. **Determine the Desired Metric:**\n   - Select the average time spent per visit for the category with the highest total quantity purchased.\n   - Sort the results based on the total quantity purchased in descending order.\n   - Limit the results to get the top category.\n\n5. **Output the Result:**\n   - Convert the average time spent per visit to a string format for the final output.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE_DIFF",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "navigation-functions/LEAD",
            "string-functions/REPLACE",
            "timestamp-functions/STRING",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq258",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide a report that shows, for each product category by month and year prior to 2023, the percentage growth in revenue compared to the previous month, the percentage growth in orders compared to the previous month, the total cost of products sold, the total profit earned, and the profit to cost ratio? ",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Figure out metrics like Total Purchase Value (TPV) and Total Number of Orders (TPO) per month and product category.\n2. Get lagged values of TPV and TPO for each product category and compare with the current month's values.\n3. Calculates targeted metrics through the lagged values and latest values of TPV and TPO.\n",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "conditional-functions/NULLIF"
        ]
    },
    {
        "instance_id": "bq259",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide the percentage of users who made a purchase in the first, second, and third months after their initial purchase, organized by the month of their first purchase, using data up until the end of 2023?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Determine the cohort date and index based on the time difference between the creation date and the user's first purchase date.\n2. Aggregate the total number of unique users for each cohort date and purchase index.\n3. Summarize the total number of users at each purchase index (0, 1, 2, 3) for every cohort date.\n4. Calculate the percentage distribution of users at each purchase index level relative to the initial cohort size for each cohort date.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "date-functions/FORMAT_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq189",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the average monthly revenue growth rate for the product category with the highest average monthly order growth rate based on completed orders?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate the total sale prices and total order amount (TPV) for each product category (TPO) every month.\n2. Get the TPV and TPO in the previous month as the 'Lagged_TPV' and 'Lagged_TPO'.\n3. Calculate the revenue growth rate and the order growth rate for each product monthly.\n4. Find out the product category with the max average order growth rate.\n5. Calculate the average revenue growth rate for that product category.",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "conditional-functions/NULLIF"
        ]
    },
    {
        "instance_id": "bq260",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Find the total number of youngest and oldest users separately for each gender in the e-commerce platform created from January 1, 2019, to April 30, 2022.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Filter user data within the specified date range.\n2. Determine the youngest and oldest ages for each gender group.\n3. Identify users who are the youngest and oldest within their respective gender groups based on the age comparison.\n4. Count the number of users classified as the youngest and oldest within their gender groups.\n",
        "special_function": null
    },
    {
        "instance_id": "bq261",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the total cost and profit of products whose profit rank first per month sorted chronologically? Only consider data before 2024.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate sales information for products, including the cost and profit.\n2. Assign a ranking to products within each month based on profit.\n3. Select and return the cost and profit figures for the top-ranking product for each month, sorting the results chronologically by month-year.\n",
        "special_function": [
            "numbering-functions/RANK",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq262",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Help me generate a monthly analysis report on e-commerce sales in the second half of 2019, which should contain the total sum of order count/revenue/profit as well as their growth rates for each product category monthly. Please sort the results by months (e.g., 2019-07) and product categories in ascending order.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Filter and Aggregate Orders Data:\n   - Objective: Gather monthly order data for each product.\n   - Action: Create a CTE `orders_data` to:\n     - Extract the year-month from `created_at` timestamp in the `orders` table.\n     - Count the total orders and sum the sales price for each product per month.\n     - Filter orders within the months from 2019-06 to 2019-12.\n\n2. Fetch Product Category and Cost:\n   - Objective: Associate each product with its category and cost.\n   - Action: Create a CTE `product_data` to:\n     - Select product ID, category, and cost from the `products` table.\n\n3. Calculate Monthly Metrics:\n   - Objective: Compute monthly order count, total revenue, and total profit for each product category.\n   - Action: Create a CTE `monthly_metrics` to:\n     - Join `orders_data` with `product_data` on product ID.\n     - Aggregate data by month and product category.\n     - Calculate total orders, total revenue, and total profit (revenue minus cost of goods sold).\n\n4. Calculate Growth Rates:\n   - Objective: Determine month-over-month growth rates for order count, revenue, and profit.\n   - Action: Create a CTE `growth_metrics` to:\n     - Use window functions (`LAG`) to calculate the previous month's values for each metric within the same product category.\n     - Compute growth rates for orders, revenue, and profit as percentages.\n\n5. Generate Final Report:\n   - Objective: Present the monthly analysis report with growth rates.\n   - Action: Select all columns from `growth_metrics` except the month `2019-06`.\n     - Order the result by month and product category for organized viewing.",
        "special_function": [
            "navigation-functions/LAG",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq190",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What is the count of the youngest and oldest users respectively, broken down by gender from January 2019 to April 2022?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Determine Age Ranges**:\n   - Create two subsets to find the minimum and maximum ages for each gender within the specified date range.\n\n2. **Identify Users with Extreme Ages**:\n   - For each gender, select users whose ages match the previously identified minimum or maximum ages within the specified date range.\n\n3. **Combine Users**:\n   - Combine the users from both genders into a single set, ensuring that only users with the extreme ages (youngest and oldest) are included.\n\n4. **Tag Users**:\n   - Add a tag to each user indicating whether they are in the 'Youngest' or 'Oldest' age group based on their age.\n\n5. **Count Users by Group**:\n   - Group the tagged users by gender and tag, then count the number of users in each group.\n\n6. **Output the Results**:\n   - Select and order the results by gender and tag to present a clear summary of the counts for each group.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq263",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide a report showing the total purchase value, total cost, total number of orders, total profit, and profit-to-cost ratio for product category \u2018Sleep & Lounge\u2019 each month in 2023? Make sure to only include completed orders.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Consolidate data from the order items, orders, and products tables\n2. Computes key sales metrics including Total Purchase Value (TPV), total cost, Total Number of Orders (TPO), total profit, and Profit-to-Cost Ratio per month.\n3. Sort by month and return the result.\n",
        "special_function": [
            "date-functions/FORMAT_DATE",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq264",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Select the youngest users in terms of age for each gender within the specified date range.\n2. Identify the oldest users for each gender.\n3. Calculate the difference in the count of the oldest and youngest users.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq197",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "What are the top-selling products by sales volume and revenue for June 2024 and each month before, considering only completed orders?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Data Aggregation for Each Month**:\n    - **Extract and Format Data**: For each record, extract the product details and the order details, including the timestamp of the order, and format the timestamp to year and month.\n    - **Filter Data**: Consider only the orders that are marked as completed and occurred before July 2024.\n    - **Aggregate Data**: Group the data by product details and month to calculate the total number of sales and the total revenue for each product within each month.\n\n2. **Filter Non-relevant Data**:\n    - **Ensure Valid Data**: Ensure that the month, total revenue, and product brand are not null to maintain the integrity of the data.\n\n3. **Identify Top-Selling Products**:\n    - **Rank Products**: For each month, assign a rank to each product based on the total number of sales, with ties broken by total revenue.\n    - **Select Top Product**: Select the product with the highest rank (i.e., the top-selling product) for each month.\n\n4. **Final Output**:\n    - **Organize and Format**: Retrieve the month, product details, total sales, and total revenue for the top-selling product of each month.\n    - **Order Results**: Order the final results in ascending order of the month to provide a chronological view of the top-selling products over the specified period.",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq265",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide me with the emails of the top 10 users who have the highest average order value, considering only those users who registered in 2019 and made purchases within the same year?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Get the date level information.\n2. Get all the order level data including the sales value and number of orders.\n3. Combine date level data with the previously captured orders and sales level data.\n4. Calculate the Lifetime value (LTV) of the customer and number of orders placed by them.\n5. Figure out the average order value per user by dividing the LTV by the number of orders placed by that user.\n6. Sort to find the top 10 average order value users.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq266",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Can you provide me with the names of the products that had the lowest profit margin each month throughout the year 2020, excluding any months where this data isn't available? Please list them in chronological order based on the month.",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Calculate profit as the difference between retail price and cost.\n2. Assign a rank to each product within a month based on profit margin and partition the ranking by year and month and order products by profit in ascending order within each month.\n3. Retrieve the names of products that have the rank of 1 and return.\n",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "numbering-functions/DENSE_RANK",
            "numbering-functions/RANK",
            "string-functions/CONCAT",
            "string-functions/LPAD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq271",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Could you provide me with the country that had the highest profit from orders in 2021 and its profit, considering the difference between the total retail prices and the costs of products sold during the year?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "Extract the inventory item ID and sale price for each order.\n1. Augment the data with the country associated with each user who made a purchase.\n2. Calculate the total product retail price, and total cost.\n3. Figure out the profit through subtracting the total product retail price from the total cost.\n4. Sort the results by the profit.\n5. Return the country name which has the highest monthly profit.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_TRUNC"
        ]
    },
    {
        "instance_id": "bq272",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Please provide me with the names of the top three most profitable products for each month between January 2019 and September 2022, excluding any products that were either canceled or returned.",
        "external_knowledge": null,
        "plan": "1. Calculate monthly profit for each product by truncating the creation date to the month level.\n2. Only include data between specific dates and exclude records where the status indicates the item was either canceled or returned.\n3. For each month, rank products based on their profit in descending order. This ranking is calculated within each month's dataset.\n4. From the ranked list, select only the top 5 products by profit for each month.\n5. Count the total number of records that have been identified as top performers across all months in the filtered dataset.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_TRUNC",
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq273",
        "db": "bigquery-public-data.thelook_ecommerce",
        "question": "Could you provide me with the top 5 months where there was the highest increase in profit compared to the previous month for completed orders made by users who came from Facebook, between January 1, 2022, and June 30, 2023?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. Convert delivery dates to the month granularity.\n2. Retrieve order-related details, product cost information, user information.\n3. Apply filters to keep only records where the order status is 'Complete', the user traffic source is \u2018Facebook\u2019 and the order created time in the specific period.\n4. Compute total revenue, total profit, and count unique products, orders, and users for each month.\n5. For both revenue and profit, compute a 3-month moving average.\n6. Calculate the difference between the current month's revenue and profit against their respective moving averages.\n7. Calculate month-over-month changes in revenue and profit by comparing with the previous month's figures.\n8. Select the delivery month with the highest increase in profit compared to the prior month.\n",
        "special_function": [
            "date-functions/DATE_TRUNC",
            "navigation-functions/LAG"
        ]
    },
    {
        "instance_id": "bq020_1",
        "db": "bigquery-public-data.genomics_cannabis",
        "question": "What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Tables**:\n   - Utilize two tables: one containing variant data and another containing reference sequence data.\n\n2. **Join Tables**:\n   - Perform an inner join on the tables based on matching reference sequence names to combine variant information with reference sequence details.\n\n3. **Filter Variants**:\n   - Include only those variants where there is at least one genotype value greater than zero within the variant calls. This ensures that only relevant variants are considered.\n\n4. **Calculate Variant Density**:\n   - For each reference sequence, calculate the density of variants by dividing the count of variants by the length of the reference sequence. Also, compute the total count of variants and store the reference sequence length for later use.\n\n5. **Group and Aggregate Data**:\n   - Group the results by reference sequence name and reference sequence length to compute the variant counts and densities for each reference sequence.\n\n6. **Order by Density**:\n   - Sort the grouped results in descending order of variant density to prioritize sequences with the highest density of variants.\n\n7. **Select Top Result**:\n   - Limit the results to a single entry to identify the reference sequence with the highest variant density.\n\n8. **Output**:\n   - Extract and display the name of the reference sequence with the highest variant density.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq020_2",
        "db": "bigquery-public-data.genomics_cannabis",
        "question": "What is the variant density of the cannabis reference with the longest reference length?",
        "external_knowledge": null,
        "plan": "1. **Data Preparation (CTE Setup)**:\n   - Create a common table expression (CTE) to compute necessary metrics for each reference.\n   \n2. **Join Data**:\n   - Join two datasets: one containing variant data and another containing reference length information based on matching reference names.\n\n3. **Filter Data**:\n   - Apply a filter to ensure only those variants are considered where at least one genotype call is greater than zero.\n\n4. **Aggregation**:\n   - Group the data by reference name and reference length.\n   - Calculate the total count of variants for each reference.\n   - Compute the variant density by dividing the count of variants by the reference length.\n\n5. **Final Selection**:\n   - Select all data from the aggregated CTE.\n\n6. **Sorting and Limiting**:\n   - Order the results by reference length in descending order.\n   - In case of ties in reference length, order by reference name.\n   - Limit the result to one row to get the reference with the longest length and its variant density.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq025",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.",
        "external_knowledge": null,
        "plan": "1. **Define the Main Query Objective**:\r\n   - Retrieve and analyze the population data for countries, specifically focusing on the population under the age of 20 and its percentage relative to the total population.\r\n\r\n2. **Subquery 1 (Alias `age`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population_agespecific`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n     - `age < 20`: Only include population data for individuals under the age of 20.\r\n   - **Selected Columns**:\r\n     - `country_name`: Name of the country.\r\n     - `population`: Population of the specific age group.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n3. **Subquery 2 (Alias `pop`)**:\r\n   - **Data Source**: `bigquery-public-data.census_bureau_international.midyear_population`\r\n   - **Filters**:\r\n     - `year = 2020`: Only consider data from the year 2020.\r\n   - **Selected Columns**:\r\n     - `midyear_population`: Total midyear population for the country.\r\n     - `country_code`: Country code for joining purposes.\r\n\r\n4. **Join Operation**:\r\n   - **Type**: INNER JOIN\r\n   - **Condition**: `age.country_code = pop.country_code`\r\n   - **Purpose**: Combine age-specific population data with total population data for each country based on matching country codes.\r\n\r\n5. **Aggregation and Calculations**:\r\n   - **Grouped By**: \r\n     - `age.country_name` (Column index 1 in the SELECT clause)\r\n     - `pop.midyear_population` (Column index 3 in the SELECT clause)\r\n   - **Aggregations**:\r\n     - `SUM(age.population) AS under_25`: Calculate the total population under the age of 20 for each country.\r\n     - `ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25`: Calculate and round to two decimal places the percentage of the population under 20 relative to the total midyear population for each country.\r\n\r\n6. **Ordering**:\r\n   - **Order By**: `pct_under_25 DESC` (Column index 4 in the SELECT clause)\r\n   - **Purpose**: Sort the results in descending order based on the percentage of the population under 20.\r\n\r\n7. **Limit**:\r\n   - **Limit**: `10`\r\n   - **Purpose**: Restrict the result set to the top 10 countries based on the sorted percentage of the population under 20.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq025_1",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which country has the highest percentage of population under the age of 25 in 2017?",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Year and Age**: Begin by selecting records from the dataset that match the specific year (2017) and where the age is less than 25. This filters the population data to only include those under 25 years old.\n\n2. **Aggregate Population Under 25**: Sum the population values for the filtered records to get the total population under 25 for each country.\n\n3. **Fetch Total Population**: Select the total midyear population for each country for the same year (2017) from another dataset.\n\n4. **Combine Datasets**: Perform an inner join between the two datasets on the country code to combine the total population under 25 with the total midyear population for each country.\n\n5. **Calculate Percentage**: Compute the percentage of the population under 25 by dividing the summed population under 25 by the total midyear population, then multiply by 100 and round to two decimal places.\n\n6. **Group and Order**: Group the results by country and total population, then order the results by the calculated percentage of the population under 25 in descending order.\n\n7. **Select Top Country**: Limit the result to the top entry, which represents the country with the highest percentage of the population under 25.\n\n8. **Return Country Name**: Finally, select and return the name of the country with the highest percentage of the population under 25 in 2017.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq338",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.geo_census_tracts",
        "question": "Which Brooklyn census tract (by geo ID) had the largest increase in median income from 2010 to 2017?",
        "external_knowledge": null,
        "plan": "What is the difference in median income in Brooklyn by Census tract from 2010 to 2017?\nFind how the median household changed for each Census tract in Brooklyn between 2010 and 2017. See the visualization below for more details",
        "special_function": null
    },
    {
        "instance_id": "bq086",
        "db": "bigquery-public-data.covid19_open_data\nbigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "What percentage of each country\u2019s population was confirmed to have COVID-19 as of June 30, 2020?",
        "external_knowledge": null,
        "plan": "1. Retrieve 2018 population data for each country from the World Bank global population dataset.\n2. Select COVID-19 case data as of June 30, 2020\n3. Compute the percentage of the population with confirmed cases",
        "special_function": [
            "date-functions/DATE",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq089",
        "db": "bigquery-public-data.covid19_vaccination_access\nbigquery-public-data.census_bureau_acs",
        "question": "What is the number of vaccine sites per 1000 people for counties in California?",
        "external_knowledge": null,
        "plan": "1. Collect the number of vaccine sites for each county in California and calculate the population of each county.\n2. Compute the number of vaccine sites per 1000 people for each county.",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/LEFT",
            "string-functions/STARTS_WITH"
        ]
    },
    {
        "instance_id": "bq088",
        "db": "bigquery-public-data.covid19_symptom_search",
        "question": "Can you provide the average levels of anxiety and depression symptoms in the United States for the years 2019 and 2020, and calculate the percentage increase in these symptoms from 2019 to 2020?",
        "external_knowledge": null,
        "plan": "1. **Data Source Selection**:\r\n   - The query utilizes the `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly` dataset from BigQuery's public data.\r\n\r\n2. **Subquery for 2020 Data**:\r\n   - A subquery (`table_2020`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2020.\r\n   - The `WHERE` clause filters the records to include only those from the US (`country_region_code = \"US\"`) and within the date range from January 1, 2020, to December 31, 2020.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n3. **Subquery for 2019 Data**:\r\n   - Another subquery (`table_2019`) is executed to calculate the average levels of anxiety and depression symptoms for the year 2019.\r\n   - Similar to the 2020 subquery, the `WHERE` clause filters the records to include only those from the US and within the date range from January 1, 2019, to December 31, 2019.\r\n   - The `AVG` function calculates the average for `symptom_Anxiety` and `symptom_Depression`, casting these values to `FLOAT64` for precision.\r\n\r\n4. **Main Query**:\r\n   - The main query selects the average values calculated in the subqueries:\r\n     - `table_2019.avg_symptom_Anxiety_2019`\r\n     - `table_2020.avg_symptom_Anxiety_2020`\r\n     - `table_2019.avg_symptom_Depression_2019`\r\n     - `table_2020.avg_symptom_Depression_2020`\r\n   - Additionally, it calculates the percentage increase in anxiety and depression symptoms from 2019 to 2020 using the formula:\r\n     - `((table_2020.avg_symptom_Anxiety_2020 - table_2019.avg_symptom_Anxiety_2019)/table_2019.avg_symptom_Anxiety_2019) * 100` as `percent_increase_anxiety`\r\n     - `((table_2020.avg_symptom_Depression_2020 - table_2019.avg_symptom_Depression_2019)/table_2019.avg_symptom_Depression_2019) * 100` as `percent_increase_depression`\r\n\r\n5. **Output**:\r\n   - The final output includes the average levels of anxiety and depression for both 2019 and 2020, along with the calculated percentage increases for both symptoms between the two years.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq137",
        "db": "bigquery-public-data.census_bureau_usa\nbigquery-public-data.utility_us",
        "question": "Find details about zip code areas within 10 kilometers of the coordinates (-122.3321, 47.6062), including their geographic polygons, land and water area in meters, latitude and longitude points, state code, state name, city, county, and population from the 2010 census data.",
        "external_knowledge": null,
        "plan": "1. **Common Table Expression (CTE) - `zip_pop` Creation**:\r\n   - Query `bigquery-public-data.census_bureau_usa.population_by_zip_2010` table.\r\n   - Select `zipcode` and `population` columns.\r\n   - Filter rows where `gender` is either 'male' or 'female'.\r\n   - Ensure `minimum_age` and `maximum_age` are `NULL` (considering the entire population without age restrictions).\r\n\r\n2. **Main Query - Data Selection and Joining**:\r\n   - Query `bigquery-public-data.utility_us.zipcode_area` table.\r\n   - Select the following columns:\r\n     - `zipcode_geom` as `zipcode_polygon` (geometry of the zip code area),\r\n     - `zipcode` (zip code),\r\n     - `area_land_meters` (land area in square meters),\r\n     - `area_water_meters` (water area in square meters),\r\n     - `ST_GeogPoint(longitude, latitude)` as `lat_lon` (geographical point of the zip code),\r\n     - `state_code` (state abbreviation),\r\n     - `state_name` (full state name),\r\n     - `city` (city name),\r\n     - `county` (county name),\r\n     - `population` (population data from the CTE).\r\n\r\n3. **Join Operation**:\r\n   - Perform an inner join between `zip_area` and `zip_pop` on the `zipcode` column to combine area and population data.\r\n\r\n4. **Distance Filtering**:\r\n   - Filter results where the geographical point of the zip code (`ST_GeogPoint(longitude, latitude)`) is within 10,000 meters (10 kilometers) of the point (-122.3321, 47.6062) using the `ST_DWITHIN` function.\r\n\r\n5. **Final Output**:\r\n   - The final result includes data about zip code areas, such as their geographical polygons, land and water areas, geographical points, and associated population information, filtered to only include zip codes within 10 kilometers of the specified coordinates in Seattle, WA.",
        "special_function": [
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT"
        ]
    },
    {
        "instance_id": "bq023",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.fec\nbigquery-public-data.hud_zipcode_crosswalk\nbigquery-public-data.geo_census_tracts",
        "question": "What are the average political donation amounts and median incomes for each census tract identifier in Kings County (Brooklyn), NY, using data from the 2018 ACS and 2020 FEC contributions?",
        "external_knowledge": null,
        "plan": "1. Extract Median Income Data:\n    - Retrieve the median income for each geographical area from the census data.\n\n2. Filter Political Donations by State:\n    - Select donation amounts and corresponding ZIP codes for a specific state from political contribution records.\n\n3. Map ZIP Codes to Census Tracts:\n    - Create a mapping between ZIP codes and census tracts using a crosswalk table.\n\n4. Calculate Average Donations per Census Tract:\n    - Aggregate the donation amounts by census tract, using the mapping from ZIP codes to census tracts, to compute the average donation per tract.\n\n5. Identify Census Tracts in Target County:\n    - Select the census tracts that belong to the specific county of interest, filtering by county and state codes.\n\n6. Combine Data:\n    - Merge the census tract information with the computed average donations and the median income data, ensuring that all tracts in the target county are included, even if there are no corresponding donations or income records.\n\n7. Format and Order Results:\n    - Select the relevant fields for the final output, specifically the census tract identifier, average donation amount, and median income. Sort the results by the census tract identifier for orderly presentation.",
        "special_function": null
    },
    {
        "instance_id": "bq060",
        "db": "bigquery-public-data.census_bureau_international",
        "question": "Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?",
        "external_knowledge": null,
        "plan": "1. **Filter and Select Data for 2017**: First, select the relevant columns (country name, net migration, and country code) from the dataset containing information about growth rates for the year 2017.\n\n2. **Filter and Join on Country Area**: Next, filter another dataset to include only countries with an area greater than 500 square kilometers. Join this filtered dataset with the previous one on the country code to combine the net migration data with the area data.\n\n3. **Sort and Retrieve Top Country**: Sort the combined data by net migration in descending order and limit the result to the top 3 entry. Finally, select the country name and net migration fields from this result to identify the country with the highest net migration.",
        "special_function": [
            "conversion-functions/CAST"
        ]
    },
    {
        "instance_id": "bq061",
        "db": "bigquery-public-data.census_bureau_acs\nbigquery-public-data.geo_census_tracts",
        "question": "Which census tract has witnessed the largest increase in median income between 2015 and 2018 in California? Tell me the tract code.",
        "external_knowledge": null,
        "plan": "1. **Extract Data for 2018**: Create a temporary dataset containing the geographical IDs and median incomes for the year 2018.\n\n2. **Extract Data for 2015**: Create another temporary dataset containing the geographical IDs and median incomes for the year 2015.\n\n3. **Calculate Income Difference**: Join the 2018 and 2015 datasets based on geographical IDs and calculate the difference in median income between these two years for each geographical ID.\n\n4. **Identify Maximum Increase**: From the dataset of income differences, filter out records that are not relevant to California and then identify the geographical ID with the highest increase in median income.\n\n5. **Retrieve Tract Code**: Join the geographical ID with the highest income increase back to the dataset containing California tract codes to retrieve the specific tract code associated with that geographical ID.",
        "special_function": null
    },
    {
        "instance_id": "bq064",
        "db": "bigquery-public-data.geo_census_tracts\nbigquery-public-data.geo_us_boundaries\nbigquery-public-data.census_bureau_acs",
        "question": "Could you calculate the population and average individual income (both rounded to 1 decimal) for each zip code based on U.S. census tract data in 2017? Only include those zip codes within a 5-mile radius of a specific geographic point (47.685833\u00b0N, -122.191667\u00b0W) in Washington and sort the results according to income in descending order.",
        "external_knowledge": null,
        "plan": "1. Find intersections between zip codes and census tracts, calculating the percentage of each tract within a zip code.\n2. Convert census tract data from averages to totals (e.g., total income).\n3. Join these totals with the intersection data to compute the portion of each tract's population and income that belongs to each zip code.\n4. Aggregate these values by zip code to calculate total population and income, then convert back to averages.\n5. Filter zip codes within a 5-mile radius of a specific geographic point in Washington and retrieve their statistics.",
        "special_function": [
            "geography-functions/ST_AREA",
            "geography-functions/ST_DWITHIN",
            "geography-functions/ST_GEOGPOINT",
            "geography-functions/ST_INTERSECTION",
            "geography-functions/ST_INTERSECTS",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq198",
        "db": "bigquery-public-data.ncaa_basketball",
        "question": "What are the top 5 most successful college basketball teams over the seasons from 1900 to 2000, based on the number of times they had the maximum wins in a season?",
        "external_knowledge": null,
        "plan": "1. **Identify Maximum Wins per Season:**\n   - Select each season within the specified range (1900 to 2000).\n   - For each season, determine the maximum number of wins achieved by any team.\n\n2. **Identify Teams with Maximum Wins:**\n   - For each season, find the teams that achieved the maximum number of wins identified in the previous step.\n   - Ensure the team names are not null.\n\n3. **Create a Distinct List:**\n   - Create a distinct list of seasons and corresponding teams who had the maximum wins in those seasons.\n\n4. **Count Top Performances:**\n   - Group the distinct list by team names.\n   - Count the number of times each team appears in the list, representing how often they had the maximum wins in a season.\n\n5. **Order and Limit Results:**\n   - Sort the teams by the count of top performances in descending order.\n   - If there is a tie in the count, sort the tied teams alphabetically by their names.\n   - Limit the results to the top 5 teams.\n\nBy following these steps, the query identifies and ranks the top 5 most successful college basketball teams based on the number of seasons they had the maximum wins.",
        "special_function": null
    },
    {
        "instance_id": "bq055",
        "db": "bigquery-public-data.bls\nbigquery-public-data.google_dei",
        "question": "Can you provide the top 3 races with the largest percentage differences between Google's 2023 hiring data and the average percentages from the 2022 BLS reports, along with their respective differences?",
        "external_knowledge": null,
        "plan": "When set side by side, how does Google\u2019s hiring and representation compare to related industries?\nThis query looks at the hiring and representation of employees at Google in the U.S. in 2022 by race and gender compared to the representation of U.S. employed persons in the internet publishing and broadcasting and web search portals industry; the software publishers industry; the data processing, hosting, and related services industry; and the computer systems designs and related services industry as reported by the U.S. Bureau of Labor Statistics",
        "special_function": [
            "mathematical-functions/ABS",
            "search-functions/SEARCH"
        ]
    },
    {
        "instance_id": "bq268",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Identify the longest number of days between the first visit and the last recorded event (either the last visit or the first transaction) for a user where the last recorded event was associated with a mobile device.",
        "external_knowledge": null,
        "plan": "1. Find the first and last visit dates for each unique visitor.\n2. Determine the first transaction date for visitors with transaction information.\n3. Combine visit, transaction, and device information.\n4. Calculate the time duration in days between the event date (either transactions or last visit) and the first visit date for visitors on mobile devices.\n5. Sorted by duration and return the longest time duration.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "time-functions/TIME",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq269",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Compute the average pageviews per visitor for non-purchase events and purchase events each month between June 1st and July 31st in 2017.",
        "external_knowledge": null,
        "plan": "1. Calculate the average pageviews per visitor for sessions without transactions and non-earning product revenue through dividing pageviews by the number of visitors.\n2. Compute the average pageviews per visitor for sessions with at least one transaction and product revenue between June 1 and July 31, 2017.\n3, Join the results and order the final output by month.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq270",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "What were the monthly add-to-cart and purchase conversion rates, calculated as a percentage of pageviews, from January to March 2017?",
        "external_knowledge": "ga360_hits.eCommerceAction.action_type.md",
        "plan": "1. Filter and count the number of product views with eCommerce action type '2' between January 1, 2017, and March 31, 2017.\n2. Filter and count the number of add-to-cart actions in the specified date range.\n3. Filter and count the number of successful purchase actions and non-null product revenue in the designated period.\n4. Calculate the add-to-cart rate through dividing the number of add-to-cart actions by the number of product views. \n5. Analogously, compute the purchase rate.\n6. Sort the final output by month and return.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/PARSE_DATE",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "string-functions/CONCAT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq275",
        "db": "bigquery-public-data.google_analytics_sample",
        "question": "Can you provide a list of visitor IDs for those who made their first transaction on a mobile device on a different day than their first visit?",
        "external_knowledge": null,
        "plan": "Calculate the first and last visit dates for each visitor by grouping on visitor ID and taking the minimum and maximum of visit dates respectively.\n1. Identify distinct combinations of visitor ID, visit date, and device category used during each visit.\n2. For each visitor, identify the earliest date on which a transaction occurred and flag these records as having a transaction.\n3. For each transaction, capture the distinct visitor ID, date of transaction, and device category used during the transaction.\n4. Merge the datasets obtained above.\n5. From the combined data, prepare a table where for each visitor, the event date is determined (use the transaction date if it exists; otherwise, use the last visit date) and the device used is specified (use the transaction device if it exists; otherwise, use the device from the last visit).\n6. Select visitors with at least one transaction whose event happened after the first visit and the device used was mobile.\n",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq016",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the highest release versions of PYPI packages, which one and its version has the most dependent packages?",
        "external_knowledge": null,
        "plan": "1. Firstly, we declare a system variable to denote the PYPI system.\n2. Identify the highest released versions for each package.\n   - Use a subquery to:\n     - Partition data by `Name`.\n     - Order the partitions by `VersionInfo.Ordinal` in descending order to rank versions.\n     - Assign row numbers to each version within their respective partitions.\n   - Filter to include only the first row (highest version) for each package where:\n     - The `System` is 'PYPI' (using the variable `Sys`).\n     - The `VersionInfo.IsRelease` is true (indicating it is a release version).\n3. Join the table Dependencies with the table defined in Step 2.\n4. Aggregate and order dependencies by name and version.\n5. Restrict the output to only one record, which is the dependency with the highest count of dependent packages.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq062",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the most frequently used license by packages in each system?",
        "external_knowledge": null,
        "plan": "1. Generate a Common Table Expression (CTE) that aggregates the number of distinct packages using each license within each system.\n- Use `CROSS JOIN UNNEST` to handle the array of licenses, effectively normalizing the data for easier counting.\n- Group the results by `System` and `License`.\n2. Generate another CTE that ranks the licenses within each system based on the number of distinct packages using them.\n- Use the `ROW_NUMBER()` window function partitioned by `System` and ordered by `NPackages` in descending order to assign a rank to each license within each system.\n3. Select and output only top ranked licenses for each system.",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq063",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "What is the github URL of the latest released package from the NPM system that have the highest number of dependencies? Exlcude those package whose names contain character '@' or the URL label is not 'SOURCE_REPO'.",
        "external_knowledge": null,
        "plan": "1. Initialize a variable `Sys` with the value 'NPM' to filter data related to the NPM system.\n2. Create a Common Table Expression (CTE) to identify the latest released version of each package in the NPM system.\n- Select `Name` and `Version` from the `PackageVersions` table.\n- Use `ROW_NUMBER` to assign a unique sequential integer to rows within a partition of the dataset, partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order.\n- Filter to include only rows where `VersionInfo.IsRelease` is true.\n- Retrieve only the rows where `RowNumber` equals 1, ensuring only the latest released version is selected for each package.\n3. Create another CTE to find packages with the highest number of dependencies.\n- Select `Name`, `Version`, and the count of dependencies from the `Dependencies` table after joining it with the table created in Step 2.\n- Order the results by the count of dependencies (`NDependencies`) in descending order.\n4. Select the `URL` from the `PackageVersions` table, unnesting the `Links` array to access individual URLs.\n- Filter to include only rows where:\n   - `System` equals `Sys`.\n   - `Name` does not contain the character '@'.\n   - `Label` is 'SOURCE_REPO'.\n   - `lnk.URL` contains 'github.com'.\n5. Limit the result to the top 1, which gives the GitHub URL of the package with the highest number of dependencies among the latest releases.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "string-functions/LOWER",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq028",
        "db": "spider2-public-data.deps_dev_v1",
        "question": "Considering only the latest release versions of NPM package, which packages are the top 3 most popular based on the Github star number, as well as their versions?",
        "external_knowledge": null,
        "plan": "1. Declare system variable: define a variable `Sys` to filter records for the NPM system.\n2. Identify latest release versions:\n- Create a common table expression (CTE) to determine the latest release version for each package.\n- Use `ROW_NUMBER()` window function partitioned by `Name` and ordered by `VersionInfo.Ordinal` in descending order to rank the versions.\n- Filter to keep only the top-ranked version (`RowNumber = 1`) which represents the latest release.\n3. Join latest versions with projects to extract the project name and type and constrain the project type to GITHUB.\n4. Select package details and rank by the number of github stars count:\n5. Limit the final result to the top 3 packages based on the number of GitHub stars and return the package names and versions.",
        "special_function": [
            "json-functions/STRING",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq022",
        "db": "bigquery-public-data.chicago_taxi_trips",
        "question": "Given the taxi trip data in Chicago, tell me the total number of trips and average fare for each 10-minute interval (using rounded-off minutes) of trip durations no more than 1 hour.",
        "external_knowledge": null,
        "plan": "Step 1: Filter and Calculate Duration\n- First, filter and calculate duration. Only include trips where the duration is less than 1 hour (=3600 seconds).\n- Calculate duration and quantiles. Calculate the duration of each trip in minutes and divide the trips into 6 quantiles based on their duration using `NTILE(6) OVER (ORDER BY trip_seconds / 60)`.\n- Aggregate trips and fare. For each duration, count the number of trips and sum the total fare. Then, group the results by `trip_seconds` and `duration_in_minutes`.\nStep 2: Calculate Min and Max Duration for Each Quantile\n- For each quantile, calculate the minimum and maximum duration in minutes using window functions.\n- Sum the trips and total fare for each quantile.\nStep 3: Final Aggregation and Formatting\n- Format the minimum and maximum duration for each quantile into a string representing the range (e.g., \"01m to 10m\").\n- Sum the total trips and calculate the average fare per trip for each duration range.\n- Group and order the results by the formatted duration range",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/NTILE",
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "bq076",
        "db": "bigquery-public-data.chicago_crime",
        "question": "Which month generally has the greatest number of motor vehicle thefts in 2016?",
        "external_knowledge": null,
        "plan": "Which month generally has the greatest number of motor vehicle thefts?\nThe following query summarizes the number of MOTOR VEHICLE THEFT incidents for each year and month, and ranks the month\u2019s total from 1 to 12. Then, the outer SELECT clause limits the final result set to the first overall ranking for each year. According to the data, in 3 of the past 10 years, December had the highest number of car thefts",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "numbering-functions/RANK",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq077",
        "db": "bigquery-public-data.chicago_crime",
        "question": "For each year from 2010 to 2016, what is the highest number of motor thefts in one month?",
        "external_knowledge": null,
        "plan": "1. Filters the crime dataset for motor vehicle theft incidents between 2010 and 2016.\n2. Extracts the month from the date of each incident and groups the data by year and month.\n3. Counts the number of incidents for each year-month combination and ranks the months within each year based on the number of incidents.\n4. Selects the highest number of incidents for each year by filtering for ranking = 1 and orders the final results by year in ascending order.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "numbering-functions/RANK",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq350",
        "db": "open-targets-prod.platform",
        "question": "Please display the drug id, drug type and withdrawal status for approved drugs with a black box warning and known drug type among 'Keytruda', 'Vioxx', 'Premarin', and 'Humira'",
        "external_knowledge": null,
        "plan": "1. **Initialize a List of Drug Names**: Start by declaring a list that contains specific drug names of interest ('Keytruda', 'Vioxx', 'Humira', and 'Premarin').\n\n2. **Select Relevant Columns**: Prepare to select the drug ID, trade name, drug type, and withdrawal status from the dataset.\n\n3. **Unnest Trade Names**: Expand the nested list of trade names associated with each drug entry to allow for individual trade names to be examined.\n\n4. **Filter by Trade Name**: Ensure that only those drugs whose trade name matches one of the names in the initialized list are included.\n\n5. **Filter by Approval Status**: Further restrict the results to only include drugs that have been approved.\n\n6. **Filter by Black Box Warning**: Additionally, include only those drugs that have a black box warning.\n\n7. **Exclude Unknown Drug Types**: Finally, exclude any drugs where the drug type is listed as 'Unknown'.\n\nBy following these steps, the query will return the desired information about the specific drugs of interest that meet all the given criteria.",
        "special_function": [
            "other-functions/SET",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq351_1",
        "db": "open-targets-prod.platform",
        "question": "Which target approved symbol has the overall association score closest to the mean score for psoriasis?",
        "external_knowledge": null,
        "plan": "1. **Calculate the Mean Score:**\n   - Compute the average association score for the specified condition from the association data.\n   \n2. **Retrieve Target Symbols:**\n   - Identify all targets associated with the specified condition by joining the association data with the condition data.\n\n3. **Match Targets to Scores:**\n   - Join the target data with the association data to get the approved symbols of the targets.\n\n4. **Compare with Mean Score:**\n   - For each target associated with the specified condition, calculate the absolute difference between its association score and the previously computed mean score.\n\n5. **Find Closest Match:**\n   - Order the targets by the smallest absolute difference to the mean score.\n\n6. **Select Closest Target:**\n   - Limit the result to the single target whose score is closest to the mean score.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq109",
        "db": "open-targets-genetics.genetics",
        "question": "Find the average, variance, max-min difference, and the QTL source(right study) of the maximum log2(h4/h3) for data where right gene id is \"ENSG00000169174\", h4 > 0.8, h3 < 0.02, reported trait includes \"lesterol levels\", right biological feature is \"IPSC\", and the variant is '1_55029009_C_T'.",
        "external_knowledge": null,
        "plan": "For a given gene, what studies have evidence of colocalisation with molecular QTLs?\n\nWith our variant_disease_coloc and studies, you can find associated GWAS studies with evidence of colocalisation and an H4 greater than 0.8",
        "special_function": [
            "statistical-aggregate-functions/VAR_SAMP",
            "statistical-aggregate-functions/VARIANCE",
            "string-functions/CONCAT"
        ]
    },
    {
        "instance_id": "bq084",
        "db": "bigquery-public-data.goog_blockchain_polygon_mainnet_us",
        "question": "Please count the monthly transaction numbers and transactions per second for each month in 2023, and arrange them in descending order of monthly transaction count.",
        "external_knowledge": null,
        "plan": "What\u2019s the TPS - Transactions Per Second on Polygon per month in 2024? ",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/MOD",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq278",
        "db": "bigquery-public-data.sunroof_solar",
        "question": "Please provide me with details about the state that has the highest number of buildings suitable for solar installations according to Google Maps data, including the state name, percent of buildings in Google Maps covered by Project Sunroof, total solar energy generation potential and potential carbon dioxide abatement.",
        "external_knowledge": null,
        "plan": "1. Compute the average percentage of buildings from the mapping database that are covered by a specific solar project.\n2. Compute the total potential yearly energy generation from solar installations, measured in kilowatt-hours.\n3. Estimate the potential reduction in carbon dioxide emissions due to solar installations.\n4. Calculate the total number of buildings suitable for solar installations as recorded in a mapping database by summing up the counts of qualified buildings across each state.\n5. Sort the query results by the number of buildings in Google Map suitable for solar in descending order.\n6. Return the related information for the state which has the largest number of qualified buildings.\n",
        "special_function": [
            "conversion-functions/CAST",
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq067",
        "db": "bigquery-public-data.nhtsa_traffic_fatalities",
        "question": "I want to build a ML model which can predict whether there will be more than one fatality in a crash invloving 2 or more people. Construct a labelled (0 or 1) dataset for me, and the predictors include the state, vehicle type, the number of drunk drivers, day of the week, hour of the day and another two engineered features, whether the accident happened in the work zone and the average absolute difference between travel speed and speed limit. Please use numeric value for each predictor and categorize the speed difference into levels 0 to 4 based on 20MPH increments (lower bound inclusive while upper exclusive).",
        "external_knowledge": "nhtsa_traffic_fatalities.md",
        "plan": "1. **Label Creation**:\n   - Determine if a crash has more than one fatality by counting the number of severe injuries (categorized as fatal). Assign a binary label (1 for more than one fatality, 0 otherwise).\n\n2. **Feature Extraction**:\n   - Extract relevant features for each crash: state, vehicle type, number of drunk drivers involved, day of the week, and hour of the crash.\n   - Create a binary feature indicating whether the accident occurred in a work zone.\n   - Calculate the average absolute difference between travel speed and speed limit for each crash and categorize this difference into levels (0 to 4) based on 20 MPH increments.\n\n3. **Data Filtering**:\n   - Ensure the data only includes crashes involving at least two people by filtering out crashes with only one person.\n\n4. **Data Aggregation**:\n   - Join the necessary tables to gather all required features for each crash, ensuring the features and label are aggregated correctly. Group the data by crash identifiers and the extracted features to compile the final dataset for model training.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "mathematical-functions/ABS",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq352",
        "db": "bigquery-public-data.sdoh_bea_cainc30\nbigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.sdoh_hrsa_shortage_areas\nbigquery-public-data.sdoh_hud_housing\nbigquery-public-data.sdoh_hud_pit_homelessness\nbigquery-public-data.sdoh_snap_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.",
        "external_knowledge": null,
        "plan": "Is there an association between high commute times [from ACS] and average number of prenatal visits in Wisconsin by County?\nThis query examines the potential correlation between the average number of prenatal visits and length of commutes",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/SUBSTR"
        ]
    },
    {
        "instance_id": "bq352_1",
        "db": "bigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.census_bureau_acs",
        "question": "How much higher is the average percentage of employees commuting between 45 and 59 minutes in 2017 among the top three Wisconsin counties with the highest average prenatal weeks in 2018, compared to that in the bottom three counties?",
        "external_knowledge": null,
        "plan": "1. **Data Preparation**:\n   - Calculate the percentage of employees commuting between 45 and 59 minutes in a specific year.\n   - Extract the average number of prenatal weeks for the same dataset.\n   - Ensure the data is filtered to include only entries from a specific state and year.\n\n2. **Ranking Counties**:\n   - Create a temporary dataset that includes the calculated percentage and the average prenatal weeks for each county.\n   - Rank the counties based on the average number of prenatal weeks.\n\n3. **Select Top and Bottom Counties**:\n   - From the ranked dataset, select the top three counties with the highest average prenatal weeks.\n   - Similarly, select the bottom three counties with the lowest average prenatal weeks.\n\n4. **Calculate Averages**:\n   - Compute the average percentage of employees commuting between 45 and 59 minutes for the top three counties.\n   - Compute the same average for the bottom three counties.\n\n5. **Determine Difference**:\n   - Subtract the average percentage of the bottom three counties from the average percentage of the top three counties.\n   - Return this difference as the final result.",
        "special_function": [
            "mathematical-functions/ROUND",
            "string-functions/SUBSTR"
        ]
    },
    {
        "instance_id": "bq074",
        "db": "bigquery-public-data.sdoh_cms_dual_eligible_enrollment\nbigquery-public-data.census_bureau_acs",
        "question": "How many counties experienced an increase in unemployment between 2015 and 2018, and a decrease in dual-eligible enrollee counts between 2015-12-01 and 2018-12-01?",
        "external_knowledge": null,
        "plan": "1. Prepare datasets for 2015 and 2018 to capture unemployment figures and dual-eligible enrollee counts by county.\n2. Calculate the changes in unemployment and dual-eligible enrollee counts for each county between 2015 and 2018.\n3. Combine the datasets on county identifiers to align the changes in both metrics.\n4. Count the counties where unemployment increased and dual-eligible enrollee counts decreased during the specified period.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq066",
        "db": "bigquery-public-data.sdoh_cdc_wonder_natality\nbigquery-public-data.census_bureau_acs",
        "question": "Could you assess the relationship between poverty rates in 2017 and the percentage of births without maternal morbidity in 2018 by computing their Pearson correlation coefficient?",
        "external_knowledge": null,
        "plan": "1.Extract morbidity and total birth counts for each county for 2018 from separate datasets.\n2.Calculate the percentage of maternal morbidity per county based on the birth data.\n3.Retrieve poverty rates from the 2017 ACS data, ensuring to match the data year closely to the morbidity data year.\n4.Merge the morbidity data with poverty data on county FIPS codes.\n5.Calculate the Pearson correlation coefficient between poverty rates and maternal morbidity percentages to assess their relationship.",
        "special_function": [
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR"
        ]
    },
    {
        "instance_id": "bq114",
        "db": "bigquery-public-data.openaq\nbigquery-public-data.epa_historical_air_quality",
        "question": "What are the top two cities with the largest difference values between the 1990 EPA PM2.5 annual mean and the 2020 PM2.5 values?",
        "external_knowledge": null,
        "plan": "Which city in the US has improved its air quality (PM2.5) the most since 1990?\nThis query joins the EPA Historical Air Quality dataset with OpenAQ to retrieve the cities in the US with the most improved PM2.5 concentrations since 1990.",
        "special_function": [
            "mathematical-functions/ROUND"
        ]
    },
    {
        "instance_id": "bq116",
        "db": "bigquery-public-data.sec_quarterly_financials",
        "question": "What was the highest annual revenue in billions of dollars reported by a U.S. state in 2016, across the main revenue categories and covering all four quarters?",
        "external_knowledge": null,
        "plan": "Total company revenue per State in 2016:\nOther information about companies can be found in the SEC dataset as well. For example the State or Province of the companies mailing address can be found if the company is located in either the US or Canada. Using this information it is possible to compute the total revenues that companies in each state managed to achieve in a given year. For the year 2016 this example query will rank states by total corporate revenues.",
        "special_function": null
    },
    {
        "instance_id": "bq015",
        "db": "bigquery-public-data.stackoverflow\nfh-bigquery.hackernews",
        "question": "Rank the top 10 most discussed tags on Stack Overflow questions that were mentioned on Hacker News since 2014.",
        "external_knowledge": null,
        "plan": "1.Pull details from Stack Overflow questions that match with Hacker News comments based on question IDs, focusing on discussions from 2014 onwards. \n2.Break down and count how often each tag associated with these questions gets mentioned. \n3.Then, list the top 10 tags that came up most frequently.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_EXTRACT",
            "string-functions/SPLIT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/STRING",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq041",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the monthly statistics for new StackOverflow users created in 2021, including the percentage of new users who asked questions and the percentage of those who asked questions and then answered questions within their first 30 days?",
        "external_knowledge": null,
        "plan": "1. **Initialize Variables**: Define the year of interest and the period (in days) for tracking user activity after sign-up.\n\n2. **Filter Users by Year**: Retrieve all users who registered in the specified year.\n\n3. **Join and Aggregate User Questions**:\n   - Join the filtered users with their questions posted within the specified period after sign-up.\n   - Count the number of questions each user asked within this period.\n   - Identify the date of their first question within this period.\n\n4. **Join and Aggregate User Answers**:\n   - Further join the result with answers posted by these users.\n   - Count the number of answers each user provided after their first question within the specified period.\n\n5. **Calculate Monthly Statistics**:\n   - Extract the month from each user\u2019s sign-up date.\n   - Count the total number of new users each month.\n   - Count the number of users who asked at least one question within the specified period.\n   - Calculate the percentage of users who asked questions.\n   - Count the number of users who asked questions and then answered within the specified period.\n   - Calculate the percentage of users who asked questions and then answered.\n\n6. **Aggregate and Order Results**:\n   - Group the statistics by month.\n   - Order the results chronologically by month.",
        "special_function": [
            "date-functions/DATE_DIFF",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE",
            "other-functions/SET"
        ]
    },
    {
        "instance_id": "bq121",
        "db": "bigquery-public-data.stackoverflow",
        "question": "How does average reputation and badge count vary among Stack Overflow users based on their tenure, measured in years?",
        "external_knowledge": null,
        "plan": "What is the reputation and badge count of users across different tenures on StackOverflow?\nThis query breaks down Stack Overflow users into different cohorts by the number of years they\u2019ve been on the platform, and computes the average reputation and number of badges for each cohort. It\u2019s not surprising that users who have been on StackOverflow longer would have higher reputation and number of badges on average. In addition, it\u2019s interesting to see that users typically only begin to have multiple badges after two years on StackOverflow.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "mathematical-functions/ROUND",
            "timestamp-functions/CURRENT_TIMESTAMP",
            "timestamp-functions/TIMESTAMP_DIFF",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq123",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Which day of the week has the third highest percentage of questions answered within an hour?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "Which day of the week has most questions answered within an hour?\nIn this query, we find the best day of the week to ask questions to get an answer very quickly. The query returns day of the week as integers from 1 to 7 (1 = Sunday, 2 = Monday, etc), and the number of questions and answers on each day. We also query how many of these questions received an answer within 1 hour of submission, and the corresponding percentage. The volume of questions and answers is the highest in the middle of the week (Tue, Wed, and Thur), but questions are answered within 1 hour is higher on Saturdays and Sundays",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP_ADD"
        ]
    },
    {
        "instance_id": "bq280",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Please provide the display name of the user who has answered the most questions on Stack Overflow, considering only users with a reputation greater than 10.",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of answers each user has posted and filter out entries where the user ID is not available. \n2. Group the results by user ID to ensure each user is represented once with their total answer count.\n3. Select users who have both a visible display name and a reputation greater than 10.\n4. Combine the user-specific data (display names and reputation) with their corresponding answer counts into a single dataset.\n5. Implement a ranking system within the combined dataset based on the number of answers each user has posted. \n6. Order the users in descending order of answer count.\n7. Extract and return the display name of the top-ranked user.\n",
        "special_function": [
            "numbering-functions/RANK",
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq300",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the highest number of answers received for a single Python 2 specific question on Stack Overflow, excluding any discussions that involve Python 3?",
        "external_knowledge": null,
        "plan": "1. **Filter Questions Related to Python 2:**\n   - Create a temporary dataset of questions that are specific to Python 2 by checking if they contain specific tags or keywords related to Python 2.\n   - Ensure the selected questions do not mention Python 3 by excluding any questions with keywords or tags related to Python 3.\n\n2. **Join with Answers Table:**\n   - Merge the filtered dataset of Python 2 specific questions with the answers dataset, linking questions with their corresponding answers using a common identifier.\n\n3. **Count the Number of Answers:**\n   - For each question in the merged dataset, count the number of associated answers.\n\n4. **Sort and Limit Results:**\n   - Sort the counted results in descending order to identify the question with the highest number of answers.\n   - Limit the final output to just one result, which will be the highest count of answers received for a single Python 2 specific question.\n\n5. **Return the Result:**\n   - Output the highest count of answers as the final result.",
        "special_function": [
            "string-functions/LOWER"
        ]
    },
    {
        "instance_id": "bq301",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Retrieve details of accepted answers related to JavaScript security topics such as XSS, cross-site scripting, exploits, and cybersecurity, for questions posted in January 2016 on Stack Overflow. For each accepted answer, include the answer's ID, the answerer's reputation, score, and comment count, along with the associated question's tags, score, answer count, the asker's reputation, view count, and comment count.",
        "external_knowledge": null,
        "plan": "1. **Select Relevant Answer Details**: Extract key details of each answer, including the answer ID, the score of the answer, and the number of comments on the answer.\n\n2. **Fetch Answerer's Reputation**: For each answer, retrieve the reputation of the user who posted the answer by matching the user ID of the answer owner.\n\n3. **Select Associated Question Details**: For each answer, fetch the details of the associated question, including the tags, the score of the question, the number of answers to the question, the view count, and the number of comments on the question.\n\n4. **Fetch Asker's Reputation**: For each question, retrieve the reputation of the user who posted the question by matching the user ID of the question owner.\n\n5. **Join Answers with Questions**: Link each answer to its associated question using the question's ID and the parent ID of the answer.\n\n6. **Filter for Accepted Answers**: Ensure that only answers which have been accepted as the best answer for their respective questions are included.\n\n7. **Filter by Tags**: Ensure that the associated questions have tags related to JavaScript security topics, specifically those containing keywords like JavaScript, XSS, cross-site scripting, exploits, or cybersecurity.\n\n8. **Filter by Date**: Ensure that the questions were posted within January 2016 and that the corresponding answers were also posted within January 2016.\n\n9. **Compile Results**: Combine all the retrieved information into a single result set, including the answer's ID, answerer's reputation, answer's score, answer's comment count, question's tags, question's score, question's answer count, asker's reputation, question's view count, and question's comment count.",
        "special_function": [
            "date-functions/DATE"
        ]
    },
    {
        "instance_id": "bq302",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the monthly proportion of Stack Overflow questions tagged with 'python' in the year 2022?",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Year**: Extract data for the specific year of interest, filtering out all records that do not match the specified year.\n\n2. **Format Creation Date**: Transform the creation date of each record into a month-based index to facilitate monthly aggregation.\n\n3. **Aggregate Monthly Questions**: Calculate the total number of questions posted each month by grouping the data based on the month index and counting the records.\n\n4. **Extract and Count Specific Tags**: Split the tags associated with each question into individual tags, filter for the specific tag of interest, and count the occurrences of this tag for each month.\n\n5. **Calculate Proportion**: Combine the monthly counts of all questions and the counts of questions with the specific tag. Compute the proportion of questions with the specific tag by dividing the count of tagged questions by the total number of questions for each month.\n\n6. **Sort Results**: Order the final results by the month index to present the data chronologically.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "string-functions/SPLIT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/FORMAT_TIMESTAMP",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq303",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the user IDs and tags for comments, answers, and questions posted by users with IDs between 16712208 and 18712208 on Stack Overflow during July to December 2019?",
        "external_knowledge": null,
        "plan": "1. **Filter Comments on Answers**:\n   - Retrieve comments made by users within the specified ID range.\n   - Ensure the comments are posted within the given date range.\n   - Identify the original question associated with each comment by joining with answers.\n\n2. **Filter Comments on Questions**:\n   - Retrieve comments made by users within the specified ID range.\n   - Ensure the comments are posted within the given date range.\n   - Identify the original question associated with each comment by joining with questions.\n\n3. **Combine Comment Results**:\n   - Union the results of comments on answers and questions.\n   - For each combined comment result, include the user ID, creation date, comment text, tags from the associated question, and label it as a comment.\n\n4. **Filter Answers**:\n   - Retrieve answers made by users within the specified ID range.\n   - Ensure the answers are posted within the given date range.\n   - Join with questions to get the tags associated with each answer.\n   - Include the user ID, creation date, answer text, tags from the associated question, and label it as an answer.\n\n5. **Filter Questions**:\n   - Retrieve questions made by users within the specified ID range.\n   - Ensure the questions are posted within the given date range.\n   - Include the user ID, creation date, question text, tags, and label it as a question.\n\n6. **Combine All Results**:\n   - Union the results of comments, answers, and questions.\n   - Select the relevant fields: user ID and tags.\n\n7. **Order Results**:\n   - Sort the combined results by user ID and creation date to maintain chronological order.",
        "special_function": [
            "date-functions/DATE",
            "string-functions/LEFT"
        ]
    },
    {
        "instance_id": "bq304",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What are the top 50 most viewed 'how-to' questions for each of the following Android-related tags on StackOverflow: 'android-layout', 'android-activity', 'android-intent', 'android-edittext', 'android-fragments', 'android-recyclerview', 'listview', 'android-actionbar', 'google-maps', and 'android-asynctask'? Ensure that each tag has at least 50 questions and exclude any questions containing terms typically associated with troubleshooting, such as 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble'.",
        "external_knowledge": null,
        "plan": "1. **Define Relevant Tags**:\n   - Create a list of specific Android-related tags that are of interest.\n   \n2. **Filter Questions by Tags and Content**:\n   - Select questions that include any of the specified tags.\n   - Ensure these questions contain 'how' in the title or body.\n   - Exclude questions that contain troubleshooting terms like 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble' in the title or body.\n\n3. **Rank Questions by Tag**:\n   - For each tag, rank the filtered questions based on their view count.\n   - Ensure each tag has at least 50 valid questions.\n\n4. **Select Top Questions**:\n   - For each tag, select the top 50 questions based on the view count ranking.\n\n5. **Order the Results**:\n   - Order the final list of question IDs first by the tag's position in the original list and then by the view count ranking within each tag.",
        "special_function": [
            "numbering-functions/RANK",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq304_1",
        "db": "bigquery-public-data.stackoverflow",
        "question": "What is the title of the most viewed \"how-to\" question related to Android development on StackOverflow, across specified tags such as 'android-layout', 'android-activity', 'android-intent', and others",
        "external_knowledge": null,
        "plan": "1. **Create a List of Relevant Tags**:\n    - Define a list of tags related to Android development that are of interest.\n\n2. **Filter Questions by Tags and Content**:\n    - From the database of questions, filter out those that contain any of the specified tags.\n    - Additionally, ensure that these questions include the keyword \"how\" in either their title or body to identify \"how-to\" questions.\n\n3. **Identify the Most Viewed Question**:\n    - Join the relevant tags with the filtered \"how-to\" questions to ensure we are focusing on questions that match our tags of interest.\n    - Sort these questions by their view count in descending order to identify the most viewed one.\n\n4. **Retrieve the Title of the Most Viewed Question**:\n    - Select the title of the question that has the highest view count from the sorted list.\n    - Limit the result to the top-most entry to get only the title of the single most viewed question.\n\nBy following these steps, you can determine the title of the most viewed \"how-to\" question related to Android development within the specified tags.",
        "special_function": [
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq305",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Identify the top 10 users by the total view count of their associated questions. Include users who own a question, provide an accepted answer, have an answer with a score above 5, rank in the top 3 for a question, or have an answer with a score over 20% of the total answer score for that question. Use these criteria to determine the questions and answers to include in the view count calculation.",
        "external_knowledge": null,
        "plan": "1. **Initialize the View Count Calculation**:\n   - Begin with an initial setup to sum the view counts for questions.\n\n2. **Identify Relevant Users**:\n   - Establish a list of users who are relevant for the calculation based on various criteria.\n\n3. **Criteria for User Inclusion**:\n   - **Question Owners**: Include users who own a question.\n   - **Accepted Answer Providers**: Include users whose answers have been accepted.\n   - **High Score Answer Providers**: Include users whose answers have a score above a certain threshold.\n   - **Top Ranked Answer Providers**: Include users whose answers rank within the top 3 for a question.\n   - **Significant Score Contribution Providers**: Include users whose answers contribute significantly (over 20%) to the total score of answers for a question.\n\n4. **Combine All Relevant Users**:\n   - Use a union operation to merge all identified users from the different criteria into a single list, ensuring each user is only included once.\n\n5. **Calculate View Counts for Questions**:\n   - Join the list of relevant users with the questions they are associated with to sum up the view counts for these questions.\n\n6. **Group and Sum View Counts by User**:\n   - Group the summed view counts by user to get the total view count each user has reached through their questions.\n\n7. **Order and Limit the Results**:\n   - Sort the users by their total view counts in descending order to identify the top users.\n   - Limit the result to the top 10 users.\n\nBy following these steps, the query accurately identifies the top users based on the total view count of their associated questions, using the specified criteria.",
        "special_function": [
            "numbering-functions/RANK",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq306",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Identify the top 10 tags for user 1908967 by calculating a reputation score based on upvotes and accepted answers before June 7, 2018. The score is calculated as 10 times the upvotes plus 15 times the accepted answers.",
        "external_knowledge": null,
        "plan": "1. **Initialize Subqueries for Votes and Accepted Answers:**\n   - **Subquery for Votes:**\n     1. Retrieve records from the votes and answers tables.\n     2. Join the votes with answers and questions to gather tags associated with each vote.\n     3. Filter results to include only upvotes (vote_type_id = 2) made before June 7, 2018, for the specified user.\n     4. Split the tags string into individual tags.\n     5. Count occurrences of each tag.\n   - **Subquery for Accepted Answers:**\n     1. Similar to the votes subquery but filter for accepted answers (vote_type_id = 1).\n     2. Count occurrences of each tag for accepted answers.\n\n2. **Combine Subquery Results:**\n   - Perform a full outer join on the results of the votes and accepted answers subqueries based on tags to ensure all tags from both subqueries are included.\n\n3. **Calculate Reputation Score:**\n   - For each tag, compute the reputation score as 10 times the vote count plus 15 times the accepted answer count.\n\n4. **Select and Sort Results:**\n   - Select the tag, calculated reputation score, vote count, and accept count.\n   - Sort the results by the reputation score in descending order.\n\n5. **Limit Results:**\n   - Limit the final output to the top 10 tags based on the calculated reputation score.",
        "special_function": [
            "date-functions/DATE",
            "string-functions/SPLIT",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq307",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Find the top 10 most common first gold badges on Stack Overflow, showing how many users earned each and the average days from account creation to earning the badge.",
        "external_knowledge": null,
        "plan": "1. **Initial Badge and User Data Retrieval**:\n   - Retrieve user and badge data by joining the user and badge tables on the user ID.\n   - Filter the badges to include only those of a specific type (e.g., gold badges).\n\n2. **Calculate Tenure**:\n   - For each badge, calculate the number of days between the user's account creation date and the date the badge was earned.\n\n3. **Identify First Badge**:\n   - Assign a row number to each badge per user based on the date the badge was earned, ordering from the earliest to the latest.\n\n4. **Filter to First Badge**:\n   - Filter the dataset to include only the first badge earned by each user, identified by the row number being 1.\n\n5. **Aggregate Data**:\n   - Group the filtered data by the badge name.\n   - For each badge name, count the number of users who earned it as their first badge.\n   - Calculate the average number of days from account creation to earning the badge, rounding to the nearest whole number.\n\n6. **Sort and Limit Results**:\n   - Order the results by the number of users in descending order to find the most common first badges.\n   - Limit the results to the top 10 most common first badges.\n\n7. **Output**:\n   - Display the badge name, the number of users who earned it, and the average number of days from account creation to earning the badge.",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/ROW_NUMBER",
            "timestamp-functions/TIMESTAMP_DIFF"
        ]
    },
    {
        "instance_id": "bq308",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Show the number of Stack Overflow questions asked each day of the week in 2020, and find out how many and what percentage of those were answered within one hour.",
        "external_knowledge": null,
        "plan": "1. **Extract Day of the Week and Answered Status**:\n    - For each question, determine the day of the week it was asked.\n    - Check if the question has any answers.\n    - If there are answers, determine if any of them were provided within one hour of the question being posted.\n\n2. **Filter by Year**:\n    - Ensure that both the question and its answers were created in the year 2020.\n\n3. **Group Data**:\n    - Group the data by question and day of the week.\n    - For each group, calculate if it has been answered within one hour.\n\n4. **Aggregate by Day of the Week**:\n    - Group the results by the day of the week.\n    - Count the total number of questions for each day of the week.\n    - Sum the number of questions answered within one hour for each day of the week.\n\n5. **Calculate Percentage**:\n    - For each day of the week, compute the percentage of questions that were answered within one hour.\n\n6. **Order Results**:\n    - Sort the results by the day of the week in ascending order.\n\nThis plan ensures that we get a clear picture of the number of questions asked each day of the week in 2020, how many were answered within an hour, and what percentage that represents.",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "mathematical-functions/ROUND",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/UNIX_SECONDS",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq309",
        "db": "bigquery-public-data.stackoverflow",
        "question": "Show the top 10 longest Stack Overflow questions where the question has an accepted answer or an answer with a score-to-view ratio above 0.01, including the user's reputation, net votes, and badge count.",
        "external_knowledge": null,
        "plan": "1. **Create Badge Counts**:\n   - Generate a temporary table that calculates the number of unique badges each user has by joining the user data with badge data.\n   - Group the results by user IDs.\n\n2. **Label Questions**:\n   - Generate a temporary table that includes all questions, where each question is labeled based on specific criteria:\n     - Check if the question has an accepted answer.\n     - If not, check if it has any answer with a score-to-view ratio greater than 0.01.\n     - Assign a label of 1 if any of these conditions are met, otherwise assign 0.\n   - Also, include the length of the question's body and the question owner's user ID in this table.\n\n3. **Filter and Join Data**:\n   - Select questions from the labeled questions table where the label is 1 (indicating they meet the criteria).\n   - Join this filtered list with user data to retrieve the user\u2019s reputation and net votes.\n   - Also, join with the badge counts table to retrieve the number of badges each user has.\n\n4. **Sort and Limit Results**:\n   - Order the resulting questions by the length of their bodies in descending order.\n   - Limit the final output to the top 10 longest questions.",
        "special_function": [
            "string-functions/LENGTH",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq124",
        "db": "bigquery-public-data.fhir_synthea",
        "question": "Can you identify how many alive patients, currently managing chronic conditions such as diabetes or hypertension, are prescribed seven or more medications?",
        "external_knowledge": null,
        "plan": "1.Pull up information on patients who are still alive from the healthcare records.\n2.Check how many different medications each patient is currently prescribed by looking at their active prescriptions.\n3.Focus on those who are dealing with chronic issues like diabetes or hypertension.\n4.Specifically, zoom in on patients who are juggling seven or more medications.\n5.Count how many alive patients, currently managing chronic conditions such as diabetes or hypertension, are prescribed seven or more medications",
        "special_function": [
            "aggregate-functions/STRING_AGG",
            "array-functions/ARRAY_TO_STRING",
            "timestamp-functions/TIMESTAMP",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq126",
        "db": "bigquery-public-data.the_met",
        "question": "What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created by known artists, with an object end date of 1839 or earlier?",
        "external_knowledge": null,
        "plan": "What are the earliest photographs in the collection?\nThe Met\u2019s Department of Photographs has some of the earliest photographic works. This query retrieves artwork from the \u201cPhotograph\u201d department and joins the object and image tables to return the results, presented with the oldest items first. In this case, we see the British photography pioneer William Henry Fox Talbot\u2019s botanical images.",
        "special_function": null
    },
    {
        "instance_id": "bq200",
        "db": "bigquery-public-data.baseball",
        "question": "Can you find the full name of the fastest pitcher on each team by maximum pitch speed, using both regular and post-season data?",
        "external_knowledge": null,
        "plan": "1. Select the necessary fields from the baseball games dataset, such as pitcher ID, team, and pitch speed.\n2. Rank pitchers within each team based on their maximum pitch speed in descending order.\n3. Filter out the pitchers ranked first within their team, which identifies the fastest pitcher per team.\n4. Display the team, pitcher, and their fastest recorded pitch speed.",
        "special_function": [
            "numbering-functions/ROW_NUMBER",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq204",
        "db": "bigquery-public-data.eclipse_megamovie",
        "question": "Find the user with the highest total clicks across all records from all available photo collections.",
        "external_knowledge": null,
        "plan": "1. **Union All Data**:\n   - Combine all records from three different collections into one dataset using `UNION ALL`. This step ensures that every record from each collection is included in the analysis.\n\n2. **Aggregate User Clicks**:\n   - Group the combined data by the user and count the total number of clicks (or records) associated with each user. This provides a count of how many times each user appears across all collections.\n\n3. **Find Maximum Clicks**:\n   - From the aggregated user data, determine the maximum count of clicks any single user has. This identifies the highest total clicks achieved by any user across all collections.\n\n4. **Identify Top User(s)**:\n   - Select the user(s) whose total click count matches the maximum click count found in the previous step. This ensures that only the user(s) with the highest total clicks are considered.\n\n5. **Order and Limit Results**:\n   - Order the final results by the click count to maintain consistency and limit the output to the top user. This step ensures that only the user with the highest click count is returned.",
        "special_function": null
    },
    {
        "instance_id": "bq360",
        "db": "bigquery-public-data.nppes",
        "question": "Which of the top 10 most common healthcare provider specializations in Mountain View, CA, has a specialist count closest to the average of these ten specializations?",
        "external_knowledge": null,
        "plan": "1. **Count Specialists by Specialization**:\n   - Identify all unique specializations within the specified city and state.\n   - Count the number of specialists for each specialization.\n\n2. **Select Top 10 Specializations**:\n   - From the previous results, order the specializations by the number of specialists in descending order.\n   - Select the top 10 specializations based on the highest counts.\n\n3. **Calculate Average Number of Specialists**:\n   - Compute the average number of specialists across the top 10 specializations identified in the previous step.\n\n4. **Determine Differences from Average**:\n   - For each of the top 10 specializations, calculate the absolute difference between its number of specialists and the average number of specialists.\n\n5. **Identify Closest Specialization**:\n   - From the results of the previous step, order the specializations by the calculated difference.\n   - Select the specialization with the smallest difference, i.e., the one closest to the average.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq286",
        "db": "bigquery-public-data.usa_names",
        "question": "Can you tell me the name of the most popular female baby in Wyoming for the year 2021, based on the proportion of female babies given that name compared to the total number of female babies given the same name across all states?",
        "external_knowledge": null,
        "plan": "1. Calculate the total count of each name grouped by name, gender, and year.\n2. Join the above result with the main dataset containing records of names based on matching names, genders, and years.\n3. Filter the joined data to focus on records where the gender is female ('F') and the state is specific ('WY'), and the year is a particular year (2021).\n4. For the filtered results, calculate the ratio of the count of each name to its total count across all states.\n5. Sort the results in descending order based on the calculated ratio.\n6. Return the name with the highest proportion.\n",
        "special_function": null
    },
    {
        "instance_id": "bq043",
        "db": "isb-cgc-bq.TCGA_versioned",
        "question": "What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?",
        "external_knowledge": null,
        "plan": "Bring in the patient data from the ISB-CGC TCGA Clinical table so that we can see each patient\u2019s gender, vital status and days to death.",
        "special_function": null
    },
    {
        "instance_id": "bq042",
        "db": "bigquery-public-data.noaa_gsod",
        "question": "Help me analyze the weather conditions (including temperature, wind speed and precipitation) at NYC's airport LaGuardia for June 12, year over year, starting from 2011 to 2020.",
        "external_knowledge": null,
        "plan": "1. Filter Data for Specific Dates and Location: Extract the records for the specified date (June 12) across the given range of years (2011-2020) and for the specific location (LaGuardia airport).\n\n2. Handle Missing or Invalid Data: Replace any placeholder values indicating missing data with `NULL` for temperature and wind speed, and with `0` for precipitation.\n\n3. Calculate Averages: Compute the average values for temperature, wind speed, and precipitation on June 12 for each year.\n\n4. Organize Results: Create a timestamp for each record, group the results by this timestamp, and sort the output in ascending order of the timestamp for clear year-over-year analysis.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq047",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Could you help me analyze the relationship between each complaint type and daily temperature in New York city, focusing on data in airports LaGuardia and JFK over the 10 years starting from 2008? Calculate the total complaint count, the total day count, and the Pearson correlation coefficient (rounded to 4 decimals) between temperature and both the count and percentage of each common (>5000 occurrences) and strongly correlated (absolute value > 0.5) complaint type.",
        "external_knowledge": null,
        "plan": "1. Temperature Data Preparation:\n    - Extract and clean weather data for the specified 10-year period, focusing on the relevant locations.\n    - Transform date components into a single timestamp and handle any placeholder values for temperature.\n    - Calculate the average temperature for each day from the cleaned weather data.\n\n2. Prepare Complaint Data:\n    - Extract and aggregate complaint data by type and date, counting the number of complaints per day for each type.\n    - Calculate the daily proportion of each complaint type relative to the total complaints on that day.\n\n3. Join Weather and Complaint Data:\n    - Combine the weather data with the complaint data based on matching dates.\n    - Aggregate this joined data by complaint type and temperature to compute the average daily counts and proportions of each complaint.\n\n4. Analyze and Filter Results:\n    - Compute the correlation between temperature and both the average daily count and proportion of each complaint type using function `CORR`.\n    - Filter the results to include only those complaint types that are both common (more than 5000 total occurrences) and strongly correlated (with absolute correlation value greater than 0.5).\n    - Sort the results by the strength of the correlation.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ABS",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq048",
        "db": "bigquery-public-data.noaa_gsod\nbigquery-public-data.new_york",
        "question": "Which common complaint types have the strongest positive and negative correlation with wind speed respectively, given the data in NYC JFK Airport from year 2011 to 2020? Also, provide the corresponding correlation values (rounded to 4 decimals).",
        "external_knowledge": null,
        "plan": "1. Aggregate Wind Speed Data: Create a dataset with daily average wind speed values for a specific location and time range, replacing erroneous values with nulls.\n\n2. Compute Daily Complaint Data: Generate daily complaint counts and their proportions relative to all complaints for different complaint types over the same time range.\n\n3. Join Weather and Complaint Data: Merge the weather dataset with the complaint dataset on their date fields to align daily wind speed data with daily complaint data.\n\n4. Calculate Correlations: For each complaint type, calculate the correlation between the average daily wind speed and the average daily complaint count, filtering out complaint types with insufficient data.\n\n5. Identify Extremes: Select the complaint types with the strongest positive and negative correlations with wind speed, and report these types along with their correlation values.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "mathematical-functions/ROUND",
            "statistical-aggregate-functions/CORR",
            "string-functions/CONCAT",
            "string-functions/REPLACE",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq158",
        "db": "pancancer-atlas.Filtered",
        "question": "What histological type of breast cancer (BRCA) has the highest percentage of CDH1 gene mutations, according to the data in the PanCancer Atlas?",
        "external_knowledge": null,
        "plan": "1. **Filter and Select Relevant Data**:\n   - Extract a list of participants with their respective cancer types from a clinical dataset, focusing specifically on a certain type of cancer. Ensure that the cancer type information is not missing.\n\n2. **Identify Gene Mutations**:\n   - From a mutation dataset, select participants who have mutations in a specific gene. Filter out any records that do not pass quality control checks and group by participant and gene symbol to eliminate duplicate entries.\n\n3. **Combine Datasets**:\n   - Perform a left join between the clinical data (step 1) and the mutation data (step 2) based on participant identifiers. This will associate each participant's cancer type with whether they have the specific gene mutation or not.\n\n4. **Summarize Mutation Data**:\n   - For each cancer type, count the number of participants with and without the specific gene mutation. Summarize this data to get the total counts.\n\n5. **Calculate Mutation Percentages**:\n   - For each cancer type, calculate the percentage of participants that have the specific gene mutation. This involves dividing the count of participants with the mutation by the total number of participants for each cancer type.\n\n6. **Identify the Highest Mutation Percentage**:\n   - Sort the cancer types by their mutation percentage in descending order and select the top entry, which corresponds to the cancer type with the highest percentage of participants having the specific gene mutation.",
        "special_function": [
            "conditional-functions/CASE",
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq159",
        "db": "pancancer-atlas.Filtered",
        "question": "What is the chi-square value for the association between histological types and the presence of CDH1 gene mutations in BRCA patients, according to the PanCancer Atlas?",
        "external_knowledge": null,
        "plan": "1. **Define the first dataset**:\n    - Extract records related to a specific condition from a clinical data source.\n    - Select relevant identifiers and attributes for these records.\n\n2. **Define the second dataset**:\n    - Extract records related to gene mutations from a genetic data source.\n    - Filter these records to include only those related to a specific gene and study.\n    - Group the records by relevant identifiers.\n\n3. **Create a summary table**:\n    - Perform a left join between the first and second datasets on a common identifier.\n    - Categorize the presence or absence of gene mutations.\n    - Count the occurrences for each combination of attributes.\n\n4. **Calculate expected counts**:\n    - Sum the counts from the summary table for each category of the first attribute.\n    - Sum the counts for each category of the second attribute.\n    - Cross join these sums to generate all possible combinations of categories.\n    - Filter combinations where both sums are greater than a specified threshold.\n\n5. **Build the contingency table**:\n    - Join the expected counts with the summary table to align actual and expected counts.\n    - Compute expected counts for each combination of categories.\n    - Replace null values with zeroes where necessary.\n\n6. **Compute the chi-square statistic**:\n    - Calculate the chi-square value using the formula involving actual and expected counts.\n    - Sum the chi-square contributions for all combinations to get the final statistic.\n\n7. **Return the result**:\n    - Output the computed chi-square value.",
        "special_function": [
            "conditional-functions/IF"
        ]
    },
    {
        "instance_id": "bq279",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Can you provide the number of distinct active and closed bike share stations for each year 2013 and 2014 in a chronological view?",
        "external_knowledge": null,
        "plan": "1. Extract the year and start station ID from the trips data for the years 2013 and 2014.\n2. Join the extracted trip data with the station data on the station ID to consolidate trip and station information.\n3. Limit the joined data to only include records from the years 2013 and 2014.\n4. For each year, count the distinct station IDs where the station status is 'active'. This involves filtering the joined data for 'active' stations and then counting distinct station IDs for each year.\n5. Similarly, for each year, count the distinct station IDs where the station status is 'closed'. This involves filtering the joined data for 'closed' stations and then counting distinct station IDs.\n6. Group the results by year to ensure that the counts of active and closed stations are organized by year.\n7. Order the final results by year to provide a chronological view of the data.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq281",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "What is the highest number of electric bike rides lasting more than 10 minutes taken by subscribers with 'Student Membership' in a single day, excluding rides starting or ending at 'Mobile Station' or 'Repair Shop'?",
        "external_knowledge": null,
        "plan": "1. Filter the dataset to exclude records where the starting or ending locations are listed as either 'Mobile Station' or 'Repair Shop'.\n2. Narrow down the dataset to include only those records where users have students subscription type and the equipment used is electric.\n3. Consider only records where the duration of usage is greater than ten minutes.\n4. Organize the filtered data by grouping it based on the year, month, and day of the start time.\n5. For each grouped set of data (each day), count the total number of records that match the earlier set conditions.\n6. Order the results by the count of records in descending order to identify the date with the highest number of trips meeting all the specified conditions.\n7. Limit the output to only the top result from the sorted list and show the maximum usage in a single day under the defined conditions.\n",
        "special_function": [
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT"
        ]
    },
    {
        "instance_id": "bq282",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Can you tell me the numeric value of the active council district in Austin which has the highest number of bike trips that start and end within the same district, but not at the same station?",
        "external_knowledge": null,
        "plan": "1. Filter the stations table to keep records whose status is 'active'.\n2. Perform an inner join between the trips table and the filtered active stations.\n3. Select the district from the stations table for end stations that are active and match the end station ID from the trips table.\n4. Include only those trips where the starting district is present in the list of active end station districts.\n5. Exclude trips where the start station ID is the same as the end station ID.\n6. Group the results by the district and count the number of trips originating from each district.\n7. Order the districts by the descending count of trips to find the district with the highest number of trips.\n8. Limit the output to the district with the maximum number of trips.",
        "special_function": [
            "conversion-functions/SAFE_CAST"
        ]
    },
    {
        "instance_id": "bq283",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "What is the combined percentage of all bike trips that start from the top 15 most used active stations in Austin?",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of trips originating from each station.\n2. Assign a rank to each station based on the descending order of total trips, where the station with the highest number of trips has the highest rank.\n3. Calculate the percentage of total trips that each station represents by dividing the number of trips from that station by the total number of trips from all stations.\n4. Select only those stations that rank within the top 15.\n5. Join the data of the top 15 stations with another dataset that contains additional details about each station.\n6. From the joined data, filter out stations to keep only those that are currently active.\n7. From the filtered active top 15 stations, select the station with the highest rank.\n8. Retrieve the percentage of total trips for this station and display the result.\n",
        "special_function": [
            "mathematical-functions/ROUND",
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq329",
        "db": "bigquery-public-data.austin_bikeshare\nbigquery-public-data.austin_311\nbigquery-public-data.austin_crime\nbigquery-public-data.austin_incidents\nbigquery-public-data.austin_waste",
        "question": "Which bike station in the Austin bikeshare system has the lowest average trip duration? I want the station ID.",
        "external_knowledge": null,
        "plan": "1. **Define Data Sources**:\n   - Establish temporary datasets for bike stations and trip data by selecting all relevant columns from each respective source.\n\n2. **Prepare Trip Data**:\n   - Within the trip data, ensure proper data types by safely converting necessary fields to integers and extracting the required columns for analysis.\n\n3. **Calculate Average Trip Duration**:\n   - Group the trip data by the starting station and calculate the average trip duration for each group.\n\n4. **Join Data**:\n   - Merge the average trip duration data with the station data based on the station identifier.\n\n5. **Determine Station with Lowest Average Duration**:\n   - Sort the merged dataset by the average trip duration in ascending order.\n   - Select the station identifier of the station with the lowest average trip duration.\n   - Limit the result to only one record to get the station with the absolute lowest average trip duration.",
        "special_function": [
            "conversion-functions/SAFE_CAST"
        ]
    },
    {
        "instance_id": "bq284",
        "db": "bigquery-public-data.bbc_news",
        "question": "Can you provide a breakdown of the total number of articles into different categories and the percentage of those articles that mention \"Education\" within each category (tech, sport, business, politics, and entertainment) from the BBC News dataset?",
        "external_knowledge": null,
        "plan": "1. Group all records by their category.\n2. For each category group, count the total number of records.\n3. For each category, count the number of records where the body of the text contains the word 'Education'.\n4. For each category, count the total number of records that belong to the current category.\n5. For each category, calculate the percentage of records that mention 'Education' by dividing the count of 'Education' Mentions by the count of total records and then multiply by 100 to get a percentage.\n6. Combine the total count and the calculated percentage for each category into a single output table.\n",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq355",
        "db": "bigquery-public-data.cms_medicare\nbigquery-public-data.cms_codes\nbigquery-public-data.cms_synthetic_patient_data_omop",
        "question": "Please tell me the proportion of participants not using quinapril and related medications(Quinapril RxCUI: 35208).",
        "external_knowledge": null,
        "plan": "1. **Identify Target Medication:**\n   - Query a table to find the unique identifier for the specified medication based on its code and vocabulary system.\n\n2. **Determine Related Medications:**\n   - Use the identified medication's unique identifier to find all related medications by querying an ancestry table, which tracks medication hierarchies and relationships.\n\n3. **Count Participants Using Target Medications:**\n   - Query a drug exposure table to count the number of unique participants who have been prescribed any of the related medications found in the previous step.\n\n4. **Count Total Participants:**\n   - Query a participant table to count the total number of unique participants in the dataset.\n\n5. **Calculate Proportion Not Using Target Medications:**\n   - Use the counts from steps 3 and 4 to calculate the proportion of participants who are not using the target or related medications. This is done by subtracting the percentage of users from 100%.\n\n6. **Return Result:**\n   - Output the calculated proportion as the final result.",
        "special_function": null
    },
    {
        "instance_id": "bq285",
        "db": "bigquery-public-data.fdic_banks",
        "question": "Could you provide me with the zip code of the location that has the highest number of bank institutions in Florida?",
        "external_knowledge": null,
        "plan": "1. Retrieve the FIPS code for Florida from a dataset that contains FIPS codes for all states.\n2. Limit the data to zip codes that are only in the Florida state by using FIPS codes to filter the dataset.\n3. From a dataset containing institution locations, perform an aggregation to count the number of locations in each zip code.\n4. Combine the filtered list of zip codes with the aggregated location counts.\n5. Group the data from the join by zip code and sum the counts of locations. \n6. Order the results in descending order based on the sum of location counts.\n7. Limit the result to the top entry, which represents the zip code with the highest number of locations.\n",
        "special_function": null
    },
    {
        "instance_id": "bq287",
        "db": "bigquery-public-data.fdic_banks\nbigquery-public-data.census_bureau_acs",
        "question": "What is the employment rate (only consider population over 16) in the Utah zip code that has the fewest number of bank locations based on American Community Survey data in 2017?",
        "external_knowledge": null,
        "plan": "1. Retrieve the FIPS code for the state of Utah.\n2. Use the Utah FIPS code to filter and obtain a list of ZIP codes associated with Utah from a dataset containing ZIP codes and their corresponding state FIPS codes.\n3. For each ZIP code in Utah, count the number of banking institution locations and ensure that only ZIP codes with valid state and state name information are considered.\n4. Join the banking locations dataset with the banking institutions dataset.\n5. For each ZIP code in Utah, calculate the employment rate using the population over 16 and the employed population.\n6. Combine the datasets containing the Utah ZIP codes, the number of bank locations per ZIP code, and the employment rates.\n7. From the combined dataset, select the employment rate for the ZIP code that has the fewest number of banking institution locations.\n8. Order the results by the number of locations in ascending order.\n9. Limit the result to just one record to find the ZIP code with the lowest number of bank locations and its corresponding employment rate.\n",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "mathematical-functions/ROUND",
            "mathematical-functions/SAFE_DIVIDE",
            "timestamp-functions/STRING"
        ]
    },
    {
        "instance_id": "bq288",
        "db": "bigquery-public-data.fdic_banks",
        "question": "Could you tell me the number of banking institutions in the state with the highest total assets for banks established between 1900 and 2000 (both included) and its name starting with \"Bank\"?",
        "external_knowledge": null,
        "plan": "1. Calculate the number of institutions per state.\n2. Gather detailed information about the total assets for each state.\n3. Join the results above based on the matching state names.\n4. Sort the results by the total assets in descending order to bring the state with the highest sum of assets to the top.\n5. Limit the output to only the top record, which corresponds to the state with the highest total assets among the filtered institutions.\n6. Display the count of institutions for the state with the highest total assets.\n",
        "special_function": null
    },
    {
        "instance_id": "bq289",
        "db": "bigquery-public-data.geo_us_census_places",
        "question": "Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia?",
        "external_knowledge": null,
        "plan": "1. Extract all records where the place name matches 'Philadelphia'.\n2. For each point feature from a global geographic dataset, check if the point lies within the Philadelphia area.\n3. From the point's tags, filter those records where the tag key indicates an 'amenity' and the value is one of the specified types (library, place of worship, community center).\n4. Extract the value of the 'amenity' tag for each qualifying point.\n5. Perform a self-join on the amenities dataset to compare each amenity with every other amenity.\n6. For each pair of amenities (ensuring pairs are unique and non-repeating by comparing IDs), calculate the geographical distance between them.\n7. Assign a row number to each paired record, partitioned by the ID of the first amenity and ordered by the calculated distance.\n8. From the results of the self-join, filter to retain those records where the row number is 1.\n9. Order these records by distance in ascending order to find the pair with the shortest distance among all.\n",
        "special_function": [
            "geography-functions/ST_CONTAINS",
            "geography-functions/ST_DISTANCE",
            "numbering-functions/ROW_NUMBER",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq226",
        "db": "bigquery-public-data.goog_blockchain_cronos_mainnet_us",
        "question": "Can you find me the complete url of the most frequently used sender's address on the Cronos blockchain since January 1, 2023, where transactions were made to non-null addresses and in blocks larger than 4096 bytes?",
        "external_knowledge": null,
        "plan": "1. Start by joining two datasets based on a common identifier which associates transaction records with corresponding block records.\n2. Include only records where the transaction destination address is specified.\n3. Consider only blocks that are larger than 4096 bytes.\n4. Restrict the data to records timestamped after \u20182023-01-01 00:00:00\u2019 for both transactions and blocks.\n5. For each transaction record, construct a URL using a predefined format concatenated with the source address from the transaction.\n6. Group the data by the source address of the transactions.\n7. Count the number of transactions for each unique source address and sort these counts in descending order to find the most frequent source address.\n8. Return the link to the source address with the highest count of transactions.\n",
        "special_function": [
            "string-functions/CONCAT",
            "timestamp-functions/TIMESTAMP"
        ]
    },
    {
        "instance_id": "bq320",
        "db": "bigquery-public-data.idc_v11",
        "question": "What is the total count of StudyInstanceUIDs that have a segmented property type of '80891009' and belong to the 'Community' or 'nsclc_radiomics' collections?",
        "external_knowledge": null,
        "plan": "1. **Identify the Main Query Objective:**\n   - The main goal is to count unique instances based on specific criteria involving properties and collections.\n\n2. **Specify the Conditions for Segmented Property Type:**\n   - Check for instances where a particular property type matches the given code, ignoring case sensitivity.\n\n3. **Specify the Conditions for Collection Membership:**\n   - Check for instances that belong to either of the specified collections.\n\n4. **Combine the Conditions Using Nested Queries:**\n   - Use nested queries to ensure that only instances meeting both conditions are considered.\n     - First, create a subquery to filter instances by the segmented property type.\n     - Second, create another subquery to filter instances by the collection membership.\n     - Use an intersection operation to find instances that satisfy both subquery conditions.\n\n5. **Aggregate and Count the Results:**\n   - Count the total number of unique instances from the combined results of the nested subqueries.\n\n6. **Return the Total Count:**\n   - The final output is the count of instances meeting all specified conditions.",
        "special_function": [
            "string-functions/LOWER"
        ]
    },
    {
        "instance_id": "bq321",
        "db": "bigquery-public-data.idc_v14",
        "question": "How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Records**:\n   - Create a temporary dataset that includes unique identifiers from records that match specific criteria within a given collection.\n   - Filter these records based on a list of specified descriptions.\n\n2. **Additional Filtering with Nested Data**:\n   - Create another temporary dataset that includes unique identifiers from records that match a different specific description within the same collection.\n   - Additionally, this step involves expanding nested data fields to match the criteria.\n\n3. **Combine Results**:\n   - Combine the unique identifiers from both temporary datasets into a single dataset.\n   - Use a union operation to ensure all relevant records are included.\n\n4. **Count Unique Identifiers**:\n   - Count the number of unique identifiers in the combined dataset to determine the total number of unique instances that meet the criteria.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "bq322",
        "db": "bigquery-public-data.idc_v15",
        "question": "What is the most common modality in the 'Community' and 'nsclc_radiomics' collections?",
        "external_knowledge": null,
        "plan": "1. **Define a Common Subquery**: Create a subquery to count the occurrences of each type of modality within the specified collections.\n\n2. **Identify Relevant Studies**: Within the subquery, filter the dataset to include only the studies that belong to the specified collections.\n\n3. **Group and Count Modalities**: Group the filtered data by modality type and count the number of occurrences for each modality.\n\n4. **Sort and Select the Most Common Modality**: In the main query, order the results by the frequency of occurrences in descending order and select the top result to find the most common modality.\n\nBy following these steps, the query efficiently identifies the most common modality within the specified collections.",
        "special_function": null
    },
    {
        "instance_id": "bq227_1",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Could you provide the annual percentage share of the total crime committed by the top 5 minor crime categories in London year by year?",
        "external_knowledge": null,
        "plan": "1. Aggregate the total values for each category, year, and month combination.\n2. Identify the top categories based on aggregated values. \n3. Rank categories by their total and classify the top five categories.\n4. Summarize the values for the categorized data (top five versus others) for each year, without distinguishing by month or specific category anymore.\n5. Aggregate these sums over each year to compute the total annual value for both the top categories and others.\n6. For each year, calculate the percentage share of the total value that the top categories represent by dividing the total value of top categories by the total annual value and multiplying by 100 to convert this fraction into a percentage.\n7. From the results, filter out the data to only show the percentage values for the top categories across each year.\n8. Finally, order the resulting data by year.\n",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq227_2",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Could you provide the total number of 'Other Theft' incidents within the 'Theft and Handling' category for each year in the Westminster borough?",
        "external_knowledge": null,
        "plan": "1. Group data by year, month, borough, and crime categories.\n2. Sum the values in each group to get a total count of incidents per group.\n3. Categorize the crime data into two divisions for the major crime category. If the major category is specified as 'Theft and Handling', label it as such; otherwise, label it as 'Other'.\n4. Similarly, categorize the minor crime category. If the minor category matches 'Other Theft', keep the category name; otherwise, label it as 'Other'.\n5. Filter the results to focus only on the borough 'Westminster' and exclude the 'Other' categories for both major and minor divisions.\n6. Calculate the total incidents per year by summing up the totals from the filtered results. \n7. Group these results by year, major division, and minor division.\n8. Finally, order the summarized data by year.",
        "special_function": [
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "bq228",
        "db": "bigquery-public-data.london_crime\nbigquery-public-data.london_fire_brigade",
        "question": "Please provide a list of the top three major crime categories in the borough of Barking and Dagenham, along with the number of incidents in each category.",
        "external_knowledge": null,
        "plan": "1. Calculate and rank the sum of crime incidents by major category within each borough.\n2. Group the data by both borough and major category to aggregate the total number of crime incidents for each category within each borough.\n3. Apply a ranking function to order the crime categories within each borough based on the aggregated sum of incidents, in descending order. \n4. Retrieve the borough, major category, ranking, and total number of incidents.\n5. Include only those records where the rank is within the top 3 for each borough.\n6. Further filter to include only records pertaining to Barking and Dagenham.\n7. Sort the final output first by borough and then by the ranking within that borough to show the top crime categories orderly.\n8. Display the major crime categories with the most incidents in the specified borough.\n",
        "special_function": [
            "numbering-functions/RANK"
        ]
    },
    {
        "instance_id": "bq229",
        "db": "bigquery-public-data.open_images",
        "question": "Can you provide a count of how many image URLs are categorized as \u2018cat\u2019 (with label '/m/01yrx' and full confidence) and how many contain no such cat labels(categorized as \u2018other\u2019) at all? ",
        "external_knowledge": null,
        "plan": "1. Join two tables to combine information about images and their associated labels. This includes the image's URL, label name, and a confidence score for each label.\n2, Filter and select distinct image URLs with label_name '/m/01yrx' where the confidence is 100%.\n3. Identify images that have any other labels with 100% confidence.\n4. Combine these two sets using a UNION ALL, where the first set gets labeled with its label and the second set gets labeled as 'other'.\n5. From the combined and categorized URLs, group by the label.\n6. Count the number of URLs associated with each label group.\n7. Return the count and label type.\n",
        "special_function": null
    },
    {
        "instance_id": "bq325",
        "db": "bigquery-public-data.open_targets_genetics",
        "question": "What are the top 10 genes with the lowest p-values across studies?",
        "temporal": "Yes",
        "external_knowledge": null,
        "plan": "1. **Create a Common Table Expression (CTE)**: Define a temporary result set to hold intermediate results for easier querying and readability.\n\n2. **Select Required Columns**: Within the CTE, select the necessary columns including study identifiers, gene names, and p-values.\n\n3. **Join Tables**: Perform the necessary joins between the tables to combine information about gene associations, study metadata, gene details, and variant details. Ensure the joins are based on appropriate matching columns.\n\n4. **Rank Genes**: Use the window function to assign a unique row number to each gene within each study, ordered by p-value in ascending order. This ranking helps identify the gene with the lowest p-value within each study.\n\n5. **Filter Top Genes per Study**: In the main query, filter the results to include only the top-ranked gene (i.e., the gene with the lowest p-value) for each study.\n\n6. **Order and Limit Results**: Sort the filtered genes by their p-values in ascending order to identify the genes with the lowest p-values across all studies.\n\n7. **Return Top Results**: Limit the final output to the top 10 genes with the lowest p-values, fulfilling the requirement to find the top 10 genes with the lowest p-values across studies.",
        "special_function": [
            "numbering-functions/ROW_NUMBER"
        ]
    },
    {
        "instance_id": "bq230",
        "db": "bigquery-public-data.usda_nass_agriculture",
        "question": "Can you provide the maximum amount of corn produced (measured in bushels) by each state in the year 2018, listing the results by state name?",
        "external_knowledge": null,
        "plan": "1. Select and group data by state name, commodity description, and year.\n2. Filter the data to include only records for crop group 'FIELD CROPS', data type like 'PRODUCTION', at the state level, where the measured value is not null and the units are specified as bushels.\n3. Calculate the total amount of the commodity produced per state, per commodity, per year.\n4. Filter the entries for 2018 and 'CORN'.\n5. For each state, find the maximum total produce amount recorded for that commodity.\n6. Display the state name and the maximum total produce amount for the specified commodity and year.\n7. Order the results by state name to present the data in a sorted manner by geographical location.\n",
        "special_function": [
            "timestamp-functions/TIMESTAMP_TRUNC"
        ]
    },
    {
        "instance_id": "bq231",
        "db": "bigquery-public-data.usda_nass_agriculture",
        "question": "Which state produced the most mushrooms in the year 2022 according to the horticulture production statistics?",
        "external_knowledge": null,
        "plan": "1. Apply a filter to only include records that belong to group horticulture, type production, at the state level aggregation, and for commodity mushrooms. Additionally, records with null values in the value field are excluded.\n2. Sum up the values grouped by state, commodity, and year to get the total production for mushrooms per state per year.\n3. Filter to include only the data from 2022.\n4. For each state, find the maximum total production value of mushrooms.\n5. Select the state name with the highest production value of mushrooms and return.\n",
        "special_function": [
            "timestamp-functions/TIMESTAMP_TRUNC"
        ]
    },
    {
        "instance_id": "bq326",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "How many countries experienced both a change in population and a change in Current health expenditure per capita, PPP (current international $) of greater than 1 in the year 2018?",
        "external_knowledge": null,
        "plan": "1. **Unpivot Yearly Data**:\n   - Transpose the yearly data columns into rows to create a more manageable format for further analysis.\n\n2. **Filter and Format Data**:\n   - Select relevant columns and filter the data for years starting from 2010. Convert the year and population values into appropriate data types.\n\n3. **Calculate Previous Population**:\n   - Use a window function to calculate the previous year's population for each country to enable the calculation of population changes.\n\n4. **Calculate Population Change for 2018**:\n   - Calculate the change in population for the year 2018 by comparing it to the previous year's population. Handle potential division by zero errors gracefully.\n\n5. **Filter Health Expenditure Data**:\n   - Select and filter the health expenditure per capita data for the specific indicator of interest. Use a window function to calculate the previous year's expenditure.\n\n6. **Calculate Health Expenditure Change for 2018**:\n   - Calculate the change in health expenditure per capita for the year 2018 by comparing it to the previous year's expenditure. Handle potential division by zero errors gracefully.\n\n7. **Join and Filter Results**:\n   - Join the population change data and the health expenditure change data on the country identifier. Filter the joined data to find countries that experienced both a significant population change and a significant health expenditure change in 2018.\n\n8. **Count the Countries**:\n   - Count the number of countries that meet the criteria of having both changes greater than 1 in the specified year.",
        "special_function": [
            "conversion-functions/CAST",
            "debugging-functions/ERROR",
            "mathematical-functions/ROUND",
            "navigation-functions/LAG",
            "string-functions/RIGHT",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/NULLIF",
            "other-functions/UNNEST",
            "other-functions/UNPIVOT"
        ]
    },
    {
        "instance_id": "bq327",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "How many debt indicators for Russia have a value of 0, excluding NULL values?",
        "external_knowledge": null,
        "plan": "1. **Create a Subquery to Filter Countries with Regions:**\n   - Construct a subquery to select the country codes and regions from a summary table.\n   - Filter out rows where the region is empty to exclude aggregated countries that do not have a specific region.\n\n2. **Create a Subquery for Russia's Debt Indicators:**\n   - Construct another subquery to select the country code, country name, debt value, and indicator name from an international debt table.\n   - Filter this subquery to include only rows where the country code matches the code for Russia.\n\n3. **Join the Two Subqueries:**\n   - Perform an inner join between the two subqueries on the country code to combine the data, ensuring that only records with matching country codes from both subqueries are included.\n\n4. **Filter Out NULL Values:**\n   - Further refine the joined dataset by excluding rows where the debt value is NULL.\n\n5. **Define a Common Table Expression (CTE):**\n   - Use the results of the above steps to create a CTE that holds the filtered and joined data for Russia's debt indicators, ensuring that each combination of country name, value, and indicator name is distinct.\n\n6. **Count Debt Indicators with a Value of Zero:**\n   - Query the CTE to count the number of debt indicators where the value is exactly zero.\n   - Return this count as the final result.",
        "special_function": [
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "bq328",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "Which region has the highest median GDP (constant 2015 US$) value?",
        "external_knowledge": null,
        "plan": "1. **Define Country Data**: Create a temporary dataset that includes key descriptive data for countries such as their codes, names, regions, and income groups.\n  \n2. **Filter GDP Data**: Create another temporary dataset that joins the country data with a larger dataset containing various indicators. Focus specifically on GDP values, ensuring that only relevant records with specific GDP indicators and non-null region and income group information are included.\n\n3. **Calculate Median GDP**: For each region, compute the median GDP value using an approximation function to handle large datasets efficiently. Group the results by region to ensure each region's median GDP is calculated correctly.\n\n4. **Identify Region with Highest Median GDP**: From the dataset containing median GDP values per region, sort the regions by their median GDP in descending order and select the top region, which represents the region with the highest median GDP value.\n\nBy following these steps, the original user intention to find the region with the highest median GDP value is achieved systematically and efficiently.",
        "special_function": [
            "approximate-aggregate-functions/APPROX_QUANTILES",
            "other-functions/ARRAY_SUBSCRIPT",
            "other-functions/STRUCT_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "bq370",
        "db": "spider2-public-data.wide_world_importers",
        "question": "How many customers have an equal number of orders and invoices, and where the total value of their orders matches the total value of their invoices?",
        "external_knowledge": null,
        "plan": "1. **Data Preparation - Aggregating Orders and Invoices**:\n    - Combine data from the order and invoice tables, joining them based on common identifiers.\n    - For each order line, calculate the cost by multiplying quantity and unit price.\n    - For each invoice line, similarly calculate the invoice value.\n\n2. **First-Level Aggregation**:\n    - Group by individual order and invoice details along with customer identifiers.\n    - Calculate the total number of orders, total order value, total number of invoices, and total invoice value for each grouping.\n    \n3. **Intermediate Result - Customer Order Data**:\n    - Create a dataset that contains these aggregated metrics for each unique combination of order, invoice, and customer.\n\n4. **Second-Level Aggregation - Customer Summary**:\n    - Group the intermediate results by customer identifier.\n    - Summarize the total number of orders, total number of invoices, total order value, and total invoice value for each customer.\n    - Compute the absolute difference between the summed order values and the summed invoice values.\n\n5. **Final Filtering and Counting**:\n    - From the customer summary, filter out customers where the total number of orders equals the total number of invoices, and the total value of orders matches the total value of invoices.\n    - Count the number of such customers who meet these criteria.\n\n6. **Output the Result**:\n    - Return the count of customers who have an equal number of orders and invoices, with matching total values. This gives the final count of customers satisfying the conditions.",
        "special_function": null
    },
    {
        "instance_id": "bq371",
        "db": "spider2-public-data.wide_world_importers",
        "question": "What is the difference between the maximum and minimum average invoice values across the quarters in the year 2013?",
        "external_knowledge": null,
        "plan": "1. **Define the Data Subset for Analysis**:\n   - Create a temporary dataset that includes customer names, total invoice values, and the corresponding quarter for each invoice within the year 2013.\n\n2. **Calculate Invoice Values**:\n   - Compute the total value for each invoice by multiplying the unit price by the quantity. If these values are missing, treat them as zero.\n\n3. **Determine Quarter for Each Invoice**:\n   - Extract the quarter from each invoice date and assign a label (Q1, Q2, Q3, Q4) based on the quarter number.\n\n4. **Filter by Year**:\n   - Ensure that only invoices from the year 2013 are included in the dataset.\n\n5. **Compute Average Invoice Values Per Quarter**:\n   - Group the data by the quarter and calculate the average invoice value for each quarter.\n\n6. **Find Maximum and Minimum Averages**:\n   - From the quarterly average invoice values, determine the maximum and minimum values.\n\n7. **Calculate the Difference**:\n   - Compute the difference between the maximum and minimum average invoice values.\n\n8. **Return the Result**:\n   - Output the calculated difference as the final result.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/EXTRACT",
            "datetime-functions/EXTRACT",
            "interval-functions/EXTRACT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE"
        ]
    },
    {
        "instance_id": "bq372",
        "db": "spider2-public-data.wide_world_importers",
        "question": "Which customer category has the maximum order loss closest to the average maximum loss across all categories?",
        "external_knowledge": null,
        "plan": "1. **Calculate Maximum Loss per Category:**\n    - Identify the maximum order value lost for each customer category.\n    - For each customer, compute the total order value lost by summing up the product of quantity and unit price of order lines for orders that were not invoiced.\n    - Group the results by customer category and customer, then find the maximum loss for each category by grouping the results again by customer category.\n\n2. **Compute Average Maximum Loss:**\n    - Calculate the average value of the maximum losses obtained for each customer category in the previous step.\n\n3. **Identify Closest Category to Average:**\n    - For each customer category, determine the absolute difference between its maximum loss and the calculated average maximum loss.\n    - Sort the results based on the absolute difference in ascending order and select the top result.\n\n4. **Select Final Category:**\n    - Extract the customer category name that has the maximum loss closest to the average maximum loss from the sorted results.",
        "special_function": [
            "mathematical-functions/ABS"
        ]
    },
    {
        "instance_id": "bq373",
        "db": "spider2-public-data.wide_world_importers",
        "question": "What's the median of the average monthly spending across all customers for the year 2014?",
        "external_knowledge": null,
        "plan": "1. **Calculate Monthly Totals for Each Customer:**\n   - Select customer names and the total invoice values for each month of the year 2014.\n   - Use conditional aggregation to sum invoice values for each month (January to December).\n   - Handle missing values with a default of 0.00.\n\n2. **Compute Average Monthly Spending:**\n   - For each customer, compute the average monthly spending by averaging the sums of the monthly totals.\n   - This results in a single average monthly spending value for each customer.\n\n3. **Determine the Median of Average Monthly Spendings:**\n   - Use a window function to calculate the median of the average monthly spending values obtained in the previous step.\n   - The `PERCENTILE_CONT` function is used to find the 50th percentile (median).\n\n4. **Retrieve the Median Value:**\n   - Select the distinct median value from the results.\n\nThis process ensures that the final output is the median of the average monthly spending across all customers for the specified year.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/EXTRACT",
            "date-functions/FORMAT_DATE",
            "datetime-functions/EXTRACT",
            "differentially-private-aggregate-functions/PERCENTILE_CONT",
            "interval-functions/EXTRACT",
            "navigation-functions/PERCENTILE_CONT",
            "time-functions/EXTRACT",
            "timestamp-functions/EXTRACT",
            "timestamp-functions/TIMESTAMP",
            "conditional-functions/CASE",
            "conditional-functions/COALESCE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "bq398",
        "db": "bigquery-public-data.world_bank_intl_debt\nbigquery-public-data.world_bank_wdi\nbigquery-public-data.world_bank_intl_education\nbigquery-public-data.world_bank_health_population\nbigquery-public-data.world_bank_global_population",
        "question": "What are the top three debt indicators for Russia based on the highest debt values?",
        "external_knowledge": null,
        "plan": "1. **Filter for Specific Region**: \n   - Start by selecting countries that have a specified region to exclude aggregated country data that lacks regional information.\n\n2. **Filter for Specific Country**:\n   - From the international debt data, select records specifically related to the country of interest (in this case, Russia).\n\n3. **Join Data**:\n   - Combine the filtered country data with the filtered international debt data based on matching country identifiers to get comprehensive records for the country of interest.\n\n4. **Filter Non-Null Values**:\n   - Ensure that only records with non-null debt values are considered, as null values would not contribute to identifying top debt indicators.\n\n5. **Sort by Debt Value**:\n   - Sort the resulting records in descending order based on the debt values to prioritize higher debt amounts.\n\n6. **Select Relevant Information**:\n   - From the sorted data, select the indicators that correspond to the top debt values.\n\n7. **Limit Results**:\n   - Restrict the final output to the top three records to meet the requirement of identifying the top three debt indicators.",
        "special_function": [
            "string-functions/FORMAT"
        ]
    },
    {
        "instance_id": "ga001",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?\nCan you tell me what other product are most frequently bought together with the Google Navy Speckled Tee by customers in December 2020?",
        "external_knowledge": null,
        "plan": "1. Focus on the item named \"Google Navy Speckled Tee.\"\n2. Select all purchase-type events from December 2020.\n3. Extract the IDs of individuals who purchased the \"Google Navy Speckled Tee\" during these events.\n4. Calculate all items purchased by these IDs and retain the top 10 items by purchase volume.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga001_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Tell me the most purchased other products and their quantities by customers who bought the Google Red Speckled Tee each month for the three months starting from November 2020.",
        "external_knowledge": null,
        "plan": "1. **Define Parameters:**\n   - Establish a parameter for the selected product, which in this case is 'Google Red Speckled Tee'.\n\n2. **Set Date Ranges:**\n   - Define the date ranges for the three consecutive months starting from November 2020.\n\n3. **Identify Purchase Events:**\n   - Extract purchase events from the dataset for the defined date ranges.\n   - Ensure to include the event period, user identifier, and items purchased.\n\n4. **Identify Buyers of Selected Product:**\n   - Identify distinct users who purchased the selected product within each period.\n   - This step filters out only those users who bought the 'Google Red Speckled Tee'.\n\n5. **Calculate Top Products Purchased by These Buyers:**\n   - For each period, calculate the total quantity of other products bought by the users identified in the previous step.\n   - Exclude the selected product from this count.\n\n6. **Rank Products by Quantity:**\n   - Rank the products for each period based on the total quantity purchased.\n   - Use a ranking function to order products in descending order of quantity.\n\n7. **Select Top Product Per Period:**\n   - Select the top-ranked product for each period.\n   - This ensures only the most purchased product (excluding the selected product) is chosen for each period.\n\n8. **Output Results:**\n   - Retrieve and display the period, product name, and quantity of the top purchased product for each period.\n   - Order the results by period to maintain chronological order.\n\nThis plan effectively breaks down the SQL query into logical steps to achieve the user\u2019s goal of identifying the most purchased other products by customers who bought a specific product within a given timeframe.",
        "special_function": [
            "numbering-functions/RANK",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga003",
        "db": "firebase-public-project.analytics_153293282",
        "question": "I'm trying to evaluate which board types were most effective on September 15, 2018. Can you find out the average scores for each board type from the quick play mode completions on that day?\"",
        "external_knowledge": null,
        "plan": "1. Extract all data for the \"level_complete_quickplay\" mode.\n2. Focus on the board type, grouping by user ID and event timestamp to summarize board type and corresponding scores.\n3. Calculate the average score for each board type.",
        "special_function": [
            "aggregate-functions/ANY_VALUE",
            "conditional-functions/IF",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga004",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you figure out the average difference in pageviews between users who bought something and those who didn\u2019t in December 2020? Just label anyone who was involved in purchase events as a purchaser.",
        "external_knowledge": null,
        "plan": "1. Segment user activities into page views and purchase events for December 2020.\n2. Classify users based on whether they made any purchases.\n3. Calculate average page views for purchasers and non-purchasers.\n4. Determine the difference in average page views between these two groups.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga004_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me the average page views per buyer and total page views for each day in November 2020?",
        "external_knowledge": null,
        "plan": "1. **Define a Temporary Data Set:**\n   - Create a temporary data set to collect user activity information.\n   - Parse the date format to a standard date type.\n   - Count the occurrences of 'page_view' events for each user per day.\n   - Count the occurrences of 'purchase' related events for each user per day.\n   - Filter the data for the specified date range (November 2020).\n\n2. **Aggregate User Activity:**\n   - From the temporary data set, select the date, the sum of 'page_view' counts, and the average 'page_view' counts per user who made a purchase.\n   - Ensure to include only users who have made at least one purchase-related event.\n\n3. **Compute Metrics:**\n   - For each day in November 2020:\n     - Calculate the average number of 'page_view' events per purchasing user.\n     - Calculate the total number of 'page_view' events.\n\n4. **Group and Order Results:**\n   - Group the results by date to get daily metrics.\n   - Order the results by date to maintain chronological order.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "date-functions/PARSE_DATE"
        ]
    },
    {
        "instance_id": "ga017",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "How many distinct users viewed the most frequently visited page during January 2021?",
        "external_knowledge": null,
        "plan": "1. Extract `page_view` events for January 2021 and unnest their parameters to access individual event details.\n2. Aggregate these details to identify the title of each page viewed, grouping by user and event timestamp.\n3. Count occurrences and distinct users per page, then order by the frequency of visits to each page.\n4. Select the number of distinct users for the top visited page.",
        "special_function": [
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga007",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please find out what percentage of the page views on January 2, 2021, were for PDP type pages.",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Calculate the total proportion of PDP",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga007_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know all the pages visited by user 1402138.5184246691 on January 2, 2021. Please show the names of these pages and adjust the names to PDP or PLP where necessary.",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. **Create a Base Table**:\n   - Select necessary columns from the dataset.\n   - Filter the data to include only records for the specified date.\n   - Further filter to include only records for the specified user ID.\n   - Ensure that only records with the specified event type are selected.\n\n2. **Unnest Event Parameters**:\n   - Unnest the event parameters to access key-value pairs.\n   - Extract relevant values using conditional logic based on the key names.\n   - Group the data by date, timestamp, and user ID to ensure each event is uniquely identified.\n\n3. **Classify Page Names**:\n   - For each event, analyze the page location URL.\n   - Split the URL into its components for further analysis.\n   - Determine the type of page (PDP or PLP) based on predefined conditions:\n     - If the URL structure and specific keywords match certain patterns, classify the page as 'PDP'.\n     - If different patterns are matched, classify the page as 'PLP'.\n   - If none of the patterns are matched, retain the original page title.\n\n4. **Final Output**:\n   - Select all relevant fields from the unnested events.\n   - Include an additional field that contains the adjusted page title based on the classification logic.",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga018",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Calculate the conversion rate from product list pages to product detail pages for all sessions at January 2nd, 2021.",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. query the event data to retrieve all unique event names\n2. Selects events data from the Google Analytics 4 (GA4) sample e-commerce dataset for the specific date (20210102)\n3. Filter to include only events named 'page_view', which represent page views.\n4. flatten the nested event_params array and extract values for ga_session_id, ga_session_number, page_title, and page_location. This allows the analysis of individual page views within each user's session.\n5. Further processes the unnested event data to classify pages based on URL depth and specific keywords into either Product Detail Pages (PDP) or Product Listing Pages (PLP).\n6. Applies window functions to the categorized data to calculate the previous and next pages for each session per user, facilitating analysis of navigation paths between pages.\n7. Filters sessions where the current page is a PLP and the next page is a PDP.\n8. Counts the number of sessions transitioning from PLP to PDP and divides this by the total views of PLP pages to calculate the conversion rate.",
        "special_function": [
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "date-functions/DATE",
            "navigation-functions/LAG",
            "navigation-functions/LEAD",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga031",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know our user conversion rate for January 2nd, 2021, calculated as the ratio of users who reached the Checkout Confirmation page to those who landed on the Home page.",
        "external_knowledge": null,
        "plan": "1. Extract and prepare event data from the Google Analytics 4 (GA4) dataset, specifically for page views on January 2, 2021.\n2. Unnest the event parameters to retrieve values for session identifiers and page identifiers, such as page title and location.\n3. Identify and count the initial visits to the Home page and the successful visits to the Checkout Confirmation page.\n4. Join the data to match sessions that start on the Home page and end at the Checkout Confirmation page.\n5. Calculate the user conversion rate for January by comparing the number of successful checkout visits to the total home page visits.",
        "special_function": [
            "date-functions/DATE",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga032",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you pull up the sequence of pages our customer 1362228 visited on January 28th 2021, linking them with '>>' between each page? I want to see their navigation flow through our site. Please merge adjacent identical pages into one, and refer to the docs to convert the corresponding pages to PDP, PLP if necessary.",
        "external_knowledge": "ga4_page_category.md",
        "plan": "1. Select and filter page_view events on January 28th, 2021, specifically for user 1362228 from the GA4 dataset.\n2. Unpack the event_params array to extract details such as session ID, session number, page title, and page location.\n3. Determine page categories based on URL structure and keywords to classify pages as either Product Detail Pages (PDPs), Product Listing Pages (PLPs), or other types.\n4. Rank pages based on the event timestamp and use window functions to establish the sequence of page visits within each session.\n5. concatenate the page titles visited in sequence, separated by '>>', representing the user's navigation flow through the site.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "array-functions/ARRAY",
            "array-functions/ARRAY_LENGTH",
            "array-functions/ARRAY_REVERSE",
            "array-functions/ARRAY_TO_STRING",
            "date-functions/DATE",
            "mathematical-functions/POWER",
            "navigation-functions/LAG",
            "navigation-functions/LEAD",
            "numbering-functions/DENSE_RANK",
            "string-functions/CONTAINS_SUBSTR",
            "string-functions/LOWER",
            "string-functions/SPLIT",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga006",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Provide the IDs and the average spending per session for users who were engaged in multiple purchase sessions in November 2020",
        "external_knowledge": null,
        "plan": "1. **Define Events Data**:\n   - Create a temporary table to store event data.\n   - Extract session ID and spending value from event parameters.\n   - Include all columns from the original event data.\n   - Filter the event data to include only records from November 2020.\n\n2. **Calculate Session Info**:\n   - Create another temporary table to calculate session-related metrics for each user.\n   - Count the distinct session IDs for each user to determine the number of purchase sessions.\n   - Calculate the average spending per session by dividing the total spending by the number of distinct sessions for each user.\n   - Only include events that represent a purchase and have a valid session ID.\n\n3. **Filter and Select**:\n   - Select the user IDs and their corresponding average spending per session from the session info table.\n   - Include only users who have engaged in more than one purchase session.\n\nBy following these steps, the query effectively identifies users with multiple purchase sessions in the specified timeframe and calculates their average spending per session.",
        "special_function": [
            "date-functions/DATE",
            "range-functions/RANGE",
            "string-functions/REPLACE",
            "conditional-functions/COALESCE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga009",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "I want to know the average number of engaged sessions per user of December 2020.",
        "external_knowledge": null,
        "plan": "1. **Identify the Time Frame:**\n   - The query focuses on data from December 2020.\n   - Filter the dataset to include only records within this time period.\n\n2. **Engaged Sessions Calculation:**\n   - Determine the number of unique engaged sessions.\n   - An engaged session is identified by a specific event parameter indicating engagement.\n   - For each engaged session, concatenate a unique user identifier with a session identifier to ensure distinct session counts.\n\n3. **User Count Calculation:**\n   - Count the number of unique users within the specified time frame.\n   - Use a distinct user identifier to ensure each user is only counted once.\n\n4. **Calculate Averages:**\n   - Calculate the average number of engaged sessions per user.\n   - Divide the total number of engaged sessions by the total number of unique users.\n\n5. **Result Presentation:**\n   - Present the result as the average number of engaged sessions per user.",
        "special_function": [
            "conversion-functions/CAST",
            "json-functions/STRING",
            "string-functions/CONCAT",
            "timestamp-functions/STRING",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga010",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Can you give me an overview of our website traffic for December 2020? I'm particularly interested in the channel with the fourth highest number of sessions.",
        "external_knowledge": "ga4_dimensions_and_metrics.md",
        "plan": "1.First, read the document to understand how traffic is divided into 18 channel groups, primarily based on the metrics of source, medium, and campaign.\n2.Extract all visits from the database for December, each visit having a unique user ID and session ID. Retrieve the source, medium, and campaign for each visit.\n3.Based on the classification standards for channel groups in the document, write conditional statements to determine which channel each set of data belongs to, mainly using regular expressions. If the data source (source) contains any of the 4.following: 'badoo', 'facebook', 'fb', 'instagram', 'linkedin', 'pinterest', 'tiktok', 'twitter', or 'whatsapp', and the medium (medium) includes 'cp', 'ppc', or starts with 'paid', then categorize it as 'Paid Social'.\n5.Calculate the number of sessions for each channel based on the channel grouping.\n6.Select the name of the channel ranked fourth as the answer.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/SET",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga010_1",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Please tell me the number of sessions for each website traffic channel in December 2020.",
        "external_knowledge": "ga4_dimensions_and_metrics.md",
        "plan": "1. **Create a Preparation Step**: \n   - Define a subquery to prepare the necessary data.\n   - Select unique user identifiers.\n   - Extract session identifiers from event parameters.\n   - Aggregate source, medium, and campaign information for each session, ordering them by event timestamp to ensure correct sequence.\n\n2. **Filter for Specific Time Period**:\n   - Limit data to events occurring in December 2020 by specifying the date range.\n\n3. **Group Data by User and Session**:\n   - Group the prepared data by unique user and session identifiers to consolidate session information.\n\n4. **Classify Sessions into Channels**:\n   - Use a `CASE` statement to categorize each session into a traffic channel based on the source, medium, and campaign information.\n   - Define various conditions for channel grouping, such as direct traffic, cross-network campaigns, paid and organic shopping, search, social, video, email, affiliate, referral, audio, SMS, and mobile push notifications.\n   - Assign any sessions that do not match the predefined conditions to an \"Unassigned\" category.\n\n5. **Count Sessions per Channel**:\n   - Count the number of distinct sessions for each traffic channel by combining user and session identifiers.\n   - Group the results by the traffic channel to get the session count for each channel.\n\n6. **Return the Results**:\n   - Output the channel grouping and the corresponding session count for each channel.",
        "special_function": [
            "aggregate-functions/ARRAY_AGG",
            "aggregate-functions/GROUPING",
            "string-functions/CONCAT",
            "string-functions/REGEXP_CONTAINS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/SET",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga011",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "What is the highest number of page views for any page path in December 2020?",
        "external_knowledge": null,
        "plan": "1. **Subquery Preparation**: Start by identifying specific events and parameters from the dataset using a subquery. This subquery will extract certain key details related to page views for further analysis.\n\n2. **Extracting URL Path Components**: \n    - Use string manipulation functions to split and extract different segments of the URL path from the event parameters.\n    - Specifically, extract three different levels of the URL path:\n        - First level path segment\n        - Second level path segment\n        - Third level path segment\n    - Ensure to handle potential empty segments by checking and replacing them with `NULL` if necessary.\n\n3. **Concatenation of Path Components**: \n    - Concatenate the extracted path segments appropriately to form complete path levels.\n    - Ensure each path segment is prefixed with a '/' to maintain URL structure.\n\n4. **Filtering by Date Range**: \n    - Filter the dataset to only include records from December 2020.\n\n5. **Counting Page Views**: \n    - Group the results by the extracted path segments (pagepath levels).\n    - Count the occurrences of page view events for each group using a conditional count function.\n\n6. **Ordering and Limiting Results**: \n    - Order the grouped results by the count of page views in descending order to find the most viewed pages.\n    - Limit the final result to return only the highest count of page views.\n\n7. **Final Output**: \n    - Select and output the highest number of page views from the ordered results.",
        "special_function": [
            "aggregate-functions/COUNTIF",
            "string-functions/CONCAT",
            "string-functions/SPLIT",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "other-functions/UNNEST",
            "other-functions/ARRAY_SUBSCRIPT"
        ]
    },
    {
        "instance_id": "ga012",
        "db": "bigquery-public-data.ga4_obfuscated_sample_ecommerce",
        "question": "Find the transaction IDs, total item quantities, and purchase revenues for the item category with the highest tax rate on November 30, 2020.",
        "external_knowledge": null,
        "plan": "1. **Identify the Top Tax Rate Category:**\n    - Create a common table expression (CTE) to calculate the tax rate for each item category.\n    - Calculate the tax rate as the ratio of the sum of tax values to the sum of purchase revenues for each category.\n    - Select the item category with the highest tax rate by ordering the results in descending order of tax rate and limiting the result to one.\n\n2. **Select Required Transaction Details:**\n    - Using the main table and the CTE, retrieve transaction details for the item category with the highest tax rate.\n    - Filter the dataset to include only purchase events.\n    - Join the main table with the CTE on the item category to ensure only transactions from the top tax rate category are considered.\n    - Group the results by transaction ID.\n\n3. **Calculate Aggregates:**\n    - For each transaction, calculate the total item quantity by summing the quantities of items.\n    - Calculate the total purchase revenue in the specified currency by summing the purchase revenue fields.\n    - Ensure the results include transaction IDs, total item quantities, and purchase revenues.\n\n4. **Output the Results:**\n    - Return the transaction ID, the aggregated total item quantity, and the aggregated purchase revenue for transactions within the top tax rate category on the specified date.",
        "special_function": [
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "ga019",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Could you determine what percentage of users either did not uninstall our app within seven days or never uninstalled it after installing during August and September 2018?",
        "external_knowledge": null,
        "plan": "1. Extract distinct user IDs and their first open dates from events labeled as 'first_open' for the months of August and September 2020.\n2. Gather distinct user IDs and their app removal dates from events labeled 'app_remove' during the same timeframe.\n3. Join the installation data with the uninstallation data on user ID to calculate the number of days between app installation and removal.\n4. Determine the percentage of users who uninstalled the app within seven days of installation by comparing the number of users who uninstalled within this period to the total number of users who installed the app.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga030",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Can you group users by the week they first used the app starting from July 2, 2018 and show which group has the most active users remained in the next four weeks, with each group named by the Monday date of that week? Please answer in the format of \" YYYY-MM-DD\".",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range and Calculate Minimum Date**:\n    - Establish a specific start date and end date.\n    - Determine the minimum date by subtracting four weeks from the end date truncated to the start of the week.\n\n2. **Prepare Event Data**:\n    - Convert event dates to a standard date format.\n    - Identify whether a user is using the app for the first time on that date.\n\n3. **Extract New Users**:\n    - Create a list of users who are new, along with their first event date.\n\n4. **Calculate Days Since Start for Each User**:\n    - Determine the number of days since a user\u2019s first app use.\n    - Associate each event date with the respective user and their first event date.\n\n5. **Calculate Weeks Since Start**:\n    - Group users by the week they started using the app.\n    - Calculate the number of weeks since each user's initial use.\n\n6. **Count Retained Users by Week**:\n    - For each starting week, count the number of distinct users who are still active, grouped by the number of weeks since they started.\n\n7. **Find the Week with Most Retained Users After Four Weeks**:\n    - Identify the initial user cohorts for each week.\n    - Determine the number of users still active after four weeks for each cohort.\n    - Calculate the retention rate (users active after four weeks / initial users).\n    - Sort the results by retention rate in descending order.\n    - Select the week with the highest retention rate.\n\nThis step-by-step plan ensures that users are grouped by their initial week of usage, and the retention rates are calculated to find the week with the highest user retention after four weeks.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "ga030_1",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Can you group users by the week they first used the app, starting from July 9, 2018? I'd like to see the retention rate for each group over the next two weeks.",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range and Minimum Date**:\n   - Create a CTE (Common Table Expression) to define the start date, end date, and the minimum date (which is two weeks before the end date).\n\n2. **Create Date Table**:\n   - Extract the event date and user ID from the events table.\n   - Determine if the event date is the first time the user used the app by comparing it with the user's first touch timestamp.\n   - Filter events that occurred on or after July 9, 2018, and where the event name is 'session_start'.\n\n3. **Identify New Users**:\n   - Create a list of users who are new (i.e., their first use of the app) by selecting distinct user IDs and their first event dates.\n\n4. **Calculate Days Since Start for Each User**:\n   - Calculate the number of days since each new user started using the app.\n   - This involves joining the date table with the new user list to get the event date differences for each user's subsequent events.\n\n5. **Calculate Weeks Since Start for Each User**:\n   - Group the users into weekly cohorts based on their first event date.\n   - Calculate the number of weeks since the user first used the app, where the week starts on Monday.\n\n6. **Count Retained Users**:\n   - Count the number of distinct users retained in each weekly cohort for each week since they first used the app.\n   - Ensure the counts are within the defined date range.\n\n7. **Calculate Retention Rate**:\n   - Join the retention counts for the initial week cohort with the counts for the second week (two weeks since the start).\n   - Calculate the retention rate by dividing the number of users retained in the second week by the number of users in the initial cohort.\n   - Ensure to handle cases where no users are retained by using an IFNULL function.\n\n8. **Output and Order Results**:\n   - Select the initial week cohort and the calculated retention rate.\n   - Filter to include only the initial cohorts and order the results by the week cohort for clear presentation.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE",
            "conditional-functions/IFNULL"
        ]
    },
    {
        "instance_id": "ga028",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Using the 7-day retention calculation method, find the number of users who first used the app in the week starting on \"2018-07-02\" and tell me the number of retained users for each week from 0 to 4 weeks.",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define the Date Range**:\n   - Establish the start date and end date for the analysis period.\n   - Calculate the minimum date needed for the 4-week retention analysis.\n\n2. **Create Event Date Table**:\n   - Parse the event dates.\n   - Determine if a user is a new user by comparing the event date with the user's first touch timestamp.\n\n3. **Identify New Users**:\n   - Extract a list of new users who first used the app on their respective event dates.\n\n4. **Calculate Days Since First Use**:\n   - For each user, calculate the number of days since their first use.\n   - Associate each user with their event dates and the calculated days since their first use.\n\n5. **Group by Week and Calculate Retention**:\n   - Group the users by the week of their first use.\n   - Calculate the number of weeks since the user's first use.\n   - Use the ceiling function to group days into weeks, starting from 0.\n\n6. **Aggregate Retention Data**:\n   - Count the number of distinct retained users for each week since their first use.\n   - Group the data by the week cohort and the weeks since the start.\n   - Ensure that the data is limited to the defined date range and only includes up to 4 weeks.\n\n7. **Filter and Sort the Retention Data**:\n   - Filter the retention data to show only the week starting on the specified start date.\n   - Sort the data by the week cohort and the number of weeks since the start.\n\n8. **Select Final Output**:\n   - Select and display the number of retained users for each week from 0 to 4 weeks since the user's first use.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_ADD",
            "date-functions/DATE_DIFF",
            "date-functions/DATE_TRUNC",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "timestamp-functions/TIMESTAMP_MICROS",
            "conditional-functions/CASE_EXPR",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga020",
        "db": "firebase-public-project.analytics_153293282",
        "question": "Which quickplay event type had the lowest user retention rate during the second week after their initial engagement, for users who first engaged between August 1 and August 15, 2018?",
        "external_knowledge": "retention_rate.md",
        "plan": "1. Extract unique users who installed the app in September 2018, recording the date of first use.\n2. Collect data on users who uninstalled the app within a little over a month from installation, noting the uninstallation dates.\n3. Retrieve crash data for the same users during the specified timeframe to determine app stability issues.\n4. Combine the installation, uninstallation, and crash data into a single dataset using user ID as the key.\n5. Calculate the proportion of users who experienced a crash within a week of installation out of those who uninstalled the app within a week, providing insight into potential issues affecting user retention.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "range-functions/RANGE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga020_1",
        "db": "firebase-public-project.analytics_153293282",
        "question": "What is the retention rate for users two weeks after their initial quickplay event within the period from July 2, 2018, to July 16, 2018, calculated separately for each quickplay event type?",
        "external_knowledge": "retention_rate.md",
        "plan": "1. **Define Date Range:**\n   - Establish the start and end dates for the period of interest.\n\n2. **Identify Active Dates:**\n   - Select user identifiers and their active dates (based on session start events) within the defined date range.\n\n3. **Identify Initial Quickplay Events:**\n   - For each user, determine the earliest date of specific quickplay events within the defined date range.\n\n4. **Calculate Days Since Initial Event:**\n   - For each user and each initial quickplay event type, calculate the number of days since the initial quickplay event until each subsequent active date.\n\n5. **Determine Weeks Since Initial Event:**\n   - Convert the days since the initial quickplay event into weeks and count the number of active days in each week for each user and each quickplay event type.\n\n6. **Aggregate Weekly Retention Data:**\n   - Summarize the weekly retention data by counting the active days and the number of retained users for each quickplay event type and each week.\n\n7. **Calculate Retention Rate:**\n   - Calculate the retention rate by dividing the number of retained users in each week by the maximum number of retained users in the first week for each quickplay event type.\n\n8. **Filter for Two Weeks:**\n   - Select and return the retention rates for users two weeks after their initial quickplay event for each quickplay event type.",
        "special_function": [
            "conversion-functions/CAST",
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/PARSE_DATE",
            "mathematical-functions/CEIL",
            "range-functions/RANGE",
            "conditional-functions/CASE"
        ]
    },
    {
        "instance_id": "ga025",
        "db": "firebase-public-project.analytics_153293282",
        "question": "For all users who first opened the app in September 2018 and then uninstalled within seven days, I want to know what percentage of them experienced an app crash.",
        "external_knowledge": null,
        "plan": "1. Extract users who installed the app in September 2018.\n2. Extract users who uninstalled the app between September 1 and October 7, 2018.\n3. Extract users who experienced app crashes between September 1 and October 7, 2018.\n4. Combine these datasets to analyze the relationship between crashes and uninstallations.\n5. Calculate the percentage of users who uninstalled within a week and experienced crashes to determine the impact of app crashes on user retention.",
        "special_function": [
            "date-functions/DATE",
            "date-functions/DATE_DIFF",
            "date-functions/FORMAT_DATE",
            "date-functions/PARSE_DATE",
            "conditional-functions/CASE",
            "other-functions/UNNEST"
        ]
    },
    {
        "instance_id": "local002",
        "db": "E_commerce",
        "question": "Can you calculate the 5-day symmetric moving average of predicted toy sales for December 5, 6, and 7, 2018, using daily sales data from January 1, 2017, to August 29, 2018, with a simple linear regression model? Provide the total of the moving averages for those three days.",
        "external_knowledge": null,
        "plan": "Calculate the 5-day moving average of forecasted sales for December 2018 for toys category based on a linear regression model of daily sales data from January 1, 2017, to August 29, 2018.",
        "special_function": null
    },
    {
        "instance_id": "local003",
        "db": "E_commerce",
        "question": "How much is the average sales per order per customer in each RFM bucket, considering only 'delivered' orders?",
        "external_knowledge": "RFM.md",
        "plan": "Classify customers into RFM buckets based on recency, frequency, and monetary scores, then calculate and display group statistics, including average days since the last purchase, average sales per customer, and customer count for each RFM bucket.",
        "special_function": null
    },
    {
        "instance_id": "local004",
        "db": "E_commerce",
        "question": "Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order. Attention: I want the lifespan in float number if it's longer than one week, otherwise set it to be 1.0.",
        "external_knowledge": null,
        "plan": "Calculate each customer's order frequency (PF), average order value (AOV), and average customer lifespan (ACL) based on their order history, zip code prefix, and payment data.",
        "special_function": null
    },
    {
        "instance_id": "local007",
        "db": "Baseball",
        "question": "Could you help me calculate the average single career span value in years for all baseball players? Please precise the result as a float number.",
        "external_knowledge": null,
        "plan": "Calculate the career span of baseball players, including their debut and final game dates, by extracting the year, month, and day differences between the debut and final game dates, and combining these into a single career span value.",
        "special_function": null
    },
    {
        "instance_id": "local009",
        "db": "Airlines",
        "question": "What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?",
        "external_knowledge": "haversine_formula.md",
        "plan": "Calculate the average distance for each unique combination of from_city and to_city",
        "special_function": null
    },
    {
        "instance_id": "local010",
        "db": "Airlines",
        "question": "Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",
        "external_knowledge": "haversine_formula.md",
        "plan": "Create a histogram distribution of the average distances (in km) between city pairs, grouping the distances into ranges of 1000 km.",
        "special_function": null
    },
    {
        "instance_id": "local026",
        "db": "IPL",
        "question": "Please help me find the top 3 bowlers who conceded the maximum runs in a single over, along with the corresponding matches.",
        "external_knowledge": null,
        "plan": "1. Combine runs scored by player and extra runs given into a single dataset named combined_runs.\n2. Aggregate the combined runs by match, innings, and over to calculate the total runs scored in each over, creating the over_runs dataset.\n3. Identify the maximum runs scored in a single over for each match and store this information in the max_over_runs dataset.\n4. Retrieve the details of overs where the maximum runs were scored for each match by joining over_runs and max_over_runs, resulting in the top_overs dataset.\n5. Determine the bowlers who bowled the overs with the maximum runs by joining ball_by_ball and top_overs, and group by match, maximum runs scored, and bowler to create the top_bowlers dataset.\n6. Select the top 3 overs with the highest maximum runs scored, ordering by maximum runs in descending order.\n7. Join the top_bowlers dataset with the player table to get the player names for the bowlers.\n8. Output the match ID and player names of the top 3 bowlers, ordered by maximum runs scored in descending order, match ID, and player name.",
        "special_function": null
    },
    {
        "instance_id": "local026_1",
        "db": "IPL",
        "question": "Which bowler has the lowest bowling average per wicket taken?",
        "external_knowledge": null,
        "plan": "1. Join the ball_by_ball dataset with the wicket_taken, extra_runs, and batsman_scored datasets to identify wickets taken, extra runs given, and runs scored for each ball.\n2. Aggregate the data by bowler to calculate the total number of wickets taken and the total runs given.\n3. Calculate the bowling average for each bowler by dividing the total runs given by the number of wickets taken.\n4. Assign a rank to each bowler based on their bowling average, with the lowest average ranked first.\n5. Join the player table with the bowling_averages CTE to get the player names.\n6. Filter to select the player with the best (lowest) bowling average.",
        "special_function": null
    },
    {
        "instance_id": "local026_2",
        "db": "IPL",
        "question": "Could you show me the average total score of strikers who have scored more than 50 runs in at least one match?",
        "external_knowledge": null,
        "plan": "Get the total runs scored by players who have scored more than 50 runs in at least one match.\n List the names of all players and their runs scored (who have scored at least 50 runs in any match). Order result in decreasing order of runs scored. Resolve ties alphabetically. ",
        "special_function": null
    },
    {
        "instance_id": "local026_6",
        "db": "IPL",
        "question": "Please calculate the average of the highest runs conceded in a single over for each match.",
        "external_knowledge": null,
        "plan": "Get the bowler and their player name for each match who conceded the maximum runs in a single over, along with the match ID and the number of runs scored.\nFor each match id, list the maximum runs scored in any over and the bowler bowling in that over. If there is more than one over having maximum runs, return all of them and order them in increasing order of over id. Order results in increasing order of match ids. (<match id, maximum runs, player name>) [3 marks]",
        "special_function": null
    },
    {
        "instance_id": "local028_1",
        "db": "Brazilian_E_Commerce",
        "question": "What is the highest monthly delivered orders volume in the year with the lowest annual delivered orders volume among 2016, 2017, and 2018?",
        "external_knowledge": null,
        "plan": "1. **Identify Monthly Order Counts**:\n   - Extract the year and month from the order delivery date.\n   - Count the total number of delivered orders for each month.\n   - Consider only delivered orders with a non-null delivery date.\n\n2. **Calculate Annual Order Counts**:\n   - Sum the monthly order counts to get the total number of orders for each year.\n   - Focus only on the years 2016, 2017, and 2018.\n\n3. **Determine the Year with the Lowest Annual Order Count**:\n   - From the calculated annual order counts, find the year with the smallest total number of orders.\n   - Select this year for further analysis.\n\n4. **Find the Highest Monthly Order Count in the Selected Year**:\n   - Within the year identified in the previous step, find the maximum monthly order count.\n   - Return this maximum value as the highest monthly delivered orders volume for the year with the lowest annual delivered orders volume.",
        "special_function": null
    },
    {
        "instance_id": "local029",
        "db": "Brazilian_E_Commerce",
        "question": "Please calculate the average payment value, city, and state for the top 3 customers with the most delivered orders.",
        "external_knowledge": null,
        "plan": "Calculate the total number of orders, average payment value, city, and state for each unique customer who had their orders delivered.",
        "special_function": null
    },
    {
        "instance_id": "local030",
        "db": "Brazilian_E_Commerce",
        "question": "Can you find the average payments and order counts for the five cities with the lowest total payments from delivered orders?",
        "external_knowledge": null,
        "plan": "Retrieve the top 5 cities and states with the highest total customer payments, including the total number of orders and total payments by customers, for orders that have been delivered.",
        "special_function": null
    },
    {
        "instance_id": "local038",
        "db": "Pagila",
        "question": "Could you help me find the actor who appeared most in English G or PG-rated children's movies no longer than 2 hours, released between 2000 and 2010\uff1fGive me a full name.",
        "external_knowledge": null,
        "plan": "Display the top 3 actors who have most appeared in films in the \u201cChildren\u201d category. If several actors have the same number of films, output all",
        "special_function": null
    },
    {
        "instance_id": "local039",
        "db": "Pagila",
        "question": "Please help me find the film category with the highest total rental hours in cities where the city's name either starts with \"A\" or contains a hyphen. ",
        "external_knowledge": null,
        "plan": "Output the category of movies that has the largest number of hours of total rental in cities (customer.address_id in this city), and that starts with the letter \u201ca\u201d. Do the same for cities in which there is a \u201c-\u201d symbol. Write everything in one request",
        "special_function": null
    },
    {
        "instance_id": "local041",
        "db": "tree_and_income",
        "question": "What percentage of trees in the Bronx have a health status of Good?",
        "external_knowledge": null,
        "plan": "1. **Filter Data by Location and Health Status:**\n   - Create a temporary dataset that includes only the trees located in the specified area with a non-null and non-empty health status.\n\n2. **Count Trees by Health Status:**\n   - Within the temporary dataset, group the trees by their health status and count the number of trees in each group.\n\n3. **Count Total Trees in the Area:**\n   - Create another temporary dataset that counts the total number of trees in the specified area, without considering their health status.\n\n4. **Calculate the Percentage:**\n   - Select the count of trees with a specific health status from the first temporary dataset.\n   - Select the total count of trees from the second temporary dataset.\n   - Calculate the percentage by dividing the count of trees with the specific health status by the total count of trees and multiplying by 100.\n\n5. **Round the Result:**\n   - Round the calculated percentage to two decimal places for a more readable output.\n\n6. **Return the Result:**\n   - Output the rounded percentage as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local053_1",
        "db": "unicorn_companies",
        "question": "Can you help me calculate the average number of new unicorn companies per year in the top industry from 2019 to 2021?",
        "external_knowledge": null,
        "plan": "Determine which industries have the highest average valuations from 2019 to 2021, and tell me what new unicorns are in these industries?",
        "special_function": null
    },
    {
        "instance_id": "local054_2",
        "db": "chinook",
        "question": "What is the difference in average spending between customers who bought albums from the best-selling artist and those who bought from the least-selling artist? If there is a tie for either best-selling or lowest-selling, choose the artist whose name comes first alphabetically.",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local059",
        "db": "atliq",
        "question": "What is the average quantity sold for the top 3 products in each division in 2021? I want the answer in descending order of the average quantity.",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Data:** \n   - Filter the sales records to include only those from the year 2021.\n\n2. **Aggregate Sales Data:** \n   - Group the filtered records by division and product, and calculate the total quantity sold for each product within each division.\n\n3. **Rank Products by Sales:** \n   - For each division, rank the products based on their total quantity sold in descending order.\n\n4. **Filter Top Products:** \n   - Select the top 3 products from each division based on their rank.\n\n5. **Calculate Averages:** \n   - For each division, compute the average quantity sold for the top 3 products.\n\n6. **Order Results:** \n   - Sort the divisions by the calculated average quantity sold in descending order to meet the user\u2019s requirement of listing the results by average quantity.\n\nBy following these steps, the query efficiently answers the original user intention by accurately computing and displaying the required average quantities in the specified order.",
        "special_function": null
    },
    {
        "instance_id": "local062_2",
        "db": "complex_oracle",
        "question": "Could you group customers in Italy into 10 buckets based on their monthly profitability in December 2021 and calculate the average from minimum profits of each bucket?",
        "external_knowledge": null,
        "plan": "1. **Calculate Profit for Each Transaction:**\n   - Join the sales and cost data to compute the profit for each transaction by multiplying the quantity sold by the difference between unit price and unit cost.\n   - Filter the transactions to include only those from customers in the specified country and for the specified month.\n\n2. **Aggregate Monthly Profit by Customer:**\n   - Sum the profit for each customer across all their transactions to get the total monthly profit for each customer.\n\n3. **Determine the Range of Profits:**\n   - Calculate the minimum and maximum profit values across all customers to establish the range of profits.\n\n4. **Assign Customers to Profit Buckets:**\n   - Divide the range of profits into 10 equal intervals (buckets).\n   - Assign each customer to one of the 10 buckets based on their total monthly profit.\n\n5. **Calculate Minimum Profit in Each Bucket:**\n   - For each bucket, find the minimum profit value among the customers assigned to that bucket.\n\n6. **Compute the Average of Minimum Profits:**\n   - Calculate the average of the minimum profit values from all the buckets to get the final result.",
        "special_function": null
    },
    {
        "instance_id": "local062_3",
        "db": "complex_oracle",
        "question": "Could you group customers in Italy into 10 buckets based on their monthly profitability in December 2021 and calculate the average from maximum profits of each bucket?",
        "external_knowledge": null,
        "plan": "1. **Calculate Individual Profit**: \n   - Compute the profit for each transaction by multiplying the quantity sold by the difference between unit price and unit cost.\n   - Filter transactions to include only those from customers in Italy and for the specified month.\n\n2. **Aggregate Monthly Profit by Customer**: \n   - Sum up the individual profits for each customer to get the total monthly profit for each customer.\n\n3. **Determine Profit Range**:\n   - Find the minimum and maximum total monthly profit among all customers.\n\n4. **Assign Customers to Buckets**:\n   - Create 10 profit buckets by dividing the range between the minimum and maximum profit into 10 equal parts.\n   - Assign each customer to a bucket based on their total monthly profit.\n\n5. **Calculate Maximum Profit per Bucket**:\n   - For each bucket, find the maximum profit of the customers within that bucket.\n\n6. **Compute Average of Maximum Profits**:\n   - Calculate the average of the maximum profits from each of the 10 buckets.\n\nBy following these steps, the user\u2019s intention to group customers into profitability buckets and calculate the average of the maximum profits in each bucket is fulfilled.",
        "special_function": null
    },
    {
        "instance_id": "local062_4",
        "db": "complex_oracle",
        "question": "Could you group customers in Italy into 10 buckets based on their monthly profitability in December 2021 and calculate the difference between the largest and smallest number of customer?",
        "external_knowledge": null,
        "plan": "1. **Calculate Profit for Each Sale:**\n   - Combine sales data with cost data to calculate the profit for each sale by subtracting the unit cost from the unit price and multiplying by the quantity sold.\n   - Filter this data to include only customers from Italy and sales made in December 2021.\n\n2. **Aggregate Monthly Profit by Customer:**\n   - Sum the profit for each customer over the month to get the total monthly profit for each customer.\n\n3. **Determine Profit Range:**\n   - Calculate the minimum and maximum monthly profit values across all customers to determine the range of profitability.\n\n4. **Assign Customers to Buckets:**\n   - Divide the range of monthly profits into ten equal intervals.\n   - Assign each customer to one of these ten buckets based on where their monthly profit falls within these intervals.\n\n5. **Count Customers in Each Bucket:**\n   - Count the number of customers in each of the ten buckets to get the distribution of customers across the profitability range.\n\n6. **Calculate Difference in Bucket Sizes:**\n   - Identify the smallest and largest bucket sizes (i.e., the minimum and maximum counts of customers in any bucket).\n   - Compute the difference between the largest and smallest bucket sizes.\n\n7. **Return Result:**\n   - Output the difference between the largest and smallest number of customers in the buckets as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local070_3_3",
        "db": "cleaned_data",
        "question": "Could you review our records in June 2022 and identify which countries have the longest streak of consecutive inserted dates? Please list the 2-letter length country codes of these countries.",
        "external_knowledge": null,
        "plan": "1. **Filter Records for June 2022:**\n   - Extract records with dates between June 1, 2022, and June 30, 2022.\n   - Ensure each date-country pair is unique by using row numbering and filtering on the first occurrence.\n\n2. **Calculate Date Differences:**\n   - Compute the difference between the day of the month and the row number for each country to identify potential streaks.\n\n3. **Count Consecutive Dates:**\n   - For each country, count how many times each difference value appears. This helps to identify streaks of consecutive dates for each country.\n\n4. **Rank Streak Lengths:**\n   - For each country, assign a rank based on the length of these streaks, with the longest streak getting the highest rank.\n\n5. **Count Streaks by Rank:**\n   - For each country, count the number of times each rank appears. This step helps to identify the countries with the longest streaks.\n\n6. **Identify Countries with Longest Streak:**\n   - Select the countries with the maximum streak length by comparing the counts from the previous step.\n   - Return the country codes of these countries.",
        "special_function": null
    },
    {
        "instance_id": "local070_3_4",
        "db": "cleaned_data",
        "question": "How many countries have more than 10 consecutive insertion dates in June 2021?",
        "external_knowledge": null,
        "plan": "1. **Identify Relevant Data:**\n   - Create a subset of data containing only the relevant records where the insertion dates fall within June 2021.\n\n2. **Remove Duplicates:**\n   - From the subset, ensure that there are no duplicate records for any given date and country combination.\n\n3. **Calculate Date Differences:**\n   - For each country, compute a difference metric by subtracting the day of the month from a sequential row number to identify consecutive date sequences.\n\n4. **Count Consecutive Dates:**\n   - Calculate the number of consecutive dates for each country by counting the occurrences of each difference metric.\n\n5. **Rank the Sequences:**\n   - Rank these sequences for each country based on the count of consecutive dates, in descending order.\n\n6. **Filter Long Sequences:**\n   - Filter the ranked sequences to retain only those where the count of consecutive dates is greater than 10.\n\n7. **Count Distinct Countries:**\n   - Finally, count the number of distinct countries that have more than 10 consecutive insertion dates.\n\n8. **Return Result:**\n   - Return the count of these distinct countries.",
        "special_function": null
    },
    {
        "instance_id": "local074",
        "db": "data_bank",
        "question": "Please generate a summary of the closing balances at the end of each month for each customer, show the monthly changes and monthly cumulative bank account balances.",
        "external_knowledge": null,
        "plan": "1. **Generate a Series of Values**:\n    - Create a recursive Common Table Expression (CTE) to generate a sequence of numbers starting from 0 up to 3. This will be used later to generate months.\n\n2. **Calculate Monthly Transaction Amounts**:\n    - Create another CTE to calculate the net transaction amount for each customer at the end of each month. \n    - Convert transaction dates to 'YYYY-MM' format to group transactions by month.\n    - Sum the transaction amounts where deposits are added and withdrawals are subtracted.\n    - Group the results by customer and month to get the closing balance for each month.\n\n3. **Generate a List of Months**:\n    - Create a CTE to generate a list of months for each customer starting from a fixed date, adding the values from the generated series to this date.\n    - This ensures that even months without transactions are included for each customer.\n\n4. **Combine Transactions with Generated Months**:\n    - Perform a left join between the generated months and the monthly transaction amounts.\n    - For each customer, combine the list of all possible months with the actual transaction data, ensuring that all months are represented.\n\n5. **Calculate Monthly Balances**:\n    - For each customer and each month, calculate the balance activity (net transaction amount) using the joined data.\n    - Use a window function to calculate the cumulative sum of these monthly balances to get the month-end balance for each month.\n\n6. **Output the Results**:\n    - Select the customer ID, generated month, net transaction amount (balance activity), and the cumulative month-end balance.\n    - Ensure that even if there are no transactions in a month, it is represented with a balance activity of zero. \n\nThis process generates a summary of monthly balances for each customer, showing both the monthly changes and the cumulative balances at the end of each month.",
        "special_function": null
    },
    {
        "instance_id": "local074_4",
        "db": "data_bank",
        "question": "What is the difference in average month-end balance between the month with the most and the month with the fewest customers having a positive balance in 2020?",
        "external_knowledge": null,
        "plan": "1. **Generate Series of Months**:\n   - Create a recursive series of numbers to represent months in the year 2020.\n\n2. **Generate Month-Year Pairs**:\n   - Use the series of numbers to generate distinct month-year pairs for each customer based on a fixed start date in 2020.\n\n3. **Calculate Monthly Transaction Totals**:\n   - Compute the net transaction amounts for each customer by month for the year 2020. This involves summing up deposits and withdrawals.\n\n4. **Calculate Month-End Balance**:\n   - Combine the generated month-year pairs with the monthly transaction totals to compute the month-end balance for each customer. If there are no transactions for a month, the balance is assumed to be zero.\n\n5. **Count Positive Balances**:\n   - Count the number of customers with a positive month-end balance for each generated month.\n\n6. **Identify Months with Most and Fewest Positive Balances**:\n   - Determine the month with the highest count of customers with positive balances.\n   - Determine the month with the lowest count of customers with positive balances.\n\n7. **Compute Average Balances**:\n   - Calculate the average month-end balance for the month with the highest count of positive balances.\n   - Calculate the average month-end balance for the month with the lowest count of positive balances.\n\n8. **Calculate Difference**:\n   - Subtract the average balance of the month with the fewest positive balances from the average balance of the month with the most positive balances to find the difference.",
        "special_function": null
    },
    {
        "instance_id": "local297",
        "db": "data_bank",
        "question": "I want to know if the closing balances of each customer have increased by at least 5% in the most recent month based on their existing data. Can you provide these figures? (If the previous month's balance is zero, multiply the current balance by 100 as the growth rate)",
        "external_knowledge": null,
        "plan": "1. Calculate the running balance for each customer by month.\n2. Calculate the monthly balance growth rate for each customer.\n3. Select the latest balance growth rate for each customer and flag whether it exceeds 5%.\n4. Calculate the percentage of customers whose last month's growth rate exceeds 5%.",
        "special_function": null
    },
    {
        "instance_id": "local298",
        "db": "data_bank",
        "question": "We need to optimize our data allocation strategy. One option we're considering is allocating data based on the customer\u2019s balance at the end of the previous month. Could you calculate the monthly data requirements for this approach, starting from February",
        "external_knowledge": null,
        "plan": "1. **Calculate Monthly Transaction Amounts**:\n    - Create a subquery that categorizes each transaction as either a deposit or withdrawal.\n    - Calculate the net transaction amount for each month by summing categorized transactions, grouped by customer and month.\n\n2. **Compute Running Balance**:\n    - Use a window function to calculate a running balance for each customer, ordered by month.\n    - Store this running balance in a common table expression (CTE).\n\n3. **Determine Previous Month\u2019s Balance**:\n    - In another CTE, calculate the balance of the previous month for each customer using a window function.\n    - Use the `LAG` function to fetch the balance from the previous row within the same customer partition.\n\n4. **Prepare Data Storage Calculation**:\n    - Add a case statement to handle negative balances by setting them to zero, as negative balances are not considered for data allocation.\n    - Assign a row number to each month per customer to help filter out the first month (as it does not have a previous month for comparison).\n\n5. **Aggregate Data Storage Requirements**:\n    - Filter out the first month for each customer since it lacks a previous month balance.\n    - Group the data by month and sum the data storage requirements across all customers.\n\n6. **Order Results**:\n    - Order the final results by month to provide a chronological summary of total data storage requirements.",
        "special_function": null
    },
    {
        "instance_id": "local299",
        "db": "data_bank",
        "question": "To optimize our data allocation strategy, calculate the average balance for each user over the past 30 days on a monthly basis. Identify the highest average balance among all users each month and sum these highest values to determine the total data allocation needed, starting from February.",
        "external_knowledge": null,
        "plan": "1. **Generate Date Series**:\n    - Create a recursive CTE to generate a date series for each user, starting from their earliest transaction date to their latest transaction date.\n\n2. **Classify Transactions**:\n    - Create a CTE to classify transactions into positive (deposits) or negative (withdrawals) amounts.\n\n3. **Calculate Daily Balances**:\n    - Create a CTE to calculate the daily balance for each user by summing up the classified transactions up to each date in the date series.\n\n4. **Calculate 30-Day Average Balance**:\n    - Create a CTE to calculate the 30-day rolling average balance for each user. This involves averaging the daily balances over the past 30 days.\n    - Ensure that the average balance is only considered valid after the first 30 days.\n    - Replace negative average balances with zero.\n\n5. **Identify Monthly Maximum Average Balance**:\n    - Create a CTE to identify the maximum average balance for each user for each month. This involves grouping the data by user and month and selecting the highest valid 30-day average balance for each user.\n\n6. **Sum Highest Monthly Averages**:\n    - Calculate the total required data allocation by summing the highest average balances for all users for each month.\n    - Exclude the first month of data for each user to focus on the subsequent months starting from February.\n\n7. **Output the Results**:\n    - Group the results by month and order them chronologically to get the final total data allocation needed for each month, rounded to one decimal place.",
        "special_function": null
    },
    {
        "instance_id": "local300",
        "db": "data_bank",
        "question": "We need to understand the trends in user account balances. Calculate the sum of the highest daily balances for each user per month. This data will help us forecast future resource needs.",
        "external_knowledge": null,
        "plan": "1. **Generate Date Series:**\n   - Create a date series for each user starting from their earliest transaction date to their latest transaction date.\n   - Use a recursive CTE to incrementally add each day within the range for each user.\n\n2. **Categorize Transactions:**\n   - Categorize each transaction by determining if it is a deposit or a withdrawal.\n   - Convert withdrawal amounts to negative values to reflect deduction from the balance.\n\n3. **Calculate Daily Balances:**\n   - Combine the generated date series with the categorized transactions.\n   - Compute the running balance for each user by summing the transaction amounts day-by-day.\n\n4. **Adjust Negative Balances:**\n   - Ensure that any negative balances are set to zero, as negative balances are not considered in the final data storage calculation.\n\n5. **Extract Monthly Maximum Balances:**\n   - For each user, and for each month, find the highest daily balance.\n   - This represents the peak resource requirement for each user in a given month.\n\n6. **Summarize Monthly Data:**\n   - Sum the highest daily balances across all users for each month.\n   - Round the result to one decimal place for clarity.\n\n7. **Order Results:**\n   - Order the summarized data by month to observe trends over time.",
        "special_function": null
    },
    {
        "instance_id": "local075_2",
        "db": "clique_bait_schema",
        "question": "I need to understand customer engagement better. Can you provide a breakdown of how many times each product was viewed, how many times they were added to the cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product.",
        "external_knowledge": null,
        "plan": "1. **Identify Product Views and Adds to Cart:**\n   - Create a temporary dataset that counts the number of times each product page was viewed and added to the cart.\n   - This involves filtering the data to include only those records where the product ID is not null.\n   - Aggregate the data by the page identifier to get counts of views and adds to the cart.\n\n2. **Identify Purchases from Cart:**\n   - Create another temporary dataset to count the number of times products were actually purchased from the cart.\n   - This involves filtering the data to include only those records where the product ID is not null and ensuring the product was purchased (by checking for a specific purchase event type).\n   - Exclude certain page identifiers from this dataset.\n   - Aggregate the data by the page identifier to get counts of purchases from the cart.\n\n3. **Identify Abandoned Carts:**\n   - Create a third temporary dataset to count the number of times products were added to the cart but not purchased.\n   - This involves filtering the data to include only those records where the product ID is not null and ensuring the product was not purchased (by checking for the absence of a specific purchase event type).\n   - Exclude certain page identifiers from this dataset.\n   - Aggregate the data by the page identifier to get counts of abandoned carts.\n\n4. **Combine Results:**\n   - Join the three temporary datasets with the main dataset containing page information.\n   - This involves matching the page identifiers across all datasets.\n   - Select and format the relevant counts for views, adds to cart, abandoned carts, and purchases.\n\n5. **Output the Results:**\n   - Produce a final result set that includes the page identifier, page name, and the counts for each type of customer engagement: views, adds to cart, abandoned carts, and purchases.\n   - Ensure the output is clear and organized for analysis.",
        "special_function": null
    },
    {
        "instance_id": "local077",
        "db": "fresh_segments_schema",
        "question": "Please review our interest data from September 2018 to August 2019. I need to know the max average composition value for each month, as well as the three-month rolling average. Ensure the output includes the date, the interest name, the max index composition for that month, the rolling average, and the top-ranking interests from the one month ago and two months ago with their names.",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Composition and Rank:**\n   - Create a temporary dataset that calculates the average composition for each interest per month.\n   - Rank the interests within each month based on their average composition in descending order.\n\n2. **Determine Monthly Maximum and Rolling Average:**\n   - From the ranked dataset, select only the top-ranked interest for each month.\n   - Calculate a three-month rolling average of the top-ranked average compositions.\n\n3. **Add Lagged Interest Data:**\n   - Enhance the dataset by including interest names and average compositions from the previous one and two months for each month.\n\n4. **Final Selection:**\n   - Filter the enhanced dataset to include only the specified date range from September 2018 to August 2019.\n   - Select and display the month, interest name, maximum average composition for the month, three-month rolling average, and the top interests from one and two months ago with their corresponding names and average compositions.",
        "special_function": null
    },
    {
        "instance_id": "local077_2",
        "db": "fresh_segments_schema",
        "question": "Identify the top 10 and bottom 10 interest categories based on their highest composition values across all months. For each category, display the time(MM-YYYY), interest name, and the composition value",
        "external_knowledge": null,
        "plan": "1. **Rank Interest Categories**:\n    - Create a temporary result set to rank each interest category based on its composition value across all months.\n    - For each category, rank the composition values in descending order.\n\n2. **Filter Top Composition Values**:\n    - From the ranked result set, filter out the highest composition value for each interest category.\n    - This ensures that we are considering only the peak composition value for each category.\n\n3. **Select Top 10 Categories**:\n    - From the filtered set of highest composition values, select the top 10 categories with the highest values.\n    - Order these top 10 categories by their composition values in descending order.\n\n4. **Select Bottom 10 Categories**:\n    - Similarly, from the filtered set of highest composition values, select the bottom 10 categories with the lowest values.\n    - Order these bottom 10 categories by their composition values in ascending order.\n\n5. **Combine Results**:\n    - Combine the top 10 and bottom 10 categories into a single result set.\n    - Use a union operation to merge these two sets.\n\n6. **Final Ordering**:\n    - Order the combined result set by the composition values in descending order to provide a coherent view of the data.\n\n7. **Output**:\n    - Display the time, interest category name, and the composition value for the selected top 10 and bottom 10 categories.",
        "special_function": null
    },
    {
        "instance_id": "local081",
        "db": "northwind",
        "question": "How many customers were in each spending group in 1998, and what percentage of the total customer base does each group represent?",
        "external_knowledge": null,
        "plan": "1. **Filter Orders by Year**:\n   - Select orders from the specified year.\n   - Calculate the total spending for each customer by summing up the product of unit price and quantity for all their orders within that year.\n   - Group the data by customer to get the total spending per customer.\n\n2. **Classify Customers into Spending Groups**:\n   - Using the total spending per customer, classify each customer into a predefined spending group based on their total spending amount.\n   - This classification is based on predefined thresholds for each group.\n\n3. **Count Customers per Spending Group**:\n   - Count the number of customers in each spending group.\n   - Group the data by spending group to get the total number of customers in each group.\n\n4. **Calculate Percentage Representation**:\n   - For each spending group, calculate the percentage of the total customer base that the group represents.\n   - This is done by dividing the number of customers in each group by the total number of customers across all groups and converting it to a percentage.\n\n5. **Output Results**:\n   - Select the spending group, the total number of customers in each group, and the percentage representation of each group.\n   - Order the results by the number of customers in each group in descending order to show the most populous groups first.",
        "special_function": null
    },
    {
        "instance_id": "local084",
        "db": "northwind",
        "question": "Can you tell me the ID of employee who has the highest proportion of late orders compared to their total orders?",
        "external_knowledge": null,
        "plan": "1. **Identify Late Orders**:\n   - Create a temporary table to count the number of late orders for each employee. An order is considered late if the required delivery date is on or before the actual shipping date.\n   - Group the results by employee.\n\n2. **Summarize Total Orders**:\n   - Create another temporary table to count the total number of orders handled by each employee.\n   - Group the results by employee.\n\n3. **Join Summarized Data**:\n   - Join the total orders summary with the employee details.\n   - Perform a left join with the late orders table to ensure that employees with no late orders are also included.\n\n4. **Calculate Late Order Proportion**:\n   - For each employee, compute the proportion of late orders to total orders. Use a function to handle cases where the late orders count might be null, ensuring it is treated as zero.\n\n5. **Find Employee with Highest Proportion**:\n   - Sort the results in descending order based on the calculated proportion of late orders.\n   - Limit the results to return only the employee with the highest proportion.\n\nBy following these steps, the query identifies the employee with the highest proportion of late orders relative to their total number of orders.",
        "special_function": null
    },
    {
        "instance_id": "local097",
        "db": "Db-IMDB",
        "question": "I'm curious about the most prolific decade in terms of film production. Could you analyze our data and identify which decade had the largest number of films? I need the total count for that specific period.",
        "external_knowledge": null,
        "plan": "1. Create a table containing each distinct year and its corresponding decade (start year and end year).\n2. Calculate the number of movies for each year.\n3. Calculate the total number of movies within each decade.\n4. Identify the decade with the highest number of movies and output the decade and its total movie count.",
        "special_function": null
    },
    {
        "instance_id": "local098",
        "db": "Db-IMDB",
        "question": "I'd like to know how many actors have managed to avoid long breaks in their careers. Could you check our records to see how many actors haven't been out of work for more than three years at any point?",
        "external_knowledge": null,
        "plan": "1. **Calculate Annual Movie Count**:\n   - Extract the year and count the number of distinct movies an actor has appeared in for each year.\n\n2. **Identify Multi-Year Actors**:\n   - Determine which actors have worked for more than one year by counting the distinct years and finding the earliest and latest year they worked.\n\n3. **Prepare Yearly Range Data**:\n   - For each actor, calculate a range of years from each year to three years ahead. This helps in checking employment continuity within this range.\n\n4. **Summarize Movie Count Till Each Year**:\n   - For each actor and each year, calculate the cumulative number of movies up to that year. This helps track the actor's work history year by year.\n\n5. **Summarize Movie Count Till Year Plus Three**:\n   - For each actor and each year, calculate the cumulative number of movies up to three years after that year. This helps in checking if the actor continued to work without a break in the subsequent years.\n\n6. **Identify Actors with Breaks**:\n   - Identify actors who have the same cumulative number of movies in a given year and three years later, indicating a break in their career.\n\n7. **Count Actors Without Long Breaks**:\n   - Count the number of actors who do not appear in the list of actors with breaks. This gives the total number of actors who have not had breaks longer than three years.\n\n8. **Final Result**:\n   - Return the count of actors who have never been unemployed for more than three years at any point in their careers.",
        "special_function": null
    },
    {
        "instance_id": "local099",
        "db": "Db-IMDB",
        "question": "I need you to look into the actor collaborations and tell me how many actors have made more films with Yash Chopra than with any other director. This will help us understand his influence on the industry better.",
        "external_knowledge": null,
        "plan": "1. **Identify the Director**: Extract the unique identifier for the director named 'Yash Chopra'.\n\n2. **Count Collaborations**: For each actor and director pair, count the number of distinct movies they have collaborated on.\n\n3. **Filter Yash Chopra Collaborations**: From the previous results, isolate the records where the director is 'Yash Chopra' and store the number of movies each actor has done with him.\n\n4. **Max Collaborations with Other Directors**: For each actor, find the maximum number of movies they have done with any director other than 'Yash Chopra'.\n\n5. **Comparison**: Compare the number of movies each actor has done with 'Yash Chopra' to the maximum number of movies they have done with any other director. Mark actors who have more movies with 'Yash Chopra'.\n\n6. **Final Count**: Count the number of actors who have collaborated more frequently with 'Yash Chopra' than with any other director.",
        "special_function": null
    },
    {
        "instance_id": "local100",
        "db": "Db-IMDB",
        "question": "Can you investigate our database and find out how many actors have a 'Shahrukh number' of 2? This means they acted in a film with someone who acted with Shahrukh Khan, but not directly with him.",
        "external_knowledge": null,
        "plan": "1. **Identify the Target Actor**:\n   - Create a list of IDs for actors whose name includes 'Shahrukh'.\n\n2. **Find First Degree Connections**:\n   - Identify movies featuring the target actor.\n   - List all unique movies along with the target actor's ID.\n\n3. **Find Co-Actors in the Same Movies**:\n   - Determine actors who acted in the same movies as the target actor.\n   - Exclude the target actor from this list.\n   - Generate a unique list of these co-actors.\n\n4. **Find Second Degree Connections**:\n   - Identify movies featuring these first-degree co-actors.\n   - List all unique movies along with these co-actors' IDs.\n\n5. **Count Second Degree Actors**:\n   - Join the main actor list with the second-degree movie list to find actors who acted in these movies.\n   - Ensure these actors are not the same as the first-degree co-actors.\n   - Count the distinct number of these second-degree actors.\n\n6. **Final Result**:\n   - Output the count of distinct actors who have a 'Shahrukh number' of 2, i.e., acted with someone who acted with the target actor but not directly with the target actor.",
        "special_function": null
    },
    {
        "instance_id": "local114",
        "db": "parch_posey",
        "question": "How many total orders were placed for the region with the largest sales in USD?",
        "external_knowledge": null,
        "plan": "1. **Identify Region with Maximum Sales:**\n   - Create a subquery to calculate the total sales amount for each region by summing up the order amounts.\n   - Group the results by region to get the total sales per region.\n   - From this grouped result, select the region with the highest total sales amount.\n\n2. **Store Region with Maximum Sales:**\n   - Use a Common Table Expression (CTE) to store the region that has the maximum total sales.\n\n3. **Count Orders for the Identified Region:**\n   - In the main query, join the necessary tables to link orders to regions.\n   - Use a WHERE clause to filter the orders based on the region identified in the CTE.\n   - Count the total number of orders for this specific region.\n\n4. **Return the Result:**\n   - Output the count of orders as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local115",
        "db": "parch_posey",
        "question": "Who is the sales representative with the largest sales in USD in each region?",
        "external_knowledge": null,
        "plan": "1. **Identify Sales Data**:\n    - Retrieve the total sales amount for each sales representative.\n    - Aggregate the sales data to sum the total sales amount for each representative.\n\n2. **Combine with Region Information**:\n    - Associate each sales representative with their respective region.\n    - Group the data by region and sales representative to compute the total sales per representative within each region.\n\n3. **Determine the Highest Sales by Region**:\n    - From the aggregated data, determine the maximum sales amount within each region.\n    - Capture both the region and the corresponding sales representative who achieved this maximum sales amount.\n\n4. **Extract and Order Results**:\n    - Select the region and the sales representative with the highest sales amount for each region.\n    - Order the results by the total sales amount in descending order to highlight the top performers.\n\nThis reference plan outlines the logical steps to determine the sales representative with the highest sales in USD for each region, and then orders them based on their total sales.",
        "special_function": null
    },
    {
        "instance_id": "local128",
        "db": "BowlingLeague",
        "question": "Can you list the bowlers, the match number, the game number, the handicap score, the tournament date, and the tournament location for bowlers who won a game with a handicap score of 190 or less at Thunderbird Lanes, Totem Lanes, and Bolero Lanes?",
        "external_knowledge": null,
        "plan": "1. **Identify Qualified Bowlers**:\n   - Create a temporary result set (CTE) to identify bowlers who meet the initial conditions.\n   - Filter records where the bowler\u2019s handicap score is 190 or less, and they won the game.\n   - Further filter these records to include only those where the tournament location is one of the specified locations.\n   - Group by bowler ID to ensure each bowler is considered only once.\n   - Use a HAVING clause to ensure the bowler has participated in tournaments at all three specified locations.\n\n2. **Retrieve Detailed Information**:\n   - Select the required details including bowler's ID, first name, last name, match number, game number, handicap score, tournament date, and location.\n   - Join the main tables to connect bowlers to their scores, match details, and tournament information.\n   - Join with the previously identified qualified bowlers to filter the final results.\n   - Apply the same filters again on the final query to ensure only the relevant records are selected, specifically those with a handicap score of 190 or less, who won the game, and the tournament location is one of the specified locations.",
        "special_function": null
    },
    {
        "instance_id": "local130",
        "db": "school_scheduling",
        "question": "Can you list the last name of all students who have completed English courses, rank them by Quintile based on the grades they received, and order the results from the highest to the lowest grades?",
        "external_knowledge": null,
        "plan": "1. **Identify Completed Courses**: Filter out records of students who have completed courses in a specific subject category.\n\n2. **Determine Ranks**: For each student who completed courses in the specified category, calculate their rank by comparing their grade with the grades of other students in the same category. This rank represents the number of students who have grades equal to or higher than the student.\n\n3. **Calculate Total Students**: Count the total number of students who have completed courses in the specified category.\n\n4. **Assign Quintiles**: Based on the rank calculated in step 2 and the total number of students from step 3, assign a quintile to each student. The quintiles are determined by dividing the student population into five equal parts. For example:\n   - The top 20% of students are in the 'First' quintile.\n   - The next 20% are in the 'Second' quintile, and so on.\n\n5. **Select and Order Results**: Select the last name of each student and their corresponding quintile. Order the final results based on the rank in descending order, so that students with the highest grades appear first.",
        "special_function": null
    },
    {
        "instance_id": "local131",
        "db": "EntertainmentAgency",
        "question": "Which musical styles are the most frequently ranked as first, second, or third preferences, and how many times does each style appear in each rank?",
        "external_knowledge": null,
        "plan": "1. **Identify Musical Preferences**: Extract and label the style preferences based on their ranking position (first, second, third) from the preferences table.\n    - If the preference sequence is 1, label it as `FirstStyle`.\n    - If the preference sequence is 2, label it as `SecondStyle`.\n    - If the preference sequence is 3, label it as `ThirdStyle`.\n\n2. **Join Preferences with Styles**: Link the extracted and labeled preferences with the styles table to get the style names.\n    - Join on the style identifier from both the styles table and the preferences (considering each rank separately).\n\n3. **Count Preferences by Rank**: Calculate the number of times each style appears as a first, second, or third preference.\n    - For each style, count how many times it is listed as `FirstStyle`.\n    - Similarly, count how many times it is listed as `SecondStyle`.\n    - Similarly, count how many times it is listed as `ThirdStyle`.\n\n4. **Filter Out Styles with No Preferences**: Ensure that only styles which have at least one count in any of the preference ranks (first, second, or third) are included.\n    - Apply a condition to filter out styles with zero counts in all preference categories.\n\n5. **Group and Aggregate Data**: Group the results by style identifier and style name to aggregate the counts of preferences.\n    - Use the grouping to ensure that each style appears only once in the result set.\n\n6. **Order the Results**: Sort the aggregated results in descending order based on the counts of first preferences, then second preferences, and finally third preferences.\n    - If the counts are equal, ensure a consistent order by including the style identifier.\n\nThis structured approach ensures that the query identifies and counts the preferences accurately, while filtering and organizing the results to meet the initial user instruction.",
        "special_function": null
    },
    {
        "instance_id": "local131_3",
        "db": "EntertainmentAgency",
        "question": "Which musical style has a weighted score closest to the average, where first preferences are weighted by 3, second preferences by 2, and third preferences by 1, if there are ties, choose the one that comes first alphabetically?",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local132",
        "db": "EntertainmentAgency",
        "question": "Show entertainer and customer pairs where both the first and second style preferences of customers match the first and second strengths of entertainers (or vice versa), displaying only the entertainer's stage name and the customer's last name.",
        "external_knowledge": null,
        "plan": "Display customers and their first, second and third-ranked preferences along with entertainers and their first, second and third-ranked strengths, then match customers to entertainers when the first or second preference matches the first or second strength",
        "special_function": null
    },
    {
        "instance_id": "local141",
        "db": "AdventureWorks",
        "question": "How did each salesperson's annual total sales compare to their annual sales quota? Provide the difference between their total sales and the quota for each year, organized by salesperson and year.",
        "external_knowledge": null,
        "plan": "1. **Aggregate Sales Data:**\n   - Create a temporary dataset that summarizes the annual total sales for each salesperson.\n   - Filter out any records where the salesperson identifier is missing.\n   - Group the data by salesperson and year, calculating the total sales for each combination.\n\n2. **Aggregate Quota Data:**\n   - Create another temporary dataset that summarizes the annual sales quota for each salesperson.\n   - Group the data by salesperson and year, calculating the total quota for each combination.\n\n3. **Combine Aggregated Data:**\n   - Join the two temporary datasets on the salesperson identifier and year, ensuring that each salesperson's annual total sales data aligns with their corresponding annual sales quota data.\n\n4. **Calculate Difference:**\n   - For each matched record, calculate the difference between the total sales and the sales quota.\n\n5. **Format and Output:**\n   - Select relevant columns, including salesperson identifier, year, total sales, sales quota, and the calculated difference.\n   - Ensure that the results are ordered by salesperson and year for clarity.\n   - Convert numerical results to text format for consistent output representation.",
        "special_function": null
    },
    {
        "instance_id": "local230",
        "db": "imdb_movies",
        "question": "Please analyze our database to identify the top three genres with the most movies that have ratings above 8. Then, determine which director has directed the most movies in these genres and provide the number of movies for each director.",
        "external_knowledge": null,
        "plan": "1. Identify the top 3 genres with the highest number of movies that have an average rating above 8.\n2. Identify the top 3 directors who have directed the most movies within the top 3 genres identified in the previous subquery, with an average rating above 8.\n3. Select the top 3 directors from the top_director subquery.",
        "special_function": null
    },
    {
        "instance_id": "local156",
        "db": "trading",
        "question": "Could you provide the yearly average cost of Bitcoin purchases for each region? Rank the regions based on these averages for each year, and calculate the annual percentage change. This data will be crucial for our upcoming financial planning. Please exclude the results for 2017 from the output.",
        "external_knowledge": null,
        "plan": "1. **Create Initial Data Set (CTE: cte_dollar_cost_average)**:\n   - Extract the year from the transaction date.\n   - Group the transactions by year and region.\n   - Calculate the average cost for purchases by taking the weighted average of the purchase quantities and prices.\n   - This results in a dataset with columns for the year, region, and the calculated average cost.\n\n2. **Rank and Calculate Changes (CTE: cte_window_functions)**:\n   - For each year and region, determine the ranking based on the average cost.\n   - Retrieve the average cost from the previous year for each region.\n   - Assign a row number to each region-year pair to facilitate filtering out the first year, which lacks a previous year for comparison.\n\n3. **Final Selection and Calculations**:\n   - Select records from the window functions CTE, excluding the first year for each region.\n   - Exclude the year 2017 from the results.\n   - For each record, include the year, region, average cost, ranking, and the annual percentage change in average cost.\n   - Round the average cost and percentage change to two decimal places for readability.\n   - Sort the results by region and year to provide a clear and ordered output.",
        "special_function": null
    },
    {
        "instance_id": "local157",
        "db": "trading",
        "question": "For our upcoming meeting, please provide the daily percentage change in trading volume for all tickers from August 1 to August 10, 2021. This trend analysis is crucial for our strategic planning.",
        "external_knowledge": null,
        "plan": "1. **Data Transformation for Volume:**\n   - Create a temporary dataset where the trading volumes are standardized.\n   - If the volume ends with 'K', multiply the numerical part by 1000.\n   - If the volume ends with 'M', multiply the numerical part by 1,000,000.\n   - If the volume is '-', set it to 0.\n   - Otherwise, convert the volume to a numeric value as it is.\n\n2. **Calculate Previous Volume:**\n   - Create another temporary dataset to calculate the previous day's volume for each ticker.\n   - Partition the data by ticker and order by the date to align each volume with the previous day\u2019s volume using the LAG function.\n   - Filter out records where the volume is 0 to avoid division by zero in later calculations.\n\n3. **Calculate Daily Percentage Change:**\n   - Select the relevant data from the temporary dataset, including the ticker, date, current volume, and previous volume.\n   - Calculate the daily percentage change in volume using the formula: \\((\\text{current volume} - \\text{previous volume}) / \\text{previous volume} \\times 100\\).\n   - Round the percentage change to two decimal places.\n\n4. **Filter and Order Results:**\n   - Filter the results to include only the dates between August 1 and August 10, 2021.\n   - Order the final output by ticker and date to present a clear trend analysis.\n\nBy following these steps, the query provides the daily percentage change in trading volume for each ticker within the specified date range, supporting trend analysis for strategic planning.",
        "special_function": null
    },
    {
        "instance_id": "local162",
        "db": "university",
        "question": "How many students have taken every IS course offered and managed to secure at least a grade of 3.0 in each course?",
        "external_knowledge": null,
        "plan": "1. **Identify All Relevant Courses**:\n    - Extract a list of all unique course identifiers that match a specific pattern indicating they are the courses of interest.\n    - Ensure these identifiers are correctly formatted and fall within a specified range to include all relevant courses.\n\n2. **Count Total Number of Relevant Courses**:\n    - Calculate the total number of unique courses identified in the previous step. This provides a benchmark to compare against students' enrollments.\n\n3. **Filter and Group Students by Course Performance**:\n    - From the student enrollments, filter out only those enrollments that correspond to the identified courses and where the student has achieved a minimum performance grade.\n    - Group these enrollments by individual students to analyze their performance across multiple courses.\n\n4. **Verify Complete Course Coverage by Students**:\n    - For each student group, count the number of unique courses they have taken and compare this count to the total number of relevant courses.\n    - Ensure only students who have taken all the relevant courses and met the performance criteria are selected.\n\n5. **Count Qualified Students**:\n    - Count the total number of students who have met all the criteria, indicating how many students have taken every relevant course and achieved at least the minimum grade in each.\n\nBy following these steps, the original intention of determining the number of students meeting the specific criteria can be accurately resolved.",
        "special_function": null
    },
    {
        "instance_id": "local162_2",
        "db": "university",
        "question": "Which faculty members' salaries are closest to the average salary for their respective ranks? Please provide the ranks, first names, last names, and salaries.",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Salaries by Rank**:\n   - Create a temporary table to compute the average salary for each rank.\n   - Group the data by rank and calculate the average salary for each group.\n\n2. **Determine Salary Differences**:\n   - Create another temporary table to list each faculty member along with their rank, first name, last name, salary, and the absolute difference between their salary and the average salary for their rank.\n   - Join the original data with the previously created temporary table to associate each faculty member with the average salary for their rank.\n\n3. **Find Minimum Differences**:\n   - Create a temporary table to determine the minimum salary difference for each rank.\n   - Group the data by rank and find the minimum difference in salary from the previously calculated differences.\n\n4. **Retrieve Closest Salaries**:\n   - Join the temporary table containing faculty members and their salary differences with the temporary table containing minimum differences.\n   - Select and display the rank, first name, last name, and salary for faculty members whose salary difference matches the minimum difference for their respective ranks.\n\nThis step-by-step plan ensures that the faculty members with salaries closest to the average salary for their ranks are identified and their relevant details are retrieved.",
        "special_function": null
    },
    {
        "instance_id": "local168",
        "db": "job",
        "question": "What is the average salary for remote Data Analyst jobs requiring the top three most in-demand skills?",
        "external_knowledge": null,
        "plan": "1. **Identify Top Skills by Demand:**\n   - Create a temporary dataset to count how many job postings require each skill.\n   - Filter the dataset to include only remote job postings for the specified job title that have a salary listed.\n   - Group the data by skill and count the number of job postings for each skill.\n   - Order the skills by the count in descending order and select the top three most in-demand skills.\n\n2. **Calculate Average Salary for Each Skill:**\n   - Create another temporary dataset to calculate the average salary for job postings that require each skill.\n   - Filter the dataset similarly to include only remote job postings for the specified job title with a listed salary.\n   - Group the data by skill and calculate the average salary for job postings that require each skill.\n\n3. **Combine Top Skills with Their Salaries:**\n   - Join the dataset of top three most in-demand skills with the dataset of average salaries by skill.\n   - Select the average salary values for these top skills.\n\n4. **Calculate Overall Average Salary:**\n   - Calculate the average of the average salaries obtained for the top three most in-demand skills.\n   - Return this value as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local194",
        "db": "sakila_master",
        "question": "I'd like a report on our top-performing films based on revenue per actor. Could you provide a list of the top-three films that generate the most revenue for each actor involved and the average revenue?",
        "external_knowledge": null,
        "plan": "1. **Calculate the Number of Actors per Film:**\n   - Create a temporary result set that includes each film along with the total number of actors involved in that film. This is done by joining the films with their actors and grouping the results by film.\n\n2. **Calculate Gross Revenue per Film:**\n   - Create another temporary result set that includes each film along with the total gross revenue it generated. This is achieved by joining the payments with rentals and inventory items, and then grouping by film.\n\n3. **Calculate Revenue per Actor for Each Film:**\n   - Combine the results from the first two steps to calculate the revenue generated per actor for each film. This involves joining the two temporary result sets on the film identifier and dividing the gross revenue by the number of actors for each film. Round the result to two decimal places for precision.\n\n4. **Retrieve Top-Performing Films:**\n   - Select the necessary details from the combined result set and order the films based on the calculated revenue per actor in descending order to identify the top-performing films.\n\n5. **Limit the Results:**\n   - Limit the final output to the top three films that generate the most revenue per actor.",
        "special_function": null
    },
    {
        "instance_id": "local202",
        "db": "aliens",
        "question": "How many of the top 10 states by alien population have a higher percentage of friendly aliens than hostile aliens, with an average alien age exceeding 200?",
        "external_knowledge": null,
        "plan": "1. **Calculate Aggression Counts**:\n   - Create a temporary table to calculate the number of friendly and hostile individuals for each region.\n   - For each record, increment the count of hostile or friendly individuals based on a specific condition.\n\n2. **Aggregate Alien Statistics**:\n   - Create another temporary table to gather statistics for each region.\n   - Compute the total number of individuals and the average age.\n   - Join this data with the aggression counts from the first temporary table.\n\n3. **Identify Top Regions**:\n   - From the aggregated statistics, select the top 10 regions based on total population size.\n\n4. **Filter and Count Regions**:\n   - Select records from the aggregated statistics where the region is one of the top 10 identified previously.\n   - Ensure the percentage of friendly individuals is greater than the percentage of hostile individuals.\n   - Ensure the average age of individuals in the region exceeds a specified threshold.\n   - Count the number of regions that meet these criteria and return the result.",
        "special_function": null
    },
    {
        "instance_id": "local209",
        "db": "delivery_center",
        "question": "Can you calculate the delivery completion ratio of the store with the most orders?",
        "external_knowledge": null,
        "plan": "1. Calculate the total number of orders for each store and select the top 5 stores with the most orders\n2. Calculate the number of completed deliveries for each store\n3. Calculate the number of cancelled deliveries for each store\n4. Calculate the delivery completion ratio for each store",
        "special_function": null
    },
    {
        "instance_id": "local210",
        "db": "delivery_center",
        "question": "Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?",
        "external_knowledge": null,
        "plan": "1. Calculate the number of finished orders for each hub in Februrary\n2. Calculate the number of finished orders for each hub in March\n3. Calculate the percentage difference in orders between February and March",
        "special_function": null
    },
    {
        "instance_id": "local211_2",
        "db": "delivery_center",
        "question": "Can you find 5 delivery drivers with the highest average number of daily deliveries?",
        "external_knowledge": null,
        "plan": "1. **Subquery to Calculate Daily Deliveries:**\n   - Select the driver ID, the date of the order, and the count of orders per day.\n   - Filter the orders to include only those that are finished and delivered.\n   - Group the results by driver ID and the specific day.\n\n2. **Main Query with Common Table Expression (CTE):**\n   - Create a CTE to calculate the average daily deliveries per driver, grouped by driver and month.\n   - Convert the date to extract the month.\n   - Calculate the average number of daily deliveries for each driver within the month and round it to two decimal places.\n   - Rank the drivers within each month based on their average daily deliveries in descending order.\n\n3. **Final Selection:**\n   - Select the driver IDs from the CTE where the rank is 5 or less, meaning we are interested in the top 5 drivers with the highest average daily deliveries.\n   - Order the final results by month and rank to ensure the output is organized.\n\nThis reference plan ensures the selection of the top 5 drivers with the highest average number of daily deliveries, grouped and ranked within each month.",
        "special_function": null
    },
    {
        "instance_id": "local220",
        "db": "EU_soccer",
        "question": "Who is the player with the most wins?",
        "external_knowledge": null,
        "plan": "1. Find all players who were part of a winning team, whether they were playing at home or away\n2. Calculate the total wins for each player and select the player with the most wins",
        "special_function": null
    },
    {
        "instance_id": "local220_1",
        "db": "EU_soccer",
        "question": "Who is the player with the most losses?",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local228",
        "db": "ipl_database",
        "question": "Identify the top three batsmen with the most runs and the top three bowlers with the most wickets in each season. In case of ties, prioritize players with lower player_ids. Exclude 'run out', 'hit wicket', and 'retired hurt' as out_types for bowlers.",
        "external_knowledge": null,
        "plan": "1. **Identify Top Batsmen for Each Season:**\n   - Create a temporary result set that calculates the total runs scored by each player in each season.\n   - For each player in each season, assign a rank based on the total runs scored, prioritizing players with lower IDs in case of ties.\n\n2. **Identify Top Bowlers for Each Season:**\n   - Create another temporary result set that calculates the total wickets taken by each player in each season, excluding specific types of outs.\n   - For each player in each season, assign a rank based on the total wickets taken, prioritizing players with lower IDs in case of ties.\n\n3. **Filter Top Batsmen and Bowlers:**\n   - From the temporary result sets, select only the top three batsmen and top three bowlers for each season based on their ranks.\n\n4. **Combine Results:**\n   - Join the filtered lists of top batsmen and top bowlers by matching the season and the rank.\n\n5. **Output the Results:**\n   - Select the season, player IDs of the top batsmen and bowlers, along with their respective total runs and wickets.\n   - Order the results by season and rank to ensure the output is organized and easy to read.",
        "special_function": null
    },
    {
        "instance_id": "local229",
        "db": "ipl_database",
        "question": "Find the IDs of players who scored the highest number of partnership runs for each match. The output should include the IDs of two players, each with their individual scores and the total partnership score. There can  be multiple rows for a single match.",
        "external_knowledge": null,
        "plan": "1. **Data Preparation**: Extend the original data by adding two new columns that ensure the smaller ID is always in the first new column and the larger ID in the second new column. This helps in standardizing the player pairings.\n\n2. **Calculate Partnership Runs**:\n    - Create a summarized dataset that groups the data by match and player pairs.\n    - For each group, calculate the total partnership runs and assign a rank based on the total runs within each match.\n\n3. **Detailed Scoring Information**:\n    - Join the summarized dataset with the original data to get individual run contributions for each player in the pair.\n    - Calculate the individual scores for the two players in each partnership and retain the total partnership runs and rank.\n\n4. **Normalize Player IDs**:\n    - For each partnership, determine the order of player IDs based on their individual scores. Ensure the player with the higher score is consistently listed first.\n    - If the scores are tied, use the player IDs to determine the order.\n\n5. **Final Output**:\n    - Filter the results to include only the top-ranked partnerships (i.e., the partnerships with the highest total runs) for each match.\n    - Select and display the match ID, player IDs in the correct order, their individual scores, and the total partnership runs.\n    - Sort the final output by total partnership runs in descending order and by match ID in ascending order.",
        "special_function": null
    },
    {
        "instance_id": "local253",
        "db": "salary",
        "question": "reate a detailed SQL report that compares the top 5 companies by average salary in Mumbai, Pune, New Delhi, and Hyderabad to the national average salary. The report should include the following columns: Location, Company Name, Average Salary in State, and Average Salary in Country.",
        "external_knowledge": null,
        "plan": "1. **Clean Salary Data**:\n   - Create a temporary dataset where the salary values are cleaned by removing specific characters and symbols to make them numeric.\n\n2. **Calculate State Average Salary**:\n   - From the cleaned salary data, compute the average salary for each company located in specified cities.\n   - Group the results by location and company.\n\n3. **Rank Companies by State Average Salary**:\n   - Assign a rank to each company within each location based on the computed average salary, ordered from highest to lowest.\n\n4. **Filter Top 5 Companies by Location**:\n   - From the ranked companies, select only the top 5 companies for each location.\n\n5. **Calculate National Average Salary**:\n   - Compute the average salary for each company across the entire dataset to get the national average.\n\n6. **Compile Final Report**:\n   - Join the top 5 companies per location with their corresponding national average salary.\n   - The final report includes columns for location, company name, average salary in the location, and average salary across the country.",
        "special_function": null
    },
    {
        "instance_id": "local262",
        "db": "stacking",
        "question": "Identify the models where, across all steps and versions, traditional models (excluding the Stack model) perform worse than the Stack model more times than the total number of evaluations of that model across all steps.",
        "external_knowledge": null,
        "plan": "1. **Calculate Total Evaluations**:\n   - Create a subquery to determine the total number of evaluations for each model by counting all the evaluations, grouping by the model's name.\n\n2. **Identify Performance Per Step**:\n   - Create another subquery to identify, for each model (excluding a specific one), the number of times it performs worse than the specific model in each step.\n   - This involves:\n     - Filtering and grouping the model data by model name, version, and step, and calculating the maximum score for each step.\n     - Joining this data with the scores of the specific model for the corresponding name, version, and step.\n     - Summing up the cases where the traditional model's maximum score is less than the specific model's score for each of the steps.\n\n3. **Aggregate Poor Performances**:\n   - Sum the counts of poor performances across all steps for each model.\n\n4. **Compare and Filter**:\n   - Join the total evaluations data with the aggregated poor performance data based on the model name.\n   - Filter out models where the sum of poor performances across all steps is greater than the total number of evaluations of that model.\n\n5. **Select Models**:\n   - Select the names of the models that meet the condition identified in the previous step.",
        "special_function": null
    },
    {
        "instance_id": "local269",
        "db": "oracle_sql",
        "question": "What is average quantity across all packaging combinations?",
        "external_knowledge": null,
        "plan": "1. **Initialize Recursive Query**:\n   - Start by selecting the initial set of records from the relations table where the item is not contained by any other item.\n   - Assign a level of 1 to these records.\n\n2. **Recursive Step**:\n   - In each recursive step, join the current set of records with the relations table to find items that are contained by the current items.\n   - Multiply the quantity from the previous level by the quantity in the current level.\n   - Increment the level by 1.\n\n3. **Rank Records**:\n   - After the recursive query completes, rank the results for each root item based on their levels using row numbering.\n\n4. **Identify Leaf Nodes**:\n   - For each record, determine if it is a leaf node. A leaf node is identified by checking if there is no lower level for the same root item.\n\n5. **Calculate Total Quantities**:\n   - For each combination of packaging and contained items, sum up the quantities of the leaf nodes.\n\n6. **Compute Average Quantity**:\n   - Calculate the average of the total quantities across all packaging combinations and round the result to two decimal places.\n\nBy following these steps, the query achieves the objective of finding the average quantity across all packaging combinations.",
        "special_function": null
    },
    {
        "instance_id": "local270",
        "db": "oracle_sql",
        "question": "Which packaging containers include items in quantities greater than 500, and what are those items?",
        "external_knowledge": null,
        "plan": "1. **Initial Recursive Query Setup**:\n   - Begin by defining a recursive common table expression (CTE) to traverse a hierarchical relationship of packaging containers.\n   - Select the root containers that are not contained within any other container, initializing the recursion with these root containers and their immediate contents.\n\n2. **Recursive Part of the Query**:\n   - Continue the recursive CTE by joining each level of containers with their contents.\n   - Multiply the quantities at each level to propagate the correct quantity through the hierarchy.\n   - Track the recursion depth to help identify the leaf nodes later.\n\n3. **Identify Leaf Nodes**:\n   - Use a subquery to determine which nodes are leaf nodes (i.e., they do not contain other items).\n   - Utilize window functions to detect changes in recursion level, marking the last level as a leaf node.\n\n4. **Summarize Quantities at Leaf Nodes**:\n   - Aggregate the quantities at the leaf node level to get the total quantity for each item contained within the root containers.\n\n5. **Final Selection**:\n   - Join the summarized quantities with the main table to get the names of the root containers and their contained items.\n   - Filter the results to include only those with quantities greater than the specified threshold (500).\n   - Order the final output by the identifiers of the root containers and their contained items to ensure a structured result.",
        "special_function": null
    },
    {
        "instance_id": "local272",
        "db": "oracle_sql",
        "question": "Which product ID, aisle, and position should be selected to pick the highest quantity for order 423, ensuring the picked quantity does not exceed the available inventory in warehouse 1, and calculate the quantity to be picked while prioritizing locations with earlier dates and smaller quantities?",
        "external_knowledge": null,
        "plan": "1. **Identify Order Lines**:\n   - Create a temporary dataset of order lines for the specified order.\n   - For each line, calculate a range (`from_q` to `to_q`) representing cumulative quantities up to and including the current line.\n\n2. **Prepare Inventory Data**:\n   - Create a temporary dataset for the inventory.\n   - Calculate cumulative quantities for each product in each inventory location, ordered by date and quantity.\n\n3. **Aggregate Order Quantities**:\n   - Sum the quantities for each product in the specified order.\n\n4. **Join Order Lines and Inventory**:\n   - Combine the order lines with the aggregated order quantities.\n   - Join this combined dataset with the prepared inventory data based on matching product IDs.\n\n5. **Filter Inventory Locations**:\n   - Ensure the cumulative quantities in the inventory do not exceed the order quantity.\n   - Ensure the inventory range intersects with the order line range.\n   - Filter for inventory only in the specified warehouse.\n\n6. **Calculate Quantity to be Picked**:\n   - For each matched inventory location, determine the quantity to be picked without exceeding the available inventory or the required order quantity.\n\n7. **Prioritize and Select**:\n   - Order the results by the quantity to be picked in descending order to prioritize locations with earlier dates and smaller quantities.\n\nBy following these steps, the query selects the appropriate product ID, aisle, and position to pick the highest quantity for the specified order while ensuring it does not exceed the available inventory in the specified warehouse, and calculates the quantity to be picked.",
        "special_function": null
    },
    {
        "instance_id": "local273",
        "db": "oracle_sql",
        "question": "What is the average pick percentage for each product, considering the quantity picked from inventory locations that are ordered by the earliest purchase date and smallest quantity, while ensuring that the picked quantity matches the overlapping range between the order quantity and the available inventory?",
        "external_knowledge": null,
        "plan": "1. **Identify Order Lines:**\n   - Create a temporary dataset to track each order line's quantity and compute the cumulative quantity for each product up to the current order line.\n\n2. **Aggregate Order Quantities:**\n   - Sum the quantities for each product across all order lines to determine the total quantity ordered for each product.\n\n3. **Match Inventory to Orders:**\n   - Generate a temporary dataset to match each product's ordered quantity with available inventory, ordered by the earliest purchase date and smallest quantity.\n   - Calculate the cumulative quantity of inventory picked so far for each product.\n\n4. **Calculate Picked Quantities:**\n   - Determine the quantity to be picked from each inventory location, ensuring it does not exceed the remaining order quantity.\n   - Establish the range of cumulative quantities covered by the current pick operation.\n\n5. **Identify Picked Data:**\n   - Match the picked quantities from inventory to the corresponding order lines by overlapping their cumulative quantity ranges.\n   - Calculate the actual picked quantity for each order line and compute the pick percentage.\n\n6. **Compute Average Pick Percentage:**\n   - Join the picked data with product information to associate each pick percentage with the corresponding product.\n   - Calculate the average pick percentage for each product.\n\n7. **Output Results:**\n   - Select the product names and their corresponding average pick percentages.\n   - Order the results by product name for readability.",
        "special_function": null
    },
    {
        "instance_id": "local274",
        "db": "oracle_sql",
        "question": "Which products were picked for order 421, and what is the average number of units picked for each product, using FIFO (First-In, First-Out) method?",
        "external_knowledge": null,
        "plan": "1. **Create a subquery to gather order lines for the specific order:**\n   - Filter order lines to include only those belonging to the given order.\n   - Calculate the range of quantities (from `from_q` to `to_q`) for each product in the order by considering previous orders of the same product.\n\n2. **Summarize the total quantities required for each product in the order:**\n   - Aggregate the quantities from the previous step, grouping by product.\n\n3. **Prepare a FIFO inventory picking plan:**\n   - Calculate the cumulative quantity available for each product in the inventory, considering the FIFO principle.\n   - Determine the quantity to pick from each inventory location, ensuring it does not exceed the ordered quantity and follows the FIFO method.\n\n4. **Identify the picking details:**\n   - Join the FIFO inventory picking plan with the order lines to match products and their required quantity ranges.\n   - Calculate the actual quantity picked from each inventory location for the order.\n\n5. **Calculate the average units picked for each product:**\n   - Join the picking details with product information to get the product names.\n   - Compute the average quantity picked for each product.\n   - Group the results by product and order them by product identifier.\n\nThis plan ensures that the products picked for the order are determined using the FIFO method, and the average number of units picked for each product is accurately calculated.",
        "special_function": null
    },
    {
        "instance_id": "local275",
        "db": "oracle_sql",
        "question": "What are products with a seasonality-adjusted sales ratio greater than 2 for the year 2017, based on monthly sales data starting from January 2016?",
        "external_knowledge": "calculation_method.md",
        "plan": "1. **Create a Recursive Date Series:**\n   - Generate a series of dates starting from January 1, 2016, incrementing by one month until you have 48 months in total. This will be used to ensure all months are included, even if there are no sales data for some months.\n\n2. **Join Monthly Sales Data:**\n   - For each date in the series, join with the monthly sales data to associate the sales quantities with the corresponding months. This ensures that every month has a corresponding sales record, even if it\u2019s zero.\n\n3. **Calculate Centered Moving Average (CMA):**\n   - For months between the 7th and 30th month, calculate the centered moving average (CMA) of sales quantities. The CMA is computed by averaging the sales quantities over an 11-month window (5 months before and 5 months after the current month).\n\n4. **Compute Seasonality Index:**\n   - For each month, calculate the seasonality index by dividing the actual sales quantity by the CMA. If the CMA is zero, adjust to avoid division by zero.\n\n5. **Filter and Aggregate Data:**\n   - Identify products that have a seasonality-adjusted sales ratio greater than 2 for the year 2017. This involves checking if the seasonality index exceeds 2 and ensuring the year is 2017.\n\n6. **Join with Product Information:**\n   - Join the filtered results with the product information to retrieve the product names.\n\n7. **Select and Order Results:**\n   - Select distinct product names that meet the criteria and order the results by product name and month.\n\nBy following these steps, you ensure that the query identifies products with significant seasonality-adjusted sales increases in 2017, based on a detailed analysis of monthly sales data starting from January 2016.",
        "special_function": null
    },
    {
        "instance_id": "local277",
        "db": "oracle_sql",
        "question": "What is the average forecasted annual sales for products 4160 and 7790 in 2018, using a weighted regression model based on the first 36 months from January 2016, with time steps between 7 and 30?",
        "external_knowledge": "calculation_method.md",
        "plan": "1. **Generate Time Series**:\n   - Create a recursive common table expression (CTE) to generate a series of months starting from January 2016 for 48 months, assigning a time step to each month.\n\n2. **Join Sales Data**:\n   - Left join the generated time series with the sales data for the specified products, ensuring that months with no sales data are included with a quantity of zero.\n\n3. **Calculate Centered Moving Average (CMA)**:\n   - For time steps between 7 and 30, calculate the centered moving average (CMA) using a weighted average over a window of 12 months, centered on the current month.\n\n4. **Calculate Seasonal Index**:\n   - Compute the seasonal index for each month by taking the average ratio of actual sales to CMA, replacing zero sales with a small value to avoid division by zero.\n\n5. **Deseasonalize Sales**:\n   - Deseasonalize the sales data for the first 36 months by dividing actual sales by the seasonal index.\n\n6. **Compute Regression Parameters**:\n   - Calculate the slope and intercept for a linear regression model using the deseasonalized sales data and the time steps.\n\n7. **Forecast Sales**:\n   - Use the regression model to forecast sales for 2018 by applying the slope and intercept to the time steps and adjusting by the seasonal index.\n\n8. **Calculate Average Forecasted Sales**:\n   - Average the forecasted sales values for 2018 and round to two decimal places to obtain the average forecasted annual sales for the specified products.",
        "special_function": null
    },
    {
        "instance_id": "local279",
        "db": "oracle_sql",
        "question": "Can you provide the product_id, month, and the smallest difference between the inventory and the minimum required level for each product in 2019?",
        "external_knowledge": null,
        "plan": "1. **Define a Recursive Common Table Expression (CTE)**:\n   - Initialize the recursive CTE by selecting initial values for each product, including the initial date, quantities, inventory levels, and other necessary parameters.\n   - Ensure to set up the base case of the recursion with the initial month and inventory levels.\n\n2. **Recursive Step**:\n   - Set up the recursive part of the CTE to iterate through each month.\n   - Calculate the inventory at the beginning and end of each month, considering purchases and required minimum levels.\n   - Determine the purchase date and quantity if the inventory falls below the minimum required level.\n   - Update the inventory for the next month based on the purchases made.\n\n3. **Compute Inventory Differences**:\n   - Create another CTE to calculate the absolute difference between the ending inventory and the minimum required level for each product and month.\n   - Filter the results to include only the year 2019.\n\n4. **Find Minimum Differences**:\n   - Aggregate the differences by product to find the smallest difference for each product within the specified timeframe.\n   \n5. **Final Selection**:\n   - Join the aggregated minimum differences back with the detailed differences to retrieve the specific month and difference value for each product.\n   - Order the final result by product and month for clear presentation.",
        "special_function": null
    },
    {
        "instance_id": "local283",
        "db": "EU_soccer",
        "question": "Analyze our match data to identify the name, leagues, and countries of the champion team for each season. Include the total points accumulated by each team.",
        "external_knowledge": null,
        "plan": "1. **Data Preparation:**\n   - Create an initial dataset by joining match data with country, league, and team information.\n   - Determine the results for each match, identifying whether the home team or away team won, lost, or tied.\n\n2. **Separate Team Data:**\n   - Generate two subsets of data:\n     - One for matches from the perspective of the home team.\n     - Another for matches from the perspective of the away team.\n\n3. **Combine Team Data:**\n   - Combine the two subsets into a single dataset, ensuring all matches are included with appropriate team identifiers and results.\n\n4. **Assign Points:**\n   - For each match, assign points based on the result:\n     - 3 points for a win.\n     - 1 point for a tie.\n     - 0 points for a loss.\n\n5. **Aggregate Points:**\n   - Group the combined dataset by season and team.\n   - Calculate the total points accumulated by each team for each season.\n\n6. **Rank Teams:**\n   - Rank the teams within each season based on their total points, with the highest points getting the top rank.\n\n7. **Identify Champions:**\n   - Filter the ranked data to retain only the top-ranked team (champion) for each season.\n\n8. **Final Output:**\n   - Select and display the relevant information (team name, league, country, total points) for the champion team of each season.\n   - Order the results by total points in descending order.",
        "special_function": null
    },
    {
        "instance_id": "local284",
        "db": "annex",
        "question": "Can you generate a summary of our items' loss rates? Include the average loss rate, and also break down the count of items that are below, above, and within one standard deviation from this average.",
        "external_knowledge": null,
        "plan": "1. **Calculate Average Loss Rate and Total Count:**\n   - Create a temporary table to store the average loss rate and the total number of items from the dataset.\n   - Filter out any records where the loss rate is empty.\n\n2. **Calculate Standard Deviation:**\n   - Create another temporary table to calculate the standard deviation of the loss rates.\n   - Use the previously calculated average loss rate.\n   - Apply the standard deviation formula which involves summing the squared differences from the mean, dividing by the number of items, and then taking the square root.\n   - Round the result to two decimal places.\n\n3. **Main Query - Summarize Loss Rates:**\n   - Select the average loss rate directly from the dataset.\n   - Count the number of items that fall within one standard deviation from the average loss rate using a conditional sum.\n   - Count the number of items with a loss rate above one standard deviation from the average using another conditional sum.\n   - Count the number of items with a loss rate below one standard deviation from the average using yet another conditional sum.\n\n4. **Combine Results:**\n   - Use the previously calculated average loss rate and standard deviation within the main query to perform the conditional checks and aggregations.\n   - Output the average loss rate, the count of items within one standard deviation, the count of items above one standard deviation, and the count of items below one standard deviation.",
        "special_function": null
    },
    {
        "instance_id": "local285",
        "db": "annex",
        "question": "Can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year.",
        "external_knowledge": null,
        "plan": "1. **Define Subqueries for Each Year:**\n   - Create subqueries for the years 2020, 2021, 2022, and 2023.\n   - For each year, select transactions and relevant join data.\n\n2. **Extract Yearly Data:**\n   - Use a function to extract the year from the transaction date.\n   - Ensure only transactions with a positive quantity sold are considered.\n\n3. **Aggregate Data by Category:**\n   - Group data by year and category.\n   - Calculate average, maximum, and minimum wholesale prices.\n   - Determine the difference between the maximum and minimum wholesale prices.\n   - Compute the total wholesale price and total selling price.\n   - Calculate the average loss rate.\n\n4. **Combine Yearly Data:**\n   - Use a UNION operation to combine the results of the subqueries for 2020, 2021, 2022, and 2023 into a single dataset.\n\n5. **Calculate Additional Metrics:**\n   - Calculate the total loss by applying the average loss rate to the total wholesale price.\n   - Calculate profit by subtracting the total wholesale price and total loss from the total selling price.\n\n6. **Final Selection:**\n   - Select all columns from the combined dataset, along with the calculated total loss and profit for each category within each year.",
        "special_function": null
    },
    {
        "instance_id": "local286",
        "db": "electronic_sales",
        "question": "Prepare a comprehensive performance report on our sellers, focusing on total sales, average item price, average review scores, and packing times. Ensure that the report includes only those sellers who handle more than 100 products and highlight their top-selling product.",
        "external_knowledge": null,
        "plan": "1. **Translate Product Categories:**\n   - Create a temporary table to map product IDs to their English category names by joining the products table with the category translation table.\n\n2. **Calculate Product Frequency:**\n   - Generate a temporary table to count how many times each product appears for each seller by joining the order items table with the translated products table. Group the results by seller and product.\n\n3. **Identify Top-Selling Product for Each Seller:**\n   - Create a temporary table to determine the top-selling product for each seller. This involves selecting the product with the highest count for each seller from the product frequency table.\n\n4. **Aggregate Seller Performance Metrics:**\n   - Construct a temporary table to aggregate various performance metrics for each seller, including total sales, average item price, average review scores, and average packing times. This is achieved by joining the order items, orders, and reviews tables. Also, include the top-selling product from the previous step and group the results by seller.\n\n5. **Filter Sellers by Product Count:**\n   - Select sellers who handle more than 100 products from the aggregated performance metrics table created in the previous step.\n\n6. **Present the Final Report:**\n   - Retrieve and display the final report containing seller ID, product count, average price, total sales, average packing time, average review score, and their top-selling product for sellers who meet the product count criteria.",
        "special_function": null
    },
    {
        "instance_id": "local301",
        "db": "data_mart",
        "question": "I need an analysis of our sales performance around mid-June for the years 2018, 2019, and 2020. Specifically, calculate the percentage change in sales between the four weeks leading up to June 15 and the four weeks following June 15 for each year.",
        "external_knowledge": null,
        "plan": "1. **Subquery for Date Difference Calculation**:\n    - Calculate the difference in days between each record's date and June 15 of the respective year.\n    - Convert this difference into weeks by dividing by 7 and rounding the result.\n    - Label these weekly differences as `delta_weeks`.\n\n2. **Subquery for Aggregating Sales Before and After June 15**:\n    - Sum the sales for the four weeks following June 15 (`delta_weeks` between 1 and 4) and label this sum as `after_effect`.\n    - Sum the sales for the four weeks leading up to June 15 (`delta_weeks` between -3 and 0) and label this sum as `before_effect`.\n\n3. **Main Query for Year 2018**:\n    - Retrieve `before_effect` and `after_effect` from the subquery.\n    - Calculate the absolute change in sales as `after_effect - before_effect`.\n    - Calculate the percentage change as `((after_effect / before_effect) - 1) * 100` and round it to two decimal places.\n    - Label the result with the year '2018'.\n\n4. **Repeat Steps for Year 2019**:\n    - Perform the same calculations and aggregations as in step 3, but for the year 2019.\n    - Label the result with the year '2019'.\n\n5. **Repeat Steps for Year 2020**:\n    - Perform the same calculations and aggregations as in step 3, but for the year 2020.\n    - Label the result with the year '2020'.\n\n6. **Combine Results Using UNION ALL**:\n    - Combine the results from the three years (2018, 2019, 2020) using the `UNION ALL` operator.\n\n7. **Order the Final Results**:\n    - Order the combined results by year for clarity.",
        "special_function": null
    },
    {
        "instance_id": "local302",
        "db": "data_mart",
        "question": "Analyze the sales performance across various business areas for the 12 weeks before and after June 15, 2020. Focus on regions, platforms, age band, demographics, and customer types, identify the area with the highest negative impact in sales metrics performance, provied its name and the percentage change in sales.",
        "external_knowledge": null,
        "plan": "1. **Define Metrics and Time Frames:**\n   - Identify the metrics to be analyzed: region, platform, age band, demographic, and customer type.\n   - Calculate the sales performance for two time frames: 12 weeks before and 12 weeks after a specific date (June 15, 2020).\n\n2. **Calculate Week Differences:**\n   - For each metric, determine the number of weeks before and after the specific date by calculating the difference in days and converting it to weeks.\n\n3. **Aggregate Sales Data:**\n   - For each metric, sum the sales data for the periods before and after the specific date.\n   - Use conditional aggregation to separate sales data into \"before\" and \"after\" periods based on the calculated week differences.\n\n4. **Compute Sales Change and Percentage Change:**\n   - For each metric, calculate the change in sales by subtracting the total sales in the \"before\" period from the total sales in the \"after\" period.\n   - Compute the percentage change in sales by comparing the \"after\" sales to the \"before\" sales, and convert it to a percentage.\n\n5. **Combine Results for All Metrics:**\n   - Combine the results for all the metrics into a single dataset, ensuring that each metric's results are labeled appropriately.\n\n6. **Calculate Average Percentage Change:**\n   - For each metric, compute the average percentage change in sales across all instances of that metric.\n\n7. **Identify the Highest Negative Impact:**\n   - Sort the metrics based on their average percentage change in ascending order to identify the metric with the highest negative impact on sales.\n   - Select the metric with the lowest average percentage change and limit the result to the top one.\n\n8. **Output the Result:**\n   - Return the metric name and its corresponding average percentage change to identify the area with the highest negative impact on sales performance.",
        "special_function": null
    },
    {
        "instance_id": "local329",
        "db": "log",
        "question": "How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",
        "external_knowledge": null,
        "plan": "1. **Define Stages and Paths**:\n   - Create a temporary table to define the stages and their corresponding paths. In this case, stage 1 is associated with the first page, and stage 2 is associated with the second page.\n\n2. **Map Logs to Stages**:\n   - Join the log data with the stages based on the paths.\n   - For each session and stage, calculate the earliest and latest timestamps when the stage was accessed.\n   - Filter out any logs with non-empty statuses.\n\n3. **Calculate Previous Stage Timestamps**:\n   - For each session and stage, retrieve the minimum timestamp from the previous stage.\n   - Determine the initial stage for each session.\n   - Count the cumulative stages completed by each session up to the current stage.\n\n4. **Identify Valid Session Paths**:\n   - Filter sessions where the initial stage is correctly identified.\n   - Ensure the cumulative count of stages matches the current stage.\n   - Validate that the timestamp conditions between stages are satisfied (i.e., the current stage's timestamp should be greater than or equal to the previous stage's timestamp).\n\n5. **Count Sessions Matching Path Sequence**:\n   - Join the valid session paths for the first stage with those of the second stage within the same session.\n   - Ensure that the timestamp of the first stage is earlier than the timestamp of the second stage.\n   - Count the number of unique sessions that follow the specified path sequence from the first stage to the second stage.\n\n6. **Return the Result**:\n   - Output the count of unique sessions that visited the first page and then the second page in the specified order.",
        "special_function": null
    },
    {
        "instance_id": "local329_2",
        "db": "log",
        "question": "Which path has the highest total count of visits across both landing and exit types? ",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local329_3",
        "db": "log",
        "question": "Which page type has fewer unique paths: landing or exit? ",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local330",
        "db": "log",
        "question": "What is the most common path after two consecutive '/detail' pages, including previous paths taken?",
        "external_knowledge": null,
        "plan": "1. **Identify Consecutive Paths**:\n   - Create a temporary dataset where each session\u2019s paths are listed along with the next two paths in chronological order.\n   - Use window functions to fetch the next path (1 step ahead) and the path after that (2 steps ahead) for each entry.\n\n2. **Filter and Count Paths**:\n   - From the temporary dataset, filter entries where the current path is a specific target path.\n   - Group by the current path and the next two paths.\n   - Count occurrences of each path combination and calculate overall counts for current path and the combination of the current and next paths.\n\n3. **Rank Paths by Frequency**:\n   - Create another temporary dataset ranking the filtered and grouped paths by their frequency.\n   - Use window functions to keep track of the previous paths leading up to the target path combination in descending order of their counts.\n\n4. **Extract Most Frequent Third Path**:\n   - From the ranked dataset, select the third path where the first two paths match the target path.\n   - Order the results by the frequency of the third path in descending order and limit the results to the top entry.\n\nBy following these steps, the query effectively identifies the most common third path following two consecutive target paths, taking into account previous navigation patterns.",
        "special_function": null
    },
    {
        "instance_id": "local334",
        "db": "log",
        "question": "What is the action count for the most frequent user action, categorized by login status?",
        "external_knowledge": null,
        "plan": "1. **Define User Status**:\n    - Create a temporary dataset categorizing each record based on whether a user is logged in or a guest by checking if a user identifier exists.\n  \n2. **Count Actions**:\n    - Generate another temporary dataset that counts the occurrences of each action for both logged-in users and guests by grouping the records by action and user status.\n\n3. **Rank Actions by Frequency**:\n    - Create another temporary dataset to rank the actions by their count for each user status category (logged-in or guest). The highest count gets the top rank (rank 1).\n\n4. **Retrieve Most Frequent Action Count**:\n    - Select the action count for the most frequent action (rank 1) for each user status category from the ranked dataset.",
        "special_function": null
    },
    {
        "instance_id": "local334_1",
        "db": "log",
        "question": "What is the difference between the maximum and minimum action counts for user actions, categorized by login status?",
        "external_knowledge": null,
        "plan": "1. **Classify User Sessions by Login Status**:\n   - Create a temporary result set that includes each user session and assigns a login status. If a user ID exists, label it as 'login'; otherwise, label it as 'guest'.\n\n2. **Count Actions by Login Status**:\n   - Using the classified user sessions, count the number of times each action occurs for both 'login' and 'guest' statuses. Group the results by action type and login status.\n\n3. **Determine Maximum and Minimum Action Counts**:\n   - From the action counts, identify the maximum and minimum counts of actions across all action types and login statuses.\n\n4. **Calculate the Difference**:\n   - Compute the difference between the maximum and minimum action counts obtained in the previous step. This difference represents the variation in action frequencies by login status.\n\n5. **Return the Result**:\n   - Output the computed difference as the final result.",
        "special_function": null
    },
    {
        "instance_id": "local344",
        "db": "f1",
        "question": "How many times has each type of overtake occurred in Formula 1?",
        "external_knowledge": "f1_overtake.md",
        "plan": "1. **Select Overtake Type and Count**: The query aims to count the number of occurrences for each type of overtake in Formula 1.\n\n2. **Subquery Setup**: Start with a subquery to prepare the data required for counting overtakes. This subquery includes various joins and calculations to determine the overtake types.\n\n3. **Distinct Overtakes**: Ensure that only distinct overtakes are considered by selecting unique combinations of race, driver, lap, and positions.\n\n4. **Join Previous Lap Data**: Join the current lap positions with the previous lap positions of the same driver to compare and identify changes in positions.\n\n5. **Identify Overtaken Cars**: Join the current lap positions with the positions of cars that are behind the current driver on the same lap.\n\n6. **Exclude Cars that Were Behind in Previous Lap**: Use a left join to exclude cars that were already behind in the previous lap to focus on new overtakes.\n\n7. **Identify Special Conditions**: Use left joins with additional tables to identify if the overtake was influenced by special conditions such as retirements, pit stops, or if it occurred at the start of the race.\n\n8. **Determine Overtake Type**: Use conditional logic to classify the overtake into types like retirement-induced, pit stop-related, start of the race, or regular track overtake.\n\n9. **Finalize Subquery Data**: Select the relevant fields from the subquery that include the overtake type and additional details for each identified overtake.\n\n10. **Count Overtakes by Type**: Group the results by the overtake type and count the occurrences of each type to get the final result.\n\n11. **Return Results**: The final query returns the count of each type of overtake, satisfying the user's request to know how many times each type of overtake occurred in Formula 1.",
        "special_function": null
    },
    {
        "instance_id": "local344_1",
        "db": "f1",
        "question": "How many overtakes of each type occurred during the first five laps of the race?",
        "external_knowledge": "f1_overtake.md",
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local335",
        "db": "f1",
        "question": "Which five constructors have had the most seasons in the 21st century where their drivers scored the fewest points in a Formula 1 season?",
        "external_knowledge": null,
        "plan": "1. **Identify Seasons with Points**:\n    - Combine data from two related sets to determine the total points scored by each participant in each season.\n    - Filter to include only entries where participants scored points.\n\n2. **Find Minimum Points per Season**:\n    - For each season, find the lowest points scored by any participant, considering both individual participants and their associated groups separately.\n\n3. **Determine Participants with Fewest Points**:\n    - For each season, identify the participants and their groups who scored the fewest points based on the previously calculated minimum points.\n\n4. **Filter and Count Seasons in the 21st Century**:\n    - Focus on seasons starting from the year 2001.\n    - Count how many times each group appears as having participants with the fewest points in a season.\n\n5. **Select Top Groups**:\n    - Sort the groups by the number of seasons they had participants scoring the fewest points.\n    - Limit the result to the top five groups.",
        "special_function": null
    },
    {
        "instance_id": "local309",
        "db": "f1",
        "question": "For each year, which driver and which constructor scored the most points? I want the full name of each driver.",
        "external_knowledge": null,
        "plan": "1. **Calculate Yearly Points for Drivers and Constructors:**\n   - Create a temporary dataset to store the total points scored by each driver and constructor for each year.\n   - For drivers, concatenate their first and last names to form their full name and calculate their total points per year.\n   - For constructors, calculate their total points per year.\n\n2. **Identify Maximum Points per Year:**\n   - Create another temporary dataset to store the maximum points scored by any driver and any constructor for each year.\n   - Use conditional logic to separately find the maximum points for drivers and constructors.\n\n3. **Match Maximum Points to Corresponding Driver and Constructor:**\n   - Join the maximum points dataset with the yearly points dataset to identify the driver and constructor who scored the maximum points for each year.\n   - Ensure that the driver and constructor names are not null when performing the join.\n\n4. **Select and Order Results:**\n   - Select the year, the driver with the maximum points, and the constructor with the maximum points.\n   - Order the final results by year to present them in chronological order.",
        "special_function": null
    },
    {
        "instance_id": "local309_2",
        "db": "f1",
        "question": "Which three years had the lowest combined points between the top driver and the top constructor in the same year?",
        "external_knowledge": null,
        "plan": "",
        "special_function": null
    },
    {
        "instance_id": "local309_3",
        "db": "f1",
        "question": "Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?",
        "external_knowledge": null,
        "plan": "1. **Create a Temporary Data Set for Yearly Points:**\n   - Combine data from multiple sources to get yearly points for each driver and constructor.\n   - Use a `LEFT JOIN` to merge related data.\n   - Group the data by year and driver/constructor to calculate total points.\n\n2. **Union Driver and Constructor Points:**\n   - Perform a union operation to integrate points for drivers and constructors.\n   - Ensure to separately handle drivers and constructors while calculating points.\n\n3. **Create Another Temporary Data Set for Maximum Points:**\n   - Aggregate the maximum points per year for both drivers and constructors.\n   - Use conditional logic to separate driver points and constructor points.\n\n4. **Calculate Combined Points:**\n   - Select the year and constructor.\n   - Add the maximum points of the best driver and the constructor within the same year to get combined points.\n\n5. **Join with Original Data Set to Get Details:**\n   - Join back with the original temporary data set to match maximum driver points and constructor points with their corresponding details.\n   - Ensure joins are done on the year and the specific points calculated.\n\n6. **Order and Limit the Results:**\n   - Order the results by the combined points in descending order to get the highest scores at the top.\n   - Limit the output to the top 3 entries to get the top constructors with their highest combined points.\n\n7. **Output:**\n   - Return the year, constructor name, and combined points for the top 3 results.",
        "special_function": null
    },
    {
        "instance_id": "local353",
        "db": "f1",
        "question": "What are the five most frequent types of laps, and their respective frequencies in race 1?",
        "external_knowledge": "lap_type.md",
        "plan": "1. **Combine Laps Information:**\n   - Retrieve and label laps from different sources (races, results, retirements) related to a specific race using a common identifier.\n   - Ensure all relevant laps are considered by merging them into a single dataset.\n\n2. **Label Laps:**\n   - Assign a type to each lap based on predefined conditions to categorize them.\n   - Use conditional logic to determine the appropriate label for each lap type:\n     - Label general race laps.\n     - Label starting position laps with specific conditions such as no qualification, pit lane start, grid drop, grid increase, and qualifying.\n     - Label laps based on retirement type.\n\n3. **Count Frequencies:**\n   - Group the combined and labeled laps by their type.\n   - Count how many times each lap type appears to determine its frequency.\n\n4. **Sort and Limit Results:**\n   - Order the lap types by their frequency in descending order to identify the most common types.\n   - Limit the results to the top five most frequent lap types to fulfill the query requirement.\n\nBy following these steps, the query identifies and returns the five most frequent types of laps along with their respective frequencies for the specified race.",
        "special_function": null
    },
    {
        "instance_id": "local354",
        "db": "f1",
        "question": "Can you tell me the driver IDs of those who participated in multiple races during a season in the 1950s where none of their races were the first or last of the season?",
        "external_knowledge": null,
        "plan": "1. **Identify Preliminary Data**: Create a preliminary dataset that includes distinct combinations of years, driver IDs, race rounds, and constructor IDs. For each driver in each year, determine if a race is their first or last race of that season by checking if the constructor ID changes from the previous or next race using window functions.\n\n2. **Determine First and Last Races**: From the preliminary dataset, calculate the first and last race rounds for each driver in each year.\n\n3. **Filter Out First and Last Races**: Join the preliminary dataset with the dataset of first and last races to identify races that are either the first or last of the season for each driver.\n\n4. **Exclude First and Last Races**: Filter the results to exclude races that are the first or last race for the driver in that season.\n\n5. **Filter by Decade**: Narrow down the dataset to include only the years within the 1950s.\n\n6. **Ensure Multiple Races**: Group the remaining data by driver IDs and ensure that each driver participated in more than one race that was neither their first nor their last race of the season.\n\n7. **Select Unique Driver IDs**: Finally, select the distinct driver IDs who meet all the criteria and return this list as the result.",
        "special_function": null
    },
    {
        "instance_id": "local355",
        "db": "f1",
        "question": "What is the average round number of the first and last missed rounds for drivers who missed fewer than 3 rounds and had different constructors immediately before and after their missed rounds?",
        "external_knowledge": null,
        "plan": "1. **Identify Missed Races**: \n   - Create a temporary list of races where drivers did not participate. For each missed race, also track the constructor information for the races immediately before and after the missed race.\n\n2. **Determine First and Last Missed Races**:\n   - From the list of missed races, identify the first missed race by considering the race immediately before it.\n   - Similarly, identify the last missed race by considering the race immediately after it.\n\n3. **Filter Drivers with Less than 3 Missed Races**:\n   - Count the number of missed races for each driver in each year. Keep only those drivers who missed fewer than 3 races in a year.\n\n4. **Calculate Average Round Numbers**:\n   - For drivers who meet the above criteria, calculate the average round number of their first missed race.\n   - Similarly, calculate the average round number of their last missed race.\n\n5. **Ensure Different Constructors Before and After Missed Races**:\n   - Only consider those missed races where the constructor for the race immediately before the first missed race is different from the constructor for the race immediately after the last missed race.\n\n6. **Final Output**:\n   - Return the average round numbers of the first and last missed races for the filtered drivers.",
        "special_function": null
    },
    {
        "instance_id": "local356",
        "db": "f1",
        "question": "Can you tell me the full names of drivers who have been overtaken more times than they have performed overtakes?",
        "external_knowledge": null,
        "plan": "1. **Aggregate Overtakes by Drivers**: Create a summary that counts the number of times each driver has performed an overtaking maneuver.\n\n2. **Aggregate Being Overtaken by Drivers**: Create another summary that counts the number of times each driver has been overtaken.\n\n3. **Combine Both Summaries**: Merge the two summaries to create a comprehensive view of each driver\u2019s overtaking and being overtaken counts. Use a full outer join to ensure all drivers are included, even if they only appear in one of the summaries. Use default values to handle any missing data.\n\n4. **Identify Drivers with More Overtakes Against Them**: Filter this combined summary to retain only those drivers who have been overtaken more times than they have overtaken others.\n\n5. **Retrieve Full Names**: Join the filtered list of drivers with another table that contains driver details to retrieve the full names of the drivers.\n\n6. **Output the Result**: Select and return the full names of the drivers who meet the specified condition.",
        "special_function": null
    }
]