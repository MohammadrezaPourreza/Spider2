{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json.decoder\n",
    "from utils.enums import LLM\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "\n",
    "GCP_PROJECT = os.getenv(\"GCP_PROJECT\")\n",
    "GCP_REGION = os.getenv(\"GCP_REGION\")\n",
    "GCP_CREDENTIALS = os.getenv(\"GCP_CREDENTIALS\")\n",
    "\n",
    "def init_gemini():\n",
    "    aiplatform.init(\n",
    "        project=GCP_PROJECT,\n",
    "        location=GCP_REGION,\n",
    "        credentials=service_account.Credentials.from_service_account_file(GCP_CREDENTIALS)\n",
    "    )\n",
    "    vertexai.init(project=GCP_PROJECT, location=GCP_REGION, credentials=service_account.Credentials.from_service_account_file(GCP_CREDENTIALS))\n",
    "\n",
    "init_gemini()\n",
    "\n",
    "llm = VertexAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.5,\n",
    "    safety_settings=safety_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given an SQL query. Your task is to extract the db_ids (the database identifiers), table_names (the names of the tables being queried), and column_names (the names of the columns referenced in the query). Organize this information in the form of a dictionary where:\\n\\nThe keys of the dictionary represent the db_ids (database identifiers).\\nEach db_id maps to another dictionary where:\\nThe keys are the table_names.\\nThe values are a list of the column_names used from that table.\\nThe final output should be a JSON object that follows this structure.\\n\\nTask Details:\\ndb_ids: Extract the database identifiers from the query (these are usually in the form of project.dataset in systems like BigQuery). If the query does not specify a db_id (for example, in SQLite queries), set the db_id to \"public\".\\ntable_names: Identify the table names used in the query, including any table wildcards if applicable (e.g., events_*).\\ncolumn_names: Extract the column names from the query, including fields nested inside structs (e.g., event_params.key, event_params.value.int_value).\\n\\nFew-Shot Examples:\\nExample 1:\\nInput SQL Query:\\n\\nSELECT \\n    user_id, \\n    product_name, \\n    sales_amount \\nFROM \\n    ecommerce.sales_data \\nWHERE \\n    sales_amount > 100;\\nExpected Output:\\n\\n{{\\n  \"ecommerce\": {{\\n    \"sales_data\": [\\n      \"user_id\", \\n      \"product_name\", \\n      \"sales_amount\"\\n    ]\\n  }}\\n}}\\n\\nExample 2:\\nInput SQL Query:\\n\\nSELECT \\n    customer.id, \\n    customer.name, \\n    orders.order_id, \\n    orders.order_date \\nFROM \\n    company_data.customers AS customer \\nJOIN \\n    company_data.orders AS orders \\nON \\n    customer.id = orders.customer_id;\\nExpected Output:\\n\\n{{\\n  \"company_data\": {{\\n    \"customers\": [\\n      \"id\", \\n      \"name\"\\n    ],\\n    \"orders\": [\\n      \"order_id\", \\n      \"order_date\", \\n      \"customer_id\"\\n    ]\\n  }}\\n}}\\n\\nExample 3:\\nInput SQL Query:\\n\\nSELECT\\n  COUNT(DISTINCT MDaysUsers.user_pseudo_id) AS n_day_inactive_users_count\\nFROM\\n  (\\n    SELECT\\n      user_pseudo_id\\n    FROM\\n      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T\\n    CROSS JOIN\\n      UNNEST(T.event_params) AS event_params\\n    WHERE\\n      event_params.key = \\'engagement_time_msec\\' \\n      AND event_params.value.int_value > 0\\n      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP(\\'2021-01-07 23:59:59\\'), INTERVAL 7 DAY))\\n      AND _TABLE_SUFFIX BETWEEN \\'20210101\\' AND \\'20210107\\'\\n  ) AS MDaysUsers\\nLEFT JOIN\\n  (\\n    SELECT\\n      user_pseudo_id\\n    FROM\\n      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T\\n    CROSS JOIN\\n      UNNEST(T.event_params) AS event_params\\n    WHERE\\n      event_params.key = \\'engagement_time_msec\\' \\n      AND event_params.value.int_value > 0\\n      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP(\\'2021-01-07 23:59:59\\'), INTERVAL 2 DAY))\\n      AND _TABLE_SUFFIX BETWEEN \\'20210105\\' AND \\'20210107\\'\\n  ) AS NDaysUsers\\nON MDaysUsers.user_pseudo_id = NDaysUsers.user_pseudo_id\\nWHERE\\n  NDaysUsers.user_pseudo_id IS NULL;\\nExpected Output:\\n\\n{{\\n  \"bigquery-public-data.ga4_obfuscated_sample_ecommerce\": {{\\n    \"events_*\": [\\n      \"user_pseudo_id\",\\n      \"event_params.key\",\\n      \"event_params.value.int_value\",\\n      \"event_timestamp\",\\n      \"_TABLE_SUFFIX\"\\n    ]\\n  }}\\n}}\\n\\nExample 4:\\nInput SQL Query:\\n\\nWITH watched_repos AS (     \\n    SELECT         \\n        repo.name AS repo     \\n    FROM          \\n        `githubarchive.month.2017*`     \\n    WHERE         \\n        type = \"WatchEvent\" \\n), repo_watch_counts AS (     \\n    SELECT         \\n        repo,         \\n        COUNT(*) AS watch_count     \\n    FROM         \\n        watched_repos     \\n    GROUP BY         \\n        repo \\n)  \\nSELECT     \\n    r.repo,     \\n    r.watch_count \\nFROM     \\n    `bigquery-public-data.github_repos.sample_files` AS f \\nJOIN     \\n    `bigquery-public-data.github_repos.sample_contents` AS c \\nON     \\n    f.id = c.id \\nJOIN      \\n    repo_watch_counts AS r \\nON     \\n    f.repo_name = r.repo \\nWHERE     \\n    f.path LIKE \\'%.py\\'      \\n    AND c.size < 15000      \\n    AND REGEXP_CONTAINS(c.content, r\\'def \\') \\nGROUP BY     \\n    r.repo, r.watch_count \\nORDER BY     \\n    r.watch_count DESC \\nLIMIT 3;\\nExpected Output:\\n\\n{{\\n  \"githubarchive.month\": {{\\n    \"2017*\": [\\n      \"repo.name\",\\n      \"type\"\\n    ]\\n  }},\\n  \"bigquery-public-data.github_repos\": {{\\n    \"sample_files\": [\\n      \"id\",\\n      \"repo_name\",\\n      \"path\"\\n    ],\\n    \"sample_contents\": [\\n      \"id\",\\n      \"size\",\\n      \"content\"\\n    ]\\n  }}\\n}}\\n\\nExample 5 (SQLite example):\\nInput SQL Query:\\n\\nSELECT \\n    customer_id, \\n    order_date, \\n    total_amount \\nFROM \\n    Customer_Orders\\nWHERE \\n    total_amount > 500;\\nExpected Output:\\n\\n{{\\n  \"public\": {{\\n    \"Customer_Orders\": [\\n      \"customer_id\", \\n      \"order_date\", \\n      \"total_amount\"\\n    ]\\n  }}\\n}}\\n\\nExample 6 (SQLite example):\\nInput SQL Query:\\n\\nWITH product_sales AS (\\n    SELECT product_id, SUM(sale_amount) AS total_sales\\n    FROM Sales\\n    GROUP BY product_id\\n)\\nSELECT p.product_id, p.product_name, ps.total_sales\\nFROM Products p\\nJOIN product_sales ps ON p.product_id = ps.product_id;\\nExpected Output:\\n\\n{{\\n  \"public\": {{\\n    \"Sales\": [\\n      \"product_id\", \\n      \"sale_amount\"\\n    ],\\n    \"Products\": [\\n      \"product_id\", \\n      \"product_name\"\\n    ],\\n    \"product_sales\": [\\n      \"product_id\", \\n      \"total_sales\"\\n    ]\\n  }}\\n}}\\n\\nExample 7:\\nInput SQL Query:\\n\\nWITH customer_balance AS (\\n    SELECT *,\\n           SUM(txn_amount) OVER (PARTITION BY customer_id ORDER BY month_ ASC) AS balance\\n    FROM (\\n        SELECT customer_id, strftime(\\'%Y-%m-01\\', txn_date) AS month_, SUM(txn_group) AS txn_amount \\n        FROM (\\n            SELECT *,\\n                   CASE WHEN txn_type = \\'deposit\\' THEN txn_amount\\n                        ELSE -1 * txn_amount END AS txn_group\\n            FROM Customer_Transactions\\n            ORDER BY customer_id, txn_date\\n        ) AS update_txn_amount\\n        GROUP BY customer_id, strftime(\\'%Y-%m-01\\', txn_date)\\n    ) AS monthly_totals\\n),\\ngrowth_rates AS (\\n    SELECT customer_id, month_, balance, previous_month_balance, \\n           CASE WHEN previous_month_balance IS NULL THEN NULL\\n                WHEN previous_month_balance = 0 THEN balance * 100\\n                ELSE ROUND(((balance - previous_month_balance) * 1.0 / ABS(previous_month_balance)) * 100, 1) END AS growth_rate,\\n           ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY month_ DESC) AS balance_index\\n    FROM (\\n        SELECT *,\\n               LAG(balance) OVER (PARTITION BY customer_id ORDER BY month_) AS previous_month_balance\\n        FROM customer_balance\\n    ) AS add_previous_month_balance\\n),\\ncust_last_balance AS (\\n    SELECT customer_id, month_, growth_rate,\\n           CASE WHEN growth_rate > 5 THEN 1 ELSE 0 END AS growth_rate_check \\n    FROM growth_rates\\n    WHERE balance_index = 1\\n)\\nSELECT ROUND((SUM(growth_rate_check) * 1.0 / COUNT(*) * 100), 2) || \\'%\\' AS percentage_growth_check\\nFROM cust_last_balance;\\nExpected Output:\\n\\n{{\\n  \"Customer_Transactions\": {{\\n    \"update_txn_amount\": [\\n      \"customer_id\",\\n      \"txn_date\",\\n      \"txn_amount\",\\n      \"txn_type\",\\n      \"txn_group\"\\n    ]\\n  }},\\n  \"customer_balance\": [\\n    \"customer_id\",\\n    \"month_\",\\n    \"txn_amount\",\\n    \"balance\"\\n  ],\\n  \"growth_rates\": [\\n    \"customer_id\",\\n    \"month_\",\\n    \"balance\",\\n    \"previous_month_balance\",\\n    \"growth_rate\",\\n    \"balance_index\"\\n  ],\\n  \"cust_last_balance\": [\\n    \"customer_id\",\\n    \"month_\",\\n    \"growth_rate\",\\n    \"growth_rate_check\"\\n  ]\\n}}\\n\\nNow, it\\'s your turn to output the schema for the following SQL query:\\n\\nInput SQL Query:\\n{SQL_QUERY}\\n\\nOutput Format:\\nThe final output should be in JSON format, with the structure:\\n\\n{{\\n  \"db_id\": {{\\n    \"table_name\": [\\n      \"column_name_1\", \\n      \"column_name_2\", \\n      ...\\n    ]\\n  }}\\n}}\\nThe column_names list should include any nested fields (like event_params.key or event_params.value.int_value). Only include table names and columns that are explicitly mentioned in the query.\\n\\nInstructions for Special Cases:\\n- Wildcard table names: If the query uses wildcard tables (e.g., events_*), use the wildcard table name in the output (e.g., \"events_*\").\\n- Alias handling: If a table has an alias in the query (e.g., FROM table AS T), use the original table name, not the alias.\\n- Nested fields: If a column is nested (e.g., event_params.value.int_value), ensure that the full path is included in the list of column names.\\n- SQLite queries: If the query is from the SQLite dialect and doesn\\'t specify a db_id, set the db_id to \"public\".\\n\\nOnly output the dictionary in the described format. Do not include any additional information in the output.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_path = \"schema_extraction.txt\"\n",
    "\n",
    "with open(prompt_path, \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"preprocessed_data/spider2-lite/spider2-lite_preprocessed.json\"\n",
    "import json\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_a_three_layer_dict(temp):\n",
    "    # print(temp)\n",
    "    try:\n",
    "        for db, tables_dict in temp.items():\n",
    "            if isinstance(tables_dict, dict):\n",
    "                for table, columns_list in tables_dict.items():\n",
    "                    if isinstance(columns_list, list):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"{db} {table} {columns_list}\")\n",
    "                        return False\n",
    "            else:\n",
    "                print(f\"{db} {tables_dict}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 271 schemas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 325/330 [04:12<00:18,  3.75s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "schemas = {}\n",
    "# load the schema\n",
    "if os.path.exists(\"schemas.json\"):\n",
    "    with open(\"schemas.json\", \"r\") as f:\n",
    "        schemas = json.load(f)\n",
    "    print(f\"Loaded {len(schemas)} schemas\")\n",
    "\n",
    "error_ids = {}\n",
    "\n",
    "for i, entry in enumerate(tqdm(data)):\n",
    "    instance_id = entry[\"instance_id\"]\n",
    "    query = entry[\"query\"].replace(\"\\n\", \" \")\n",
    "    if instance_id in schemas:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        for k in range(5):\n",
    "            message = prompt_template.format(SQL_QUERY=query)\n",
    "            response = llm.invoke(message)\n",
    "            if \"```json\" in response:\n",
    "                response = response.split(\"```json\")[1]\n",
    "                response = response.split(\"```\")[0]\n",
    "            temp_dict = json.loads(response)\n",
    "            if is_it_a_three_layer_dict(temp_dict):\n",
    "                schemas[instance_id] = {\n",
    "                    \"query\": query,\n",
    "                    \"schema\": temp_dict\n",
    "                }\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Retrying {k} at {instance_id}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        # schemas[instance_id] = {\n",
    "        #     \"query\": query,\n",
    "        #     \"schema\": None,\n",
    "        #     \"error\": str(e)\n",
    "        # }\n",
    "        error_ids[instance_id] = {\n",
    "            \"query\": query,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "        print(f\"Error at {i}: {str(e)}\")\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        #save the schemas\n",
    "        with open(\"schemas.json\", \"w\") as f:\n",
    "            json.dump(schemas, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the schemas\n",
    "with open(\"schemas.json\", \"w\") as f:\n",
    "    json.dump(schemas, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# load the schemas\n",
    "with open(\"schemas.json\", \"r\") as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_schemas = {}\n",
    "for instance_id, entry in schemas.items():\n",
    "    schema = entry[\"schema\"]\n",
    "    bad_generation = False\n",
    "    if len(schema.keys()) == 0:\n",
    "        print(f\"{instance_id} {entry['query']}\")\n",
    "        bad_generation = True\n",
    "        fixed_schema = {}\n",
    "    else:\n",
    "        for db, tables_dict in schema.items():\n",
    "            if len(db) < 6:\n",
    "                print(f\"{instance_id} {db}\")\n",
    "                bad_generation = True\n",
    "                fixed_schema = {\"public\": tables_dict}\n",
    "                \n",
    "    if not bad_generation:\n",
    "        good_schemas[instance_id] = entry\n",
    "    else:\n",
    "        good_schemas[instance_id] = {\n",
    "            \"query\": entry[\"query\"],\n",
    "            \"schema\": fixed_schema\n",
    "        }\n",
    "        \n",
    "# # Save the good schemas\n",
    "# with open(\"good_schemas.json\", \"w\") as f:\n",
    "#     json.dump(good_schemas, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
